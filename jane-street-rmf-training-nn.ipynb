{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful notebooks:\n",
    "\n",
    "- Preprocessing : https://www.kaggle.com/code/motono0223/js24-preprocessing-create-lags\n",
    "- Training (XGB) : https://www.kaggle.com/code/motono0223/js24-train-gbdt-model-with-lags-singlemodel\n",
    "  - trained XGB model : https://www.kaggle.com/datasets/motono0223/js24-trained-gbdt-model\n",
    "- Training (NN): **this notebook** https://www.kaggle.com/code/voix97/jane-street-rmf-training-nn\n",
    "  - trained NN model : https://www.kaggle.com/datasets/voix97/js-xs-nn-trained-model\n",
    "- Inference of NN : https://www.kaggle.com/code/voix97/jane-street-rmf-nn-with-pytorch-lightning\n",
    "- Inference of NN+XGB:  https://www.kaggle.com/code/voix97/jane-street-rmf-nn-xgb\n",
    "- EDA(1) : https://www.kaggle.com/code/motono0223/eda-jane-street-real-time-market-data-forecasting\n",
    "- EDA(2) : https://www.kaggle.com/code/motono0223/eda-v2-jane-street-real-time-market-forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks (MLP) with PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:58:17.953615Z",
     "iopub.status.busy": "2024-11-05T04:58:17.952915Z",
     "iopub.status.idle": "2024-11-05T04:58:17.962711Z",
     "shell.execute_reply": "2024-11-05T04:58:17.961348Z",
     "shell.execute_reply.started": "2024-11-05T04:58:17.953574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#input_path = './input_df' if os.path.exists('./input_df') else '/kaggle/input/js24-preprocessing-create-lags/training.parquet'\n",
    "input_path = \"./\"\n",
    "TRAINING = True\n",
    "feature_names = [\"cust_request_qty\", 'cust_request_tn', 'tn', 'stock_final', \"cat1\", \"cat2\", \"cat3\", \"brand\", \"sku_size\", \"year\", \"mes\", \"quarter\"] #+ [f\"tn_lag_{idx}\" for idx in range(9)]\n",
    "label_name = 'target'\n",
    "weight_name = 'weight'\n",
    "df = pl.scan_parquet(f\"{input_path}/df_base_train.parquet\").collect().to_pandas()\n",
    "valid = pl.scan_parquet(f\"{input_path}/df_base_valid.parquet\").collect().to_pandas()\n",
    "df = pd.concat([df, valid]).reset_index(drop=True)# A trick to boost LB from 0.0045->0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29652, 12), (29652,), (29652,), (955, 12), (955,), (955,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df[ feature_names ]\n",
    "y_train = df[ label_name ]\n",
    "w_train = df[ \"weight\" ]\n",
    "X_valid = valid[ feature_names ]\n",
    "y_valid = valid[ label_name ]\n",
    "w_valid = valid[ \"weight\" ]\n",
    "\n",
    "X_train.shape, y_train.shape, w_train.shape, X_valid.shape, y_valid.shape, w_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:58:17.965474Z",
     "iopub.status.busy": "2024-11-05T04:58:17.964754Z",
     "iopub.status.idle": "2024-11-05T04:58:17.993759Z",
     "shell.execute_reply": "2024-11-05T04:58:17.992851Z",
     "shell.execute_reply.started": "2024-11-05T04:58:17.965427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "#import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class custom_args():\n",
    "    def __init__(self):\n",
    "        self.usegpu = True\n",
    "        self.gpuid = 0\n",
    "        self.seed = 42\n",
    "        self.model = 'nn'\n",
    "        self.use_wandb = False\n",
    "        self.project = 'labo3'\n",
    "        self.dname = \"./input_df/\"\n",
    "        self.loader_workers = 4\n",
    "        self.bs = 8192\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 5e-4\n",
    "        self.dropouts = [0.1, 0.1]\n",
    "        self.n_hidden = [512, 512, 256]\n",
    "        self.patience = 25\n",
    "        self.max_epochs = 2000\n",
    "        self.N_fold = 5\n",
    "\n",
    "\n",
    "my_args = custom_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Data Module Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:58:17.995267Z",
     "iopub.status.busy": "2024-11-05T04:58:17.994912Z",
     "iopub.status.idle": "2024-11-05T04:58:18.776454Z",
     "shell.execute_reply": "2024-11-05T04:58:18.77564Z",
     "shell.execute_reply.started": "2024-11-05T04:58:17.995223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, accelerator):\n",
    "        self.features = torch.FloatTensor(df[feature_names].values).to(accelerator)\n",
    "        self.labels = torch.FloatTensor(df[label_name].values).to(accelerator)\n",
    "        self.weights = torch.FloatTensor(df[weight_name].values).to(accelerator)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        w = self.weights[idx]\n",
    "        return x, y, w\n",
    "\n",
    "\n",
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, train_df, batch_size, valid_df=None, accelerator='cpu'):\n",
    "        super().__init__()\n",
    "        self.df = train_df\n",
    "        self.batch_size = batch_size\n",
    "        self.dates = self.df['date_id'].unique()\n",
    "        self.accelerator = accelerator\n",
    "        self.train_dataset = None\n",
    "        self.valid_df = None\n",
    "        if valid_df is not None:\n",
    "            self.valid_df = valid_df\n",
    "        self.val_dataset = None\n",
    "\n",
    "    def setup(self, fold=0, N_fold=5, stage=None):\n",
    "        # Split dataset\n",
    "        selected_dates = [date for ii, date in enumerate(self.dates) if ii % N_fold != fold]\n",
    "        df_train = self.df.loc[self.df['date_id'].isin(selected_dates)]\n",
    "        self.train_dataset = CustomDataset(df_train, self.accelerator)\n",
    "        if self.valid_df is not None:\n",
    "            df_valid = self.valid_df\n",
    "            self.val_dataset = CustomDataset(df_valid, self.accelerator)\n",
    "\n",
    "    def train_dataloader(self, n_workers=0):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=n_workers)\n",
    "\n",
    "    def val_dataloader(self, n_workers=0):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=n_workers)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:58:18.778953Z",
     "iopub.status.busy": "2024-11-05T04:58:18.778669Z",
     "iopub.status.idle": "2024-11-05T04:58:18.796672Z",
     "shell.execute_reply": "2024-11-05T04:58:18.795818Z",
     "shell.execute_reply.started": "2024-11-05T04:58:18.778923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom R2 metric for validation\n",
    "def r2_val(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
    "    return r2\n",
    "\n",
    "def total_error_val(y_true, y_pred, sample_weight):\n",
    "    print(\"y_true nans: \", np.isnan(y_true).sum())\n",
    "    print(\"y_pred nans: \", np.isnan(y_pred).sum())\n",
    "    total_error = np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true))\n",
    "    return total_error\n",
    "\n",
    "\n",
    "class NN(LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.BatchNorm1d(in_dim))\n",
    "            if i > 0:\n",
    "                layers.append(nn.SiLU())\n",
    "            if i < len(dropouts):\n",
    "                layers.append(nn.Dropout(dropouts[i]))\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            # layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, 1)) \n",
    "        layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 5 * self.model(x).squeeze(-1)  \n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w  #\n",
    "        loss = loss.mean()\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w\n",
    "        loss = loss.mean()\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "        self.validation_step_outputs.append((y_hat, y, w))\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n",
    "        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        if self.trainer.sanity_checking:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        else:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            # r2_val\n",
    "            val_r_square = total_error_val(y, prob, weights)\n",
    "            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n",
    "                                                               )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.trainer.sanity_checking:\n",
    "            return\n",
    "        epoch = self.trainer.current_epoch\n",
    "        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n",
    "        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n",
    "        print(f\"Epoch {epoch}: {formatted_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PyTorch Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_424015/680207757.py:11: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[feature_names] = df[feature_names].fillna(method = 'ffill').fillna(0)\n",
      "/tmp/ipykernel_424015/680207757.py:12: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  valid[feature_names] = valid[feature_names].fillna(method = 'ffill').fillna(0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = my_args\n",
    "\n",
    "# checking device\n",
    "device = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() and args.usegpu else 'cpu')\n",
    "accelerator = 'gpu' if torch.cuda.is_available() and args.usegpu else 'cpu'\n",
    "loader_device = 'cpu'\n",
    "\n",
    "\n",
    "# Initialize Data Module\n",
    "\n",
    "df[feature_names] = df[feature_names].fillna(method = 'ffill').fillna(0)\n",
    "valid[feature_names] = valid[feature_names].fillna(method = 'ffill').fillna(0)\n",
    "data_module = DataModule(df, batch_size=args.bs, valid_df=valid, accelerator=loader_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:58:18.798262Z",
     "iopub.status.busy": "2024-11-05T04:58:18.797992Z",
     "iopub.status.idle": "2024-11-05T04:58:18.818565Z",
     "shell.execute_reply": "2024-11-05T04:58:18.817654Z",
     "shell.execute_reply.started": "2024-11-05T04:58:18.798232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 402 K  | train\n",
      "---------------------------------------------\n",
      "402 K     Trainable params\n",
      "0         Non-trainable params\n",
      "402 K     Total params\n",
      "1.612     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fede/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 3/3 [00:01<00:00,  2.20it/s, v_num=15]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 0: 100%|██████████| 3/3 [00:01<00:00,  1.94it/s, v_num=15, val_r_square=0.950]Epoch 0: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13810.95703'}\n",
      "Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  2.01it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 1: 100%|██████████| 3/3 [00:01<00:00,  1.74it/s, v_num=15, val_r_square=0.950]Epoch 1: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13644.31348'}\n",
      "Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 2: 100%|██████████| 3/3 [00:01<00:00,  1.94it/s, v_num=15, val_r_square=0.950]Epoch 2: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13634.12207'}\n",
      "Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.44it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 3: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s, v_num=15, val_r_square=0.950]Epoch 3: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13632.93457'}\n",
      "Epoch 4: 100%|██████████| 3/3 [00:01<00:00,  2.38it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 4: 100%|██████████| 3/3 [00:01<00:00,  2.09it/s, v_num=15, val_r_square=0.950]Epoch 4: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13632.66211'}\n",
      "Epoch 5: 100%|██████████| 3/3 [00:01<00:00,  2.19it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 5: 100%|██████████| 3/3 [00:01<00:00,  1.96it/s, v_num=15, val_r_square=0.950]Epoch 5: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13632.51172'}\n",
      "Epoch 6: 100%|██████████| 3/3 [00:01<00:00,  2.37it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 6: 100%|██████████| 3/3 [00:01<00:00,  2.10it/s, v_num=15, val_r_square=0.950]Epoch 6: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13632.37695'}\n",
      "Epoch 7: 100%|██████████| 3/3 [00:01<00:00,  2.48it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 7: 100%|██████████| 3/3 [00:01<00:00,  2.19it/s, v_num=15, val_r_square=0.950]Epoch 7: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13632.15723'}\n",
      "Epoch 8: 100%|██████████| 3/3 [00:01<00:00,  2.40it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 8: 100%|██████████| 3/3 [00:01<00:00,  2.13it/s, v_num=15, val_r_square=0.950]Epoch 8: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13632.00000'}\n",
      "Epoch 9: 100%|██████████| 3/3 [00:01<00:00,  2.08it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 9: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s, v_num=15, val_r_square=0.950]Epoch 9: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13631.93262'}\n",
      "Epoch 10: 100%|██████████| 3/3 [00:01<00:00,  2.05it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 10: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, v_num=15, val_r_square=0.950]Epoch 10: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13631.93359'}\n",
      "Epoch 11: 100%|██████████| 3/3 [00:01<00:00,  1.95it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 11: 100%|██████████| 3/3 [00:01<00:00,  1.72it/s, v_num=15, val_r_square=0.950]Epoch 11: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13631.78320'}\n",
      "Epoch 12: 100%|██████████| 3/3 [00:01<00:00,  2.24it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 12: 100%|██████████| 3/3 [00:01<00:00,  1.95it/s, v_num=15, val_r_square=0.950]Epoch 12: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13631.69629'}\n",
      "Epoch 13: 100%|██████████| 3/3 [00:01<00:00,  2.53it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 13: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s, v_num=15, val_r_square=0.950]Epoch 13: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13631.70410'}\n",
      "Epoch 14: 100%|██████████| 3/3 [00:01<00:00,  2.73it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 14: 100%|██████████| 3/3 [00:01<00:00,  2.39it/s, v_num=15, val_r_square=0.950]Epoch 14: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13631.65625'}\n",
      "Epoch 15: 100%|██████████| 3/3 [00:01<00:00,  2.64it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 15: 100%|██████████| 3/3 [00:01<00:00,  2.34it/s, v_num=15, val_r_square=0.950]Epoch 15: {'val_loss': '8278.01758', 'val_r_square': '0.94975', 'train_loss': '13631.57031'}\n",
      "Epoch 16: 100%|██████████| 3/3 [00:01<00:00,  2.11it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 16: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, v_num=15, val_r_square=0.950]Epoch 16: {'val_loss': '8278.01953', 'val_r_square': '0.94975', 'train_loss': '13631.55859'}\n",
      "Epoch 17: 100%|██████████| 3/3 [00:01<00:00,  2.30it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 17: 100%|██████████| 3/3 [00:01<00:00,  2.01it/s, v_num=15, val_r_square=0.950]Epoch 17: {'val_loss': '8278.08691', 'val_r_square': '0.94958', 'train_loss': '13631.55762'}\n",
      "Epoch 18: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s, v_num=15, val_r_square=0.950]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 18: 100%|██████████| 3/3 [00:01<00:00,  1.93it/s, v_num=15, val_r_square=0.945]Epoch 18: {'val_loss': '8278.48242', 'val_r_square': '0.94539', 'train_loss': '13631.49805'}\n",
      "Epoch 19: 100%|██████████| 3/3 [00:01<00:00,  2.12it/s, v_num=15, val_r_square=0.945]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 19: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, v_num=15, val_r_square=0.928]Epoch 19: {'val_loss': '8278.98242', 'val_r_square': '0.92827', 'train_loss': '13631.52246'}\n",
      "Epoch 20: 100%|██████████| 3/3 [00:01<00:00,  2.21it/s, v_num=15, val_r_square=0.928]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 20: 100%|██████████| 3/3 [00:01<00:00,  1.98it/s, v_num=15, val_r_square=0.921]Epoch 20: {'val_loss': '8278.49609', 'val_r_square': '0.92099', 'train_loss': '13631.45410'}\n",
      "Epoch 21: 100%|██████████| 3/3 [00:01<00:00,  2.09it/s, v_num=15, val_r_square=0.921]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 21: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, v_num=15, val_r_square=0.919]Epoch 21: {'val_loss': '8276.31055', 'val_r_square': '0.91913', 'train_loss': '13631.44531'}\n",
      "Epoch 22: 100%|██████████| 3/3 [00:01<00:00,  2.26it/s, v_num=15, val_r_square=0.919]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 22: 100%|██████████| 3/3 [00:01<00:00,  1.96it/s, v_num=15, val_r_square=0.916]Epoch 22: {'val_loss': '8274.61035', 'val_r_square': '0.91648', 'train_loss': '13631.51660'}\n",
      "Epoch 23: 100%|██████████| 3/3 [00:01<00:00,  2.21it/s, v_num=15, val_r_square=0.916]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 23: 100%|██████████| 3/3 [00:01<00:00,  1.96it/s, v_num=15, val_r_square=0.915]Epoch 23: {'val_loss': '8273.82910', 'val_r_square': '0.91538', 'train_loss': '13631.42773'}\n",
      "Epoch 24: 100%|██████████| 3/3 [00:01<00:00,  2.43it/s, v_num=15, val_r_square=0.915]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 24: 100%|██████████| 3/3 [00:01<00:00,  2.08it/s, v_num=15, val_r_square=0.916]Epoch 24: {'val_loss': '8273.54883', 'val_r_square': '0.91574', 'train_loss': '13631.38281'}\n",
      "Epoch 25: 100%|██████████| 3/3 [00:01<00:00,  2.12it/s, v_num=15, val_r_square=0.916]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 25: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, v_num=15, val_r_square=0.917]Epoch 25: {'val_loss': '8273.45508', 'val_r_square': '0.91658', 'train_loss': '13631.42773'}\n",
      "Epoch 26: 100%|██████████| 3/3 [00:01<00:00,  2.52it/s, v_num=15, val_r_square=0.917]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 26: 100%|██████████| 3/3 [00:01<00:00,  2.22it/s, v_num=15, val_r_square=0.917]Epoch 26: {'val_loss': '8273.42969', 'val_r_square': '0.91728', 'train_loss': '13631.38281'}\n",
      "Epoch 27: 100%|██████████| 3/3 [00:01<00:00,  2.37it/s, v_num=15, val_r_square=0.917]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 27: 100%|██████████| 3/3 [00:01<00:00,  2.10it/s, v_num=15, val_r_square=0.918]Epoch 27: {'val_loss': '8273.42383', 'val_r_square': '0.91773', 'train_loss': '13631.39355'}\n",
      "Epoch 28: 100%|██████████| 3/3 [00:01<00:00,  2.53it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 28: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s, v_num=15, val_r_square=0.918]Epoch 28: {'val_loss': '8273.42285', 'val_r_square': '0.91799', 'train_loss': '13631.34863'}\n",
      "Epoch 29: 100%|██████████| 3/3 [00:01<00:00,  2.46it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 29: 100%|██████████| 3/3 [00:01<00:00,  2.17it/s, v_num=15, val_r_square=0.918]Epoch 29: {'val_loss': '8273.41797', 'val_r_square': '0.91821', 'train_loss': '13631.42578'}\n",
      "Epoch 30: 100%|██████████| 3/3 [00:01<00:00,  2.58it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 30: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s, v_num=15, val_r_square=0.918]Epoch 30: {'val_loss': '8273.41016', 'val_r_square': '0.91831', 'train_loss': '13631.33789'}\n",
      "Epoch 31: 100%|██████████| 3/3 [00:01<00:00,  2.23it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 31: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s, v_num=15, val_r_square=0.918]Epoch 31: {'val_loss': '8273.41016', 'val_r_square': '0.91844', 'train_loss': '13631.38281'}\n",
      "Epoch 32: 100%|██████████| 3/3 [00:01<00:00,  2.31it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 32: 100%|██████████| 3/3 [00:01<00:00,  2.05it/s, v_num=15, val_r_square=0.919]Epoch 32: {'val_loss': '8273.40820', 'val_r_square': '0.91850', 'train_loss': '13631.35938'}\n",
      "Epoch 33: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s, v_num=15, val_r_square=0.919]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 33: 100%|██████████| 3/3 [00:01<00:00,  2.01it/s, v_num=15, val_r_square=0.919]Epoch 33: {'val_loss': '8273.40625', 'val_r_square': '0.91855', 'train_loss': '13631.38672'}\n",
      "Epoch 34: 100%|██████████| 3/3 [00:01<00:00,  2.33it/s, v_num=15, val_r_square=0.919]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 34: 100%|██████████| 3/3 [00:01<00:00,  2.08it/s, v_num=15, val_r_square=0.918]Epoch 34: {'val_loss': '8273.40234', 'val_r_square': '0.91850', 'train_loss': '13631.27148'}\n",
      "Epoch 35: 100%|██████████| 3/3 [00:01<00:00,  2.69it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 35: 100%|██████████| 3/3 [00:01<00:00,  2.35it/s, v_num=15, val_r_square=0.918]Epoch 35: {'val_loss': '8273.40137', 'val_r_square': '0.91848', 'train_loss': '13631.39062'}\n",
      "Epoch 36: 100%|██████████| 3/3 [00:01<00:00,  2.31it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 36: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s, v_num=15, val_r_square=0.918]Epoch 36: {'val_loss': '8273.39648', 'val_r_square': '0.91843', 'train_loss': '13631.29883'}\n",
      "Epoch 37: 100%|██████████| 3/3 [00:01<00:00,  2.45it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 37: 100%|██████████| 3/3 [00:01<00:00,  2.14it/s, v_num=15, val_r_square=0.918]Epoch 37: {'val_loss': '8273.39648', 'val_r_square': '0.91843', 'train_loss': '13631.33398'}\n",
      "Epoch 38: 100%|██████████| 3/3 [00:01<00:00,  2.46it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 38: 100%|██████████| 3/3 [00:01<00:00,  2.18it/s, v_num=15, val_r_square=0.918]Epoch 38: {'val_loss': '8273.38770', 'val_r_square': '0.91832', 'train_loss': '13631.29590'}\n",
      "Epoch 39: 100%|██████████| 3/3 [00:01<00:00,  2.58it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 39: 100%|██████████| 3/3 [00:01<00:00,  2.26it/s, v_num=15, val_r_square=0.918]Epoch 39: {'val_loss': '8273.38574', 'val_r_square': '0.91832', 'train_loss': '13631.35156'}\n",
      "Epoch 40: 100%|██████████| 3/3 [00:01<00:00,  2.54it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 40: 100%|██████████| 3/3 [00:01<00:00,  2.21it/s, v_num=15, val_r_square=0.918]Epoch 40: {'val_loss': '8273.38086', 'val_r_square': '0.91826', 'train_loss': '13631.30371'}\n",
      "Epoch 41: 100%|██████████| 3/3 [00:01<00:00,  2.65it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 41: 100%|██████████| 3/3 [00:01<00:00,  2.29it/s, v_num=15, val_r_square=0.918]Epoch 41: {'val_loss': '8273.37598', 'val_r_square': '0.91822', 'train_loss': '13631.31738'}\n",
      "Epoch 42: 100%|██████████| 3/3 [00:01<00:00,  2.24it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 42: 100%|██████████| 3/3 [00:01<00:00,  1.97it/s, v_num=15, val_r_square=0.918]Epoch 42: {'val_loss': '8273.37500', 'val_r_square': '0.91820', 'train_loss': '13631.35547'}\n",
      "Epoch 43: 100%|██████████| 3/3 [00:01<00:00,  2.74it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 43: 100%|██████████| 3/3 [00:01<00:00,  2.39it/s, v_num=15, val_r_square=0.918]Epoch 43: {'val_loss': '8273.37500', 'val_r_square': '0.91823', 'train_loss': '13631.31055'}\n",
      "Epoch 44: 100%|██████████| 3/3 [00:01<00:00,  2.18it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 44: 100%|██████████| 3/3 [00:01<00:00,  1.94it/s, v_num=15, val_r_square=0.918]Epoch 44: {'val_loss': '8273.37109', 'val_r_square': '0.91816', 'train_loss': '13631.30273'}\n",
      "Epoch 45: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 45: 100%|██████████| 3/3 [00:01<00:00,  1.63it/s, v_num=15, val_r_square=0.918]Epoch 45: {'val_loss': '8273.36523', 'val_r_square': '0.91808', 'train_loss': '13631.26562'}\n",
      "Epoch 46: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 46: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s, v_num=15, val_r_square=0.918]Epoch 46: {'val_loss': '8273.36328', 'val_r_square': '0.91807', 'train_loss': '13631.34082'}\n",
      "Epoch 47: 100%|██████████| 3/3 [00:01<00:00,  2.31it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 47: 100%|██████████| 3/3 [00:01<00:00,  2.06it/s, v_num=15, val_r_square=0.918]Epoch 47: {'val_loss': '8273.36035', 'val_r_square': '0.91802', 'train_loss': '13631.33008'}\n",
      "Epoch 48: 100%|██████████| 3/3 [00:01<00:00,  2.32it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 48: 100%|██████████| 3/3 [00:01<00:00,  2.00it/s, v_num=15, val_r_square=0.918]Epoch 48: {'val_loss': '8273.35938', 'val_r_square': '0.91804', 'train_loss': '13631.26562'}\n",
      "Epoch 49: 100%|██████████| 3/3 [00:01<00:00,  2.28it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 49: 100%|██████████| 3/3 [00:01<00:00,  2.04it/s, v_num=15, val_r_square=0.918]Epoch 49: {'val_loss': '8273.35840', 'val_r_square': '0.91801', 'train_loss': '13631.27930'}\n",
      "Epoch 50: 100%|██████████| 3/3 [00:01<00:00,  2.81it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 50: 100%|██████████| 3/3 [00:01<00:00,  2.37it/s, v_num=15, val_r_square=0.918]Epoch 50: {'val_loss': '8273.35449', 'val_r_square': '0.91797', 'train_loss': '13631.28320'}\n",
      "Epoch 51: 100%|██████████| 3/3 [00:01<00:00,  2.49it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 51: 100%|██████████| 3/3 [00:01<00:00,  2.21it/s, v_num=15, val_r_square=0.918]Epoch 51: {'val_loss': '8273.35352', 'val_r_square': '0.91798', 'train_loss': '13631.31055'}\n",
      "Epoch 52: 100%|██████████| 3/3 [00:01<00:00,  2.39it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 52: 100%|██████████| 3/3 [00:01<00:00,  2.14it/s, v_num=15, val_r_square=0.918]Epoch 52: {'val_loss': '8273.35547', 'val_r_square': '0.91803', 'train_loss': '13631.32812'}\n",
      "Epoch 53: 100%|██████████| 3/3 [00:01<00:00,  2.44it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 53: 100%|██████████| 3/3 [00:01<00:00,  2.06it/s, v_num=15, val_r_square=0.918]Epoch 53: {'val_loss': '8273.35352', 'val_r_square': '0.91801', 'train_loss': '13631.35645'}\n",
      "Epoch 54: 100%|██████████| 3/3 [00:01<00:00,  2.40it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 54: 100%|██████████| 3/3 [00:01<00:00,  2.14it/s, v_num=15, val_r_square=0.918]Epoch 54: {'val_loss': '8273.35059', 'val_r_square': '0.91795', 'train_loss': '13631.31055'}\n",
      "Epoch 55: 100%|██████████| 3/3 [00:01<00:00,  2.50it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 55: 100%|██████████| 3/3 [00:01<00:00,  2.23it/s, v_num=15, val_r_square=0.918]Epoch 55: {'val_loss': '8273.34863', 'val_r_square': '0.91793', 'train_loss': '13631.30664'}\n",
      "Epoch 56: 100%|██████████| 3/3 [00:01<00:00,  2.91it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 56: 100%|██████████| 3/3 [00:01<00:00,  2.55it/s, v_num=15, val_r_square=0.918]Epoch 56: {'val_loss': '8273.35059', 'val_r_square': '0.91796', 'train_loss': '13631.30371'}\n",
      "Epoch 57: 100%|██████████| 3/3 [00:01<00:00,  2.50it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 57: 100%|██████████| 3/3 [00:01<00:00,  2.22it/s, v_num=15, val_r_square=0.918]Epoch 57: {'val_loss': '8273.34961', 'val_r_square': '0.91795', 'train_loss': '13631.29590'}\n",
      "Epoch 58: 100%|██████████| 3/3 [00:01<00:00,  2.62it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 58: 100%|██████████| 3/3 [00:01<00:00,  2.32it/s, v_num=15, val_r_square=0.918]Epoch 58: {'val_loss': '8273.34961', 'val_r_square': '0.91792', 'train_loss': '13631.28906'}\n",
      "Epoch 59: 100%|██████████| 3/3 [00:01<00:00,  2.72it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 59: 100%|██████████| 3/3 [00:01<00:00,  2.41it/s, v_num=15, val_r_square=0.918]Epoch 59: {'val_loss': '8273.34961', 'val_r_square': '0.91794', 'train_loss': '13631.34570'}\n",
      "Epoch 60: 100%|██████████| 3/3 [00:01<00:00,  2.44it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 60: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s, v_num=15, val_r_square=0.918]Epoch 60: {'val_loss': '8273.35156', 'val_r_square': '0.91796', 'train_loss': '13631.27441'}\n",
      "Epoch 61: 100%|██████████| 3/3 [00:01<00:00,  2.43it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 61: 100%|██████████| 3/3 [00:01<00:00,  2.17it/s, v_num=15, val_r_square=0.918]Epoch 61: {'val_loss': '8273.35059', 'val_r_square': '0.91793', 'train_loss': '13631.35254'}\n",
      "Epoch 62: 100%|██████████| 3/3 [00:01<00:00,  2.62it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 62: 100%|██████████| 3/3 [00:01<00:00,  2.31it/s, v_num=15, val_r_square=0.918]Epoch 62: {'val_loss': '8273.34961', 'val_r_square': '0.91795', 'train_loss': '13631.33008'}\n",
      "Epoch 63: 100%|██████████| 3/3 [00:01<00:00,  2.19it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 63: 100%|██████████| 3/3 [00:01<00:00,  1.94it/s, v_num=15, val_r_square=0.918]Epoch 63: {'val_loss': '8273.34863', 'val_r_square': '0.91792', 'train_loss': '13631.25000'}\n",
      "Epoch 64: 100%|██████████| 3/3 [00:01<00:00,  2.46it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 64: 100%|██████████| 3/3 [00:01<00:00,  2.18it/s, v_num=15, val_r_square=0.918]Epoch 64: {'val_loss': '8273.34766', 'val_r_square': '0.91791', 'train_loss': '13631.33496'}\n",
      "Epoch 65: 100%|██████████| 3/3 [00:01<00:00,  2.77it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 65: 100%|██████████| 3/3 [00:01<00:00,  2.42it/s, v_num=15, val_r_square=0.918]Epoch 65: {'val_loss': '8273.34473', 'val_r_square': '0.91782', 'train_loss': '13631.29492'}\n",
      "Epoch 66: 100%|██████████| 3/3 [00:01<00:00,  2.62it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 66: 100%|██████████| 3/3 [00:01<00:00,  2.28it/s, v_num=15, val_r_square=0.918]Epoch 66: {'val_loss': '8273.34570', 'val_r_square': '0.91786', 'train_loss': '13631.27441'}\n",
      "Epoch 67: 100%|██████████| 3/3 [00:01<00:00,  2.47it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 67: 100%|██████████| 3/3 [00:01<00:00,  2.17it/s, v_num=15, val_r_square=0.918]Epoch 67: {'val_loss': '8273.34473', 'val_r_square': '0.91786', 'train_loss': '13631.25391'}\n",
      "Epoch 68: 100%|██████████| 3/3 [00:01<00:00,  2.40it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 68: 100%|██████████| 3/3 [00:01<00:00,  2.13it/s, v_num=15, val_r_square=0.918]Epoch 68: {'val_loss': '8273.34668', 'val_r_square': '0.91791', 'train_loss': '13631.31934'}\n",
      "Epoch 69: 100%|██████████| 3/3 [00:01<00:00,  2.54it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 69: 100%|██████████| 3/3 [00:01<00:00,  2.25it/s, v_num=15, val_r_square=0.918]Epoch 69: {'val_loss': '8273.34668', 'val_r_square': '0.91791', 'train_loss': '13631.27832'}\n",
      "Epoch 70: 100%|██████████| 3/3 [00:01<00:00,  2.72it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 70: 100%|██████████| 3/3 [00:01<00:00,  2.36it/s, v_num=15, val_r_square=0.918]Epoch 70: {'val_loss': '8273.34570', 'val_r_square': '0.91790', 'train_loss': '13631.29004'}\n",
      "Epoch 71: 100%|██████████| 3/3 [00:01<00:00,  2.62it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 71: 100%|██████████| 3/3 [00:01<00:00,  2.31it/s, v_num=15, val_r_square=0.918]Epoch 71: {'val_loss': '8273.34473', 'val_r_square': '0.91787', 'train_loss': '13631.26465'}\n",
      "Epoch 72: 100%|██████████| 3/3 [00:01<00:00,  2.65it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 72: 100%|██████████| 3/3 [00:01<00:00,  2.30it/s, v_num=15, val_r_square=0.918]Epoch 72: {'val_loss': '8273.34375', 'val_r_square': '0.91788', 'train_loss': '13631.27637'}\n",
      "Epoch 73: 100%|██████████| 3/3 [00:01<00:00,  2.33it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 73: 100%|██████████| 3/3 [00:01<00:00,  2.07it/s, v_num=15, val_r_square=0.918]Epoch 73: {'val_loss': '8273.34473', 'val_r_square': '0.91787', 'train_loss': '13631.34766'}\n",
      "Epoch 74: 100%|██████████| 3/3 [00:01<00:00,  2.50it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 74: 100%|██████████| 3/3 [00:01<00:00,  2.20it/s, v_num=15, val_r_square=0.918]Epoch 74: {'val_loss': '8273.34473', 'val_r_square': '0.91788', 'train_loss': '13631.32715'}\n",
      "Epoch 75: 100%|██████████| 3/3 [00:01<00:00,  2.33it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 75: 100%|██████████| 3/3 [00:01<00:00,  2.08it/s, v_num=15, val_r_square=0.918]Epoch 75: {'val_loss': '8273.34570', 'val_r_square': '0.91791', 'train_loss': '13631.30371'}\n",
      "Epoch 76: 100%|██████████| 3/3 [00:01<00:00,  2.76it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 76: 100%|██████████| 3/3 [00:01<00:00,  2.42it/s, v_num=15, val_r_square=0.918]Epoch 76: {'val_loss': '8273.34668', 'val_r_square': '0.91789', 'train_loss': '13631.31445'}\n",
      "Epoch 77: 100%|██████████| 3/3 [00:01<00:00,  2.81it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 77: 100%|██████████| 3/3 [00:01<00:00,  2.43it/s, v_num=15, val_r_square=0.918]Epoch 77: {'val_loss': '8273.34570', 'val_r_square': '0.91788', 'train_loss': '13631.33887'}\n",
      "Epoch 78: 100%|██████████| 3/3 [00:01<00:00,  2.69it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 78: 100%|██████████| 3/3 [00:01<00:00,  2.34it/s, v_num=15, val_r_square=0.918]Epoch 78: {'val_loss': '8273.34668', 'val_r_square': '0.91791', 'train_loss': '13631.19629'}\n",
      "Epoch 79: 100%|██████████| 3/3 [00:01<00:00,  2.73it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 79: 100%|██████████| 3/3 [00:01<00:00,  2.39it/s, v_num=15, val_r_square=0.918]Epoch 79: {'val_loss': '8273.34668', 'val_r_square': '0.91792', 'train_loss': '13631.34570'}\n",
      "Epoch 80: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 80: 100%|██████████| 3/3 [00:01<00:00,  1.98it/s, v_num=15, val_r_square=0.918]Epoch 80: {'val_loss': '8273.34473', 'val_r_square': '0.91791', 'train_loss': '13631.31641'}\n",
      "Epoch 81: 100%|██████████| 3/3 [00:01<00:00,  2.26it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 81: 100%|██████████| 3/3 [00:01<00:00,  2.01it/s, v_num=15, val_r_square=0.918]Epoch 81: {'val_loss': '8273.34766', 'val_r_square': '0.91795', 'train_loss': '13631.27441'}\n",
      "Epoch 82: 100%|██████████| 3/3 [00:01<00:00,  2.22it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 82: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, v_num=15, val_r_square=0.918]Epoch 82: {'val_loss': '8273.34668', 'val_r_square': '0.91792', 'train_loss': '13631.32422'}\n",
      "Epoch 83: 100%|██████████| 3/3 [00:01<00:00,  2.40it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 83: 100%|██████████| 3/3 [00:01<00:00,  2.12it/s, v_num=15, val_r_square=0.918]Epoch 83: {'val_loss': '8273.34766', 'val_r_square': '0.91793', 'train_loss': '13631.24316'}\n",
      "Epoch 84: 100%|██████████| 3/3 [00:01<00:00,  2.16it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 84: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, v_num=15, val_r_square=0.918]Epoch 84: {'val_loss': '8273.34570', 'val_r_square': '0.91791', 'train_loss': '13631.33691'}\n",
      "Epoch 85: 100%|██████████| 3/3 [00:01<00:00,  2.18it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 85: 100%|██████████| 3/3 [00:01<00:00,  1.95it/s, v_num=15, val_r_square=0.918]Epoch 85: {'val_loss': '8273.34668', 'val_r_square': '0.91792', 'train_loss': '13631.34863'}\n",
      "Epoch 86: 100%|██████████| 3/3 [00:01<00:00,  2.40it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 86: 100%|██████████| 3/3 [00:01<00:00,  2.14it/s, v_num=15, val_r_square=0.918]Epoch 86: {'val_loss': '8273.34863', 'val_r_square': '0.91794', 'train_loss': '13631.29980'}\n",
      "Epoch 87: 100%|██████████| 3/3 [00:01<00:00,  2.46it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 87: 100%|██████████| 3/3 [00:01<00:00,  2.08it/s, v_num=15, val_r_square=0.918]Epoch 87: {'val_loss': '8273.34863', 'val_r_square': '0.91797', 'train_loss': '13631.40137'}\n",
      "Epoch 88: 100%|██████████| 3/3 [00:01<00:00,  1.98it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 88: 100%|██████████| 3/3 [00:01<00:00,  1.75it/s, v_num=15, val_r_square=0.918]Epoch 88: {'val_loss': '8273.34863', 'val_r_square': '0.91794', 'train_loss': '13631.26465'}\n",
      "Epoch 89: 100%|██████████| 3/3 [00:01<00:00,  2.70it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 89: 100%|██████████| 3/3 [00:01<00:00,  2.37it/s, v_num=15, val_r_square=0.918]Epoch 89: {'val_loss': '8273.34570', 'val_r_square': '0.91790', 'train_loss': '13631.23242'}\n",
      "Epoch 90: 100%|██████████| 3/3 [00:01<00:00,  2.24it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 90: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, v_num=15, val_r_square=0.918]Epoch 90: {'val_loss': '8273.34766', 'val_r_square': '0.91790', 'train_loss': '13631.29297'}\n",
      "Epoch 91: 100%|██████████| 3/3 [00:01<00:00,  2.34it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 91: 100%|██████████| 3/3 [00:01<00:00,  2.08it/s, v_num=15, val_r_square=0.918]Epoch 91: {'val_loss': '8273.34473', 'val_r_square': '0.91786', 'train_loss': '13631.31934'}\n",
      "Epoch 92: 100%|██████████| 3/3 [00:01<00:00,  2.47it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 92: 100%|██████████| 3/3 [00:01<00:00,  2.17it/s, v_num=15, val_r_square=0.918]Epoch 92: {'val_loss': '8273.34277', 'val_r_square': '0.91783', 'train_loss': '13631.29004'}\n",
      "Epoch 93: 100%|██████████| 3/3 [00:01<00:00,  2.44it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 93: 100%|██████████| 3/3 [00:01<00:00,  2.16it/s, v_num=15, val_r_square=0.918]Epoch 93: {'val_loss': '8273.34180', 'val_r_square': '0.91783', 'train_loss': '13631.31055'}\n",
      "Epoch 94: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 94: 100%|██████████| 3/3 [00:01<00:00,  1.97it/s, v_num=15, val_r_square=0.918]Epoch 94: {'val_loss': '8273.34277', 'val_r_square': '0.91785', 'train_loss': '13631.28906'}\n",
      "Epoch 95: 100%|██████████| 3/3 [00:01<00:00,  2.14it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 95: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, v_num=15, val_r_square=0.918]Epoch 95: {'val_loss': '8273.34375', 'val_r_square': '0.91788', 'train_loss': '13631.29492'}\n",
      "Epoch 96: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 96: 100%|██████████| 3/3 [00:01<00:00,  1.56it/s, v_num=15, val_r_square=0.918]Epoch 96: {'val_loss': '8273.34375', 'val_r_square': '0.91788', 'train_loss': '13631.31641'}\n",
      "Epoch 97: 100%|██████████| 3/3 [00:01<00:00,  1.91it/s, v_num=15, val_r_square=0.918]y_true nans:  0\n",
      "y_pred nans:  0\n",
      "Epoch 97: 100%|██████████| 3/3 [00:01<00:00,  1.68it/s, v_num=15, val_r_square=0.918]Epoch 97: {'val_loss': '8273.34570', 'val_r_square': '0.91789', 'train_loss': '13631.24316'}\n",
      "Epoch 98:  33%|███▎      | 1/3 [00:00<00:01,  1.19it/s, v_num=15, val_r_square=0.918]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/torch/optim/adam.py:225\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03mhook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:241\u001b[39m, in \u001b[36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[39m\u001b[34m(loss)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward_fn\u001b[39m(loss: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackward\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:213\u001b[39m, in \u001b[36mStrategy.backward\u001b[39m\u001b[34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.pre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.post_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:73\u001b[39m, in \u001b[36mPrecision.backward\u001b[39m\u001b[34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[32m     63\u001b[39m \n\u001b[32m     64\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1097\u001b[39m, in \u001b[36mLightningModule.backward\u001b[39m\u001b[34m(self, loss, *args, **kwargs)\u001b[39m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1097\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     27\u001b[39m trainer = Trainer(\n\u001b[32m     28\u001b[39m     max_epochs=args.max_epochs,\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m#accelerator=accelerator,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     enable_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     34\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Start Training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# You can find trained best model in your local path\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFold-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Training completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimer.time_elapsed(\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/labo3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "pl.seed_everything(args.seed)\n",
    "for fold in range(args.N_fold):\n",
    "    data_module.setup(fold, args.N_fold)\n",
    "    # Obtain input dimension\n",
    "    input_dim = data_module.train_dataset.features.shape[1]\n",
    "    # Initialize Model\n",
    "    model = NN(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=args.n_hidden,\n",
    "        dropouts=args.dropouts,\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "    # Initialize Logger\n",
    "    if args.use_wandb:\n",
    "        wandb_run = wandb.init(project=args.project, config=vars(args), reinit=True)\n",
    "        logger = WandbLogger(experiment=wandb_run)\n",
    "    else:\n",
    "        logger = None\n",
    "    # Initialize Callbacks\n",
    "    early_stopping = EarlyStopping('val_loss', patience=args.patience, mode='min', verbose=False)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, verbose=False, filename=f\"./models/nn_{fold}.model\") \n",
    "    timer = Timer()\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=args.max_epochs,\n",
    "        accelerator=accelerator,\n",
    "        devices=[args.gpuid] if args.usegpu else None,\n",
    "        logger=logger,\n",
    "        callbacks=[early_stopping, checkpoint_callback, timer],\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "    # Start Training\n",
    "    trainer.fit(model, data_module.train_dataloader(args.loader_workers), data_module.val_dataloader(args.loader_workers))\n",
    "    # You can find trained best model in your local path\n",
    "    print(f'Fold-{fold} Training completed in {timer.time_elapsed(\"train\"):.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    },
    {
     "sourceId": 203900450,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "labo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.594014,
   "end_time": "2024-10-10T11:58:36.355301",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-10T11:58:28.761287",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
