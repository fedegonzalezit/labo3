{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, List, Optional, Callable, Tuple, Self\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "from flaml import AutoML\n",
    "import os\n",
    "from glob import glob\n",
    "import inspect\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)        \n",
    "import cloudpickle\n",
    "import hashlib\n",
    "import inspect\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "def fallback_latest_notebook():\n",
    "    notebooks = glob(\"*.ipynb\")\n",
    "    if not notebooks:\n",
    "        return None\n",
    "    notebooks = sorted(notebooks, key=os.path.getmtime, reverse=True)\n",
    "    return notebooks[0]\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "class InDiskCacheWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper class to enable in-disk caching for pipeline steps.\n",
    "    It uses the InDiskCache class to cache artifacts on disk.\n",
    "    \"\"\"\n",
    "    def __init__(self, step: \"PipelineStep\", cache_dir: str = \".cache\", execute_params: Optional[Dict[str, Any]] = None):\n",
    "        self.step = step\n",
    "        self.cache_dir = os.path.join(cache_dir, step.name)\n",
    "        if not os.path.exists(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "        self._execute_params = execute_params or {}\n",
    "\n",
    "    def execute(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"if the step has a cache, it hashes the parameters and checks if the result is already cached.\n",
    "        note that params could be any object, so it uses cloudpickle to serialize them.\n",
    "        If the result is cached, it returns the cached result.\n",
    "        If not, it executes the step and saves the result in the cache.\n",
    "        \"\"\"\n",
    "        # Bind args/kwargs to parameter names using original signature\n",
    "        bound = inspect.signature(self.step.execute).bind(*args, **kwargs)\n",
    "        #bound.apply_defaults()\n",
    "\n",
    "        # also checks que values from __init__ for the hash\n",
    "        init_params = self.step.__dict__.copy()\n",
    "        # si los parametros con los que se inicializo cambiaron entonces deberia missear el cache\n",
    "        bound.apply_defaults()\n",
    "\n",
    "        # Serialize input arguments with cloudpickle\n",
    "        try:\n",
    "            serialized = cloudpickle.dumps(bound.arguments)\n",
    "            # Include init parameters in the serialization\n",
    "            serialized += cloudpickle.dumps(init_params)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to serialize for cache: {e}\")\n",
    "\n",
    "        # Generate a hash key from inputs\n",
    "        hash_key = hashlib.sha256(serialized).hexdigest()\n",
    "        cache_file = os.path.join(self.cache_dir, f\"{hash_key}.pkl\")\n",
    "\n",
    "        # Load from cache or compute and save\n",
    "        if os.path.exists(cache_file):\n",
    "            print(f\"Loading cached result for {self.step.name} from {cache_file}\")\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            print(f\"Cache miss for {self.step.name}, executing step and saving result to {cache_file}\")\n",
    "            result = self.step.execute(*args, **kwargs)\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(result, f)\n",
    "            return result\n",
    "\n",
    "    def get_execute_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the parameters for the execute method of the wrapped step.\n",
    "        \"\"\"\n",
    "        return self._execute_params\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the name of the step.\n",
    "        \"\"\"\n",
    "        return self.step.name\n",
    "    \n",
    "\n",
    "class InMemoryCacheWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper class to enable in-memory caching for pipeline steps.\n",
    "    It uses the InMemoryCache class to cache artifacts in memory.\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    \n",
    "    def __init__(self, step: \"PipelineStep\", execute_params: Optional[Dict[str, Any]] = None):\n",
    "        self.step = step\n",
    "        self._execute_params = execute_params or {}\n",
    "\n",
    "    def execute(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Execute the step and cache the result in memory.\"\"\"\n",
    "        # Bind args/kwargs to parameter names using original signature\n",
    "        bound = inspect.signature(self.step.execute).bind(*args, **kwargs)\n",
    "\n",
    "        init_params = self.step.__dict__.copy()\n",
    "        # Merge init parameters with execute parameters\n",
    "        bound.arguments.update(init_params)\n",
    "        bound.apply_defaults()\n",
    "\n",
    "        # Serialize input arguments with cloudpickle\n",
    "        try:\n",
    "            serialized = cloudpickle.dumps(bound.arguments)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to serialize for cache: {e}\")\n",
    "\n",
    "        # Generate a hash key from inputs\n",
    "        hash_key = hashlib.sha256(serialized).hexdigest()\n",
    "\n",
    "        # Load from cache or compute and save\n",
    "        if hash_key in self.cache:\n",
    "            print(f\"Loading cached result for {self.step.name} from memory\")\n",
    "            return self.cache[hash_key]\n",
    "        else:\n",
    "            print(f\"Cache miss for {self.step.name}, executing step and saving result in memory\")\n",
    "            result = self.step.execute(*args, **kwargs)\n",
    "            self.cache[hash_key] = result\n",
    "            return result\n",
    "\n",
    "    def get_execute_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the parameters for the execute method of the wrapped step.\n",
    "        \"\"\"\n",
    "        return self._execute_params\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the name of the step.\n",
    "        \"\"\"\n",
    "        return self.step.name\n",
    "    \n",
    "\n",
    "class CachedPipelineMixin:\n",
    "    def in_disk_cache(self, cache_dir: str = \".cache\") -> Self:\n",
    "        \"\"\"\n",
    "        It activate the in-disk cache using the InDisKCache class. returns the step itself.\n",
    "        Args:\n",
    "            cache_dir (str): Directory where the cache will be stored.\n",
    "        \"\"\"\n",
    "        execute_params = self.get_execute_params()\n",
    "        return InDiskCacheWrapper(self, cache_dir=cache_dir, execute_params=execute_params)\n",
    "    \n",
    "    def in_memory_cache(self) -> Self:\n",
    "        \"\"\"\n",
    "        It activate the in-memory cache using the InMemoryCache class. returns the step itself.\n",
    "        \"\"\"\n",
    "        execute_params = self.get_execute_params()\n",
    "        return InMemoryCacheWrapper(self, execute_params=execute_params)\n",
    "    \n",
    "\n",
    "class PipelineStep(ABC, CachedPipelineMixin):\n",
    "    \"\"\"\n",
    "    Abstract base class for pipeline steps.\n",
    "    Each step in the pipeline must inherit from this class and implement the execute method.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize a pipeline step.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the step for identification and logging purposes.\n",
    "        \"\"\"\n",
    "        self._name = name or self.__class__.__name__\n",
    "\n",
    "    @abstractmethod\n",
    "    def execute(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Execute the pipeline step.\n",
    "    \n",
    "        Args:\n",
    "            pipeline (Pipeline): The pipeline instance that contains this step.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save_artifact(self, pipeline: \"Pipeline\", artifact_name: str, artifact: Any) -> None:\n",
    "        \"\"\"\n",
    "        Save an artifact produced by this step to the pipeline.\n",
    "\n",
    "        Args:\n",
    "            pipeline (Pipeline): The pipeline instance.\n",
    "            artifact_name (str): Name to identify the artifact.\n",
    "            artifact (Any): The artifact to save.\n",
    "        \"\"\"\n",
    "        pipeline.save_artifact(artifact_name, artifact)\n",
    "\n",
    "    def get_artifact(self, pipeline: \"Pipeline\", artifact_name: str, default=None, raise_not_found=True) -> Any:\n",
    "        \"\"\"\n",
    "        Retrieve a stored artifact from the pipeline.\n",
    "\n",
    "        Args:\n",
    "            pipeline (Pipeline): The pipeline instance.\n",
    "            artifact_name (str): Name of the artifact to retrieve.\n",
    "            default: Default value to return if the artifact is not found.\n",
    "            raise_not_found (bool): Whether to raise an error if the artifact is not found.\n",
    "\n",
    "        Returns:\n",
    "            Any: The requested artifact or default value.\n",
    "        \"\"\"\n",
    "        return pipeline.get_artifact(artifact_name, default=default, raise_not_found=raise_not_found)\n",
    "    \n",
    "    def del_artifact(self, pipeline: \"Pipeline\", artifact_name: str, soft=True) -> None:\n",
    "        \"\"\"\n",
    "        Delete a stored artifact from the pipeline and free memory.\n",
    "\n",
    "        Args:\n",
    "            pipeline (Pipeline): The pipeline instance.\n",
    "            artifact_name (str): Name of the artifact to delete.\n",
    "            soft (bool): If True, performs a soft delete; if False, forces garbage collection.\n",
    "        \"\"\"\n",
    "        pipeline.del_artifact(artifact_name, soft=soft)\n",
    "\n",
    "    def get_execute_params(self):\n",
    "        sig = inspect.signature(self.execute)\n",
    "        return sig.parameters\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @name.setter\n",
    "    def name(self, value):\n",
    "        self._name = value\n",
    "    \n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline class that manages the execution of steps and storage of artifacts.\n",
    "    \"\"\"\n",
    "    def __init__(self, steps: Optional[List[PipelineStep]] = None, optimize_arftifacts_memory: bool = True, needs=None):\n",
    "        \"\"\"Initialize the pipeline.\"\"\"\n",
    "        self.steps: List[PipelineStep] = steps if steps is not None else []\n",
    "        self.artifacts: Dict[str, Any] = {}\n",
    "        self.optimize_arftifacts_memory = optimize_arftifacts_memory\n",
    "        self.needs = needs or []\n",
    "        self.finished = False\n",
    "\n",
    "    def add_step(self, step: PipelineStep, position: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Add a new step to the pipeline.\n",
    "\n",
    "        Args:\n",
    "            step (PipelineStep): The step to add.\n",
    "            position (Optional[int]): Position where to insert the step. If None, appends to the end.\n",
    "        \"\"\"\n",
    "        if position is not None:\n",
    "            self.steps.insert(position, step)\n",
    "        else:\n",
    "            self.steps.append(step)\n",
    "\n",
    "    def save_artifact(self, artifact_name: str, artifact: Any) -> None:\n",
    "        \"\"\"\n",
    "        Save an artifact from a given step.\n",
    "\n",
    "        Args:\n",
    "            artifact_name (str): Name to identify the artifact.\n",
    "            artifact (Any): The artifact to save.\n",
    "        \"\"\"\n",
    "        if not self.optimize_arftifacts_memory:\n",
    "            self.artifacts[artifact_name] = artifact\n",
    "        else:\n",
    "            # guarda el artifact en /tmp/ para no guardarlo en memoria\n",
    "            if not os.path.exists(\"/tmp/\"):\n",
    "                os.makedirs(\"/tmp/\")\n",
    "            artifact_path = os.path.join(\"/tmp/\", artifact_name)\n",
    "            with open(artifact_path, 'wb') as f:\n",
    "                pickle.dump(artifact, f)\n",
    "            self.artifacts[artifact_name] = artifact_path\n",
    "\n",
    "    def get_artifact(self, artifact_name: str, default=None, raise_not_found=True) -> Any:\n",
    "        \"\"\"\n",
    "        Retrieve a stored artifact.\n",
    "\n",
    "        Args:\n",
    "            artifact_name (str): Name of the artifact to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Any: The requested artifact.\n",
    "        \"\"\"\n",
    "        if not self.optimize_arftifacts_memory:\n",
    "            return self.artifacts.get(artifact_name)\n",
    "        else:\n",
    "            artifact_path = self.artifacts.get(artifact_name)\n",
    "            if artifact_path and os.path.exists(artifact_path):\n",
    "                with open(artifact_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            else:\n",
    "                if raise_not_found:\n",
    "                    raise FileNotFoundError(f\"Artifact {artifact_name} not found in /tmp/\")\n",
    "                return default\n",
    "    \n",
    "    def del_artifact(self, artifact_name: str, soft=True) -> None:\n",
    "        \"\"\"\n",
    "        Delete a stored artifact and free memory.\n",
    "\n",
    "        Args:\n",
    "            artifact_name (str): Name of the artifact to delete.\n",
    "        \"\"\"\n",
    "        del self.artifacts[artifact_name]\n",
    "        if not soft:\n",
    "            # Force garbage collection if not soft delete\n",
    "            gc.collect()\n",
    "    \n",
    "    \n",
    "    def run(self, verbose: bool = True, last_step_callback: Callable = None) -> None:\n",
    "        \"\"\"\n",
    "        Execute all steps in sequence and log execution time.\n",
    "        \"\"\"        \n",
    "        \n",
    "        # Run steps from the last completed step\n",
    "        if self.finished:\n",
    "            if verbose:\n",
    "                print(\"Pipeline has already finished. Skipping execution.\")\n",
    "            return\n",
    "        \n",
    "        for step in self.steps:\n",
    "            if verbose:\n",
    "                print(f\"Executing step: {step.name}\")\n",
    "            start_time = time.time()\n",
    "            params = self.__fill_params_from_step(step)\n",
    "            artifacts_to_save = step.execute(**params)\n",
    "            if artifacts_to_save is None:\n",
    "                artifacts_to_save = {}\n",
    "            self.__save_step_artifacts(artifacts_to_save)\n",
    "            end_time = time.time()\n",
    "            if verbose:\n",
    "                print(f\"Step {step.name} completed in {end_time - start_time:.2f} seconds\")\n",
    "        self.finished = True\n",
    "\n",
    "    def __fill_params_from_step(self, step: PipelineStep) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Obtiene los nombres de los parametros de la implementacion de la funcion execute del paso. (excepto el pipeline el cual es obligatorio)\n",
    "        luego obtengo todos los artefactos del pipeline y los paso como parametros al paso.\n",
    "        \"\"\"\n",
    "        step_params = step.get_execute_params()\n",
    "        params = {}\n",
    "        for name, param in step_params.items():\n",
    "            if name == 'pipeline':\n",
    "                params[name] = self\n",
    "            elif param.default is inspect.Parameter.empty:\n",
    "                params[name] = self.get_artifact(name)\n",
    "            else:\n",
    "                params[name] = self.get_artifact(name, default=param.default, raise_not_found=False)\n",
    "        return params\n",
    "\n",
    "    \n",
    "\n",
    "    def __save_step_artifacts(self, artifacts_to_save: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Save artifacts produced by a step to the pipeline.\n",
    "\n",
    "        Args:\n",
    "            artifacts_to_save (Dict[str, Any]): Artifacts to save.\n",
    "        \"\"\"\n",
    "\n",
    "        for name, artifact in artifacts_to_save.items():\n",
    "            self.save_artifact(name, artifact)\n",
    "\n",
    "\n",
    "\n",
    "    def clear(self, collect_garbage: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Clean up all artifacts and free memory.\n",
    "        \"\"\"\n",
    "        if collect_garbage:\n",
    "            del self.artifacts\n",
    "            gc.collect()\n",
    "        self.artifacts = {}\n",
    "        self.last_step = None\n",
    "        self.finished = False\n",
    "\n",
    "# LoadDataFrameFromPickleStep\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import pickle\n",
    "\n",
    "class LoadDataFrameStep(PipelineStep):\n",
    "    \"\"\"\n",
    "    Example step that loads a DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.path = path\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        df = pd.read_parquet(self.path)\n",
    "        df = df.drop(columns=[\"periodo\"], errors='ignore')\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class LoadDataFrameFromPickleStep(PipelineStep):\n",
    "    \"\"\"\n",
    "    Example step that loads a DataFrame from a pickle file.\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.path = path\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        df = pd.read_pickle(self.path)\n",
    "        return {\"df\": df}\n",
    "    \n",
    "    \n",
    "class LoadScalerStep(PipelineStep):\n",
    "    def __init__(self, artifact_name: str, file_name: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.file_name = file_name\n",
    "        self.artifact_name = artifact_name\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "        Carga un scaler previamente guardado desde un archivo.\n",
    "        \"\"\"\n",
    "        with open(self.file_name, \"rb\") as f:\n",
    "            scaler = pickle.load(f)\n",
    "        return {self.artifact_name: scaler}   \n",
    "\n",
    "# SplitDataFrameStep\n",
    "from typing import Optional, Dict\n",
    "import pandas as pd\n",
    "\n",
    "class SplitDataFrameStep(PipelineStep):\n",
    "    def __init__(\n",
    "            self, \n",
    "            test_date=\"2019-12\", \n",
    "            df=\"df\", \n",
    "            gap=0,\n",
    "            name: Optional[str] = None\n",
    "        ):\n",
    "        super().__init__(name)\n",
    "        self.test_date = test_date\n",
    "        self.df = df\n",
    "        self.gap = gap \n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        df = pipeline.get_artifact(self.df)\n",
    "        test_df = df[df[\"fecha\"] == self.test_date]\n",
    "        train_df = df[df[\"fecha\"] < self.test_date]\n",
    "        last_train_date = train_df[\"fecha\"].max()\n",
    "        if isinstance(last_train_date, pd.Period):\n",
    "            last_train_date = last_train_date.to_timestamp()\n",
    "        gap_date = pd.to_datetime(last_train_date) - pd.DateOffset(months=self.gap)\n",
    "        # Convert gap_date to Period with same freq as fecha\n",
    "        if pd.api.types.is_period_dtype(df[\"fecha\"]):\n",
    "            gap_date = pd.Period(gap_date, freq=df[\"fecha\"].dt.freq)\n",
    "        train_df = train_df[train_df[\"fecha\"] < gap_date]\n",
    "        return {\n",
    "            \"train_index\": train_df.index,\n",
    "            \"test_index\": test_df.index\n",
    "        }\n",
    "\n",
    "\n",
    "class PrepareXYStep(PipelineStep):\n",
    "    def execute(self, df, train_index, test_index) -> None:\n",
    "        columns = df.columns\n",
    "        #features = [col for col in columns if col != \"fecha\" and \"target\" not in col]\n",
    "        features = [col for col in columns if col != \"fecha\" and \"target\" not in col]\n",
    "        targets = [col for col in columns if \"target\" in col]\n",
    "        X_train = df.loc[train_index][features]\n",
    "        y_train = df.loc[train_index][targets]\n",
    "        X_test = df.loc[test_index][features]\n",
    "        y_test = df.loc[test_index][targets]\n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"targets\": targets,\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "\n",
    "\n",
    "class CreateTargetColumStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2)    \n",
    "        return {\"df\": df, \"target_col\": self.target_col}\n",
    "    \n",
    "\n",
    "class CreateMultiDiffTargetColumStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target_1'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-1) - df[self.target_col]\n",
    "        df['target_2'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2) - df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-1)\n",
    "        return {\n",
    "            \"df\": df, \n",
    "            \"target_col\": self.target_col,\n",
    "            \"needs_integration\": True,\n",
    "            #\"integration_function\": lambda x: x[self.target] + x['target_1'] + x['target_2']\n",
    "        }\n",
    "\n",
    "\n",
    "class CreateTargetColumDiffStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df.drop(columns=[\"target\"], inplace=True, errors='ignore')\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2) - df[self.target_col]\n",
    "        return {\n",
    "            \"df\": df, \n",
    "            \"target_col\": self.target_col,\n",
    "            \"needs_integration\": True,\n",
    "            #\"integration_function\": lambda x: x[self.target] + x['target']\n",
    "        }\n",
    "    \n",
    "\n",
    "class PredictStep(PipelineStep):\n",
    "    def execute(self, df, test_index, model, features) -> None:\n",
    "        X_predict = df.loc[test_index][features]\n",
    "        predictions = model.predict(X_predict)\n",
    "        return {\"predictions\": predictions}\n",
    "\n",
    "\n",
    "class IntegratePredictionsStep(PipelineStep):\n",
    "    def execute(self, df, predictions, test_index, target_col, needs_integration=False) -> Dict:\n",
    "        if not needs_integration:\n",
    "            return {\n",
    "                \"y_test\": df.loc[test_index, [\"target\"]]\n",
    "            }\n",
    "        # crea un nuevo dataframe que es la suma de todas las columnas de predicciones\n",
    "        if predictions.ndim == 1:\n",
    "            predictions_sum = pd.Series(predictions, index=test_index, name='predictions')\n",
    "        else:\n",
    "            predictions_sum = predictions.sum(axis=1)\n",
    "        final_predictions = predictions_sum + df.loc[test_index, target_col]\n",
    "        predictions = pd.Series(final_predictions, index=test_index, name='predictions')\n",
    "        target_columns = [col for col in df.columns if 'target' in col]\n",
    "        test_sum = df.loc[test_index, target_columns].sum(axis=1)\n",
    "        y_test = test_sum + df.loc[test_index, target_col]\n",
    "        y_test = pd.DataFrame(y_test, index=test_index, columns=[\"target\"])\n",
    "        \n",
    "        # nuevo approach, uso integration_function\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"y_test\": y_test\n",
    "        }\n",
    "    \n",
    "\n",
    "## legacy code\n",
    "class IntegratePredictionsStepOld(PipelineStep):\n",
    "    def execute(self, pipeline, predict_set, predictions, target_col, test) -> Dict:\n",
    "        \"\"\"\n",
    "        Integra las predicciones al DataFrame de test.\n",
    "        Si el target_col es una diferencia, se suma el último valor de target_col al target.\n",
    "        \"\"\"\n",
    "        pred_original_df = pipeline.get_artifact(predict_set)\n",
    "        predictions[\"predictions\"] = predictions[\"predictions\"] + pred_original_df[target_col]\n",
    "        test[\"target\"] = test[\"target\"] + test[target_col]\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"test\": test\n",
    "        } \n",
    "\n",
    "\n",
    "\n",
    "class SplitDataFrameStepOld(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        sorted_dated = sorted(df[\"fecha\"].unique())\n",
    "        last_date = sorted_dated[-1] # es 12-2019\n",
    "        last_test_date = sorted_dated[-3] # needs a gap because forecast moth+2\n",
    "        last_train_date = sorted_dated[-4] #\n",
    "\n",
    "        kaggle_pred = df[df[\"fecha\"] == last_date]\n",
    "        test = df[df[\"fecha\"] == last_test_date]\n",
    "        eval_data = df[df[\"fecha\"] == last_train_date]\n",
    "        train = df[(df[\"fecha\"] < last_train_date)]\n",
    "        return {\n",
    "            \"train\": train,\n",
    "            \"eval_data\": eval_data,\n",
    "            \"test\": test,\n",
    "            \"kaggle_pred\": kaggle_pred\n",
    "        }\n",
    "    \n",
    "\n",
    "class PrepareXYStepOld(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, train, eval_data, test, kaggle_pred) -> None:\n",
    "        features = [col for col in train.columns if col not in\n",
    "                        ['fecha', 'target']]\n",
    "        target = 'target'\n",
    "\n",
    "        X_train = pd.concat([train[features], eval_data[features]]) # [train + eval] + [eval] -> [test] \n",
    "        y_train = pd.concat([train[target], eval_data[target]])\n",
    "\n",
    "        X_train_alone = train[features]\n",
    "        y_train_alone = train[target]\n",
    "\n",
    "        X_eval = eval_data[features]\n",
    "        y_eval = eval_data[target]\n",
    "\n",
    "        X_test = test[features]\n",
    "        y_test = test[target]\n",
    "\n",
    "        X_train_final = pd.concat([train[features], eval_data[features], test[features]])\n",
    "        y_train_final = pd.concat([train[target], eval_data[target], test[target]])\n",
    "\n",
    "        X_kaggle = kaggle_pred[features]\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_train_alone\": X_train_alone,\n",
    "            \"y_train_alone\": y_train_alone,\n",
    "            \"X_eval\": X_eval,\n",
    "            \"y_eval\": y_eval,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "            \"X_train_final\": X_train_final,\n",
    "            \"y_train_final\": y_train_final,\n",
    "            \"X_kaggle\": X_kaggle\n",
    "        }\n",
    "        \n",
    "\n",
    "# CreateTargetColumStep\n",
    "from typing import Optional, Dict\n",
    "import pandas as pd\n",
    "\n",
    "class SplitDataFrameStep(PipelineStep):\n",
    "    def __init__(\n",
    "            self, \n",
    "            test_date=\"2019-12\", \n",
    "            df=\"df\", \n",
    "            gap=0,\n",
    "            name: Optional[str] = None\n",
    "        ):\n",
    "        super().__init__(name)\n",
    "        self.test_date = test_date\n",
    "        self.df = df\n",
    "        self.gap = gap \n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        df = pipeline.get_artifact(self.df)\n",
    "        test_df = df[df[\"fecha\"] == self.test_date]\n",
    "        train_df = df[df[\"fecha\"] < self.test_date]\n",
    "        last_train_date = train_df[\"fecha\"].max()\n",
    "        if isinstance(last_train_date, pd.Period):\n",
    "            last_train_date = last_train_date.to_timestamp()\n",
    "        gap_date = pd.to_datetime(last_train_date) - pd.DateOffset(months=self.gap)\n",
    "        # Convert gap_date to Period with same freq as fecha\n",
    "        if pd.api.types.is_period_dtype(df[\"fecha\"]):\n",
    "            gap_date = pd.Period(gap_date, freq=df[\"fecha\"].dt.freq)\n",
    "        train_df = train_df[train_df[\"fecha\"] < gap_date]\n",
    "        return {\n",
    "            \"train_index\": train_df.index,\n",
    "            \"test_index\": test_df.index\n",
    "        }\n",
    "\n",
    "\n",
    "class PrepareXYStep(PipelineStep):\n",
    "    def execute(self, df, train_index, test_index) -> None:\n",
    "        columns = df.columns\n",
    "        #features = [col for col in columns if col != \"fecha\" and \"target\" not in col]\n",
    "        features = [col for col in columns if col != \"fecha\" and \"target\" not in col]\n",
    "        targets = [col for col in columns if \"target\" in col]\n",
    "        X_train = df.loc[train_index][features]\n",
    "        y_train = df.loc[train_index][targets]\n",
    "        X_test = df.loc[test_index][features]\n",
    "        y_test = df.loc[test_index][targets]\n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"targets\": targets,\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "\n",
    "\n",
    "class CreateTargetColumStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2)    \n",
    "        return {\"df\": df, \"target_col\": self.target_col}\n",
    "    \n",
    "\n",
    "class CreateMultiDiffTargetColumStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target_1'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-1) - df[self.target_col]\n",
    "        df['target_2'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2) - df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-1)\n",
    "        return {\n",
    "            \"df\": df, \n",
    "            \"target_col\": self.target_col,\n",
    "            \"needs_integration\": True,\n",
    "            #\"integration_function\": lambda x: x[self.target] + x['target_1'] + x['target_2']\n",
    "        }\n",
    "\n",
    "\n",
    "class CreateTargetColumDiffStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df.drop(columns=[\"target\"], inplace=True, errors='ignore')\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2) - df[self.target_col]\n",
    "        return {\n",
    "            \"df\": df, \n",
    "            \"target_col\": self.target_col,\n",
    "            \"needs_integration\": True,\n",
    "            #\"integration_function\": lambda x: x[self.target] + x['target']\n",
    "        }\n",
    "    \n",
    "\n",
    "class PredictStep(PipelineStep):\n",
    "    def execute(self, df, test_index, model, features) -> None:\n",
    "        X_predict = df.loc[test_index][features]\n",
    "        predictions = model.predict(X_predict)\n",
    "        return {\"predictions\": predictions}\n",
    "\n",
    "\n",
    "class IntegratePredictionsStep(PipelineStep):\n",
    "    def execute(self, df, predictions, test_index, target_col, needs_integration=False) -> Dict:\n",
    "        if not needs_integration:\n",
    "            return {\n",
    "                \"y_test\": df.loc[test_index, [\"target\"]]\n",
    "            }\n",
    "        # crea un nuevo dataframe que es la suma de todas las columnas de predicciones\n",
    "        if predictions.ndim == 1:\n",
    "            predictions_sum = pd.Series(predictions, index=test_index, name='predictions')\n",
    "        else:\n",
    "            predictions_sum = predictions.sum(axis=1)\n",
    "        final_predictions = predictions_sum + df.loc[test_index, target_col]\n",
    "        predictions = pd.Series(final_predictions, index=test_index, name='predictions')\n",
    "        target_columns = [col for col in df.columns if 'target' in col]\n",
    "        test_sum = df.loc[test_index, target_columns].sum(axis=1)\n",
    "        y_test = test_sum + df.loc[test_index, target_col]\n",
    "        y_test = pd.DataFrame(y_test, index=test_index, columns=[\"target\"])\n",
    "        \n",
    "        # nuevo approach, uso integration_function\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"y_test\": y_test\n",
    "        }\n",
    "    \n",
    "\n",
    "## legacy code\n",
    "class IntegratePredictionsStepOld(PipelineStep):\n",
    "    def execute(self, pipeline, predict_set, predictions, target_col, test) -> Dict:\n",
    "        \"\"\"\n",
    "        Integra las predicciones al DataFrame de test.\n",
    "        Si el target_col es una diferencia, se suma el último valor de target_col al target.\n",
    "        \"\"\"\n",
    "        pred_original_df = pipeline.get_artifact(predict_set)\n",
    "        predictions[\"predictions\"] = predictions[\"predictions\"] + pred_original_df[target_col]\n",
    "        test[\"target\"] = test[\"target\"] + test[target_col]\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"test\": test\n",
    "        } \n",
    "\n",
    "\n",
    "\n",
    "class SplitDataFrameStepOld(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        sorted_dated = sorted(df[\"fecha\"].unique())\n",
    "        last_date = sorted_dated[-1] # es 12-2019\n",
    "        last_test_date = sorted_dated[-3] # needs a gap because forecast moth+2\n",
    "        last_train_date = sorted_dated[-4] #\n",
    "\n",
    "        kaggle_pred = df[df[\"fecha\"] == last_date]\n",
    "        test = df[df[\"fecha\"] == last_test_date]\n",
    "        eval_data = df[df[\"fecha\"] == last_train_date]\n",
    "        train = df[(df[\"fecha\"] < last_train_date)]\n",
    "        return {\n",
    "            \"train\": train,\n",
    "            \"eval_data\": eval_data,\n",
    "            \"test\": test,\n",
    "            \"kaggle_pred\": kaggle_pred\n",
    "        }\n",
    "    \n",
    "\n",
    "class PrepareXYStepOld(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, train, eval_data, test, kaggle_pred) -> None:\n",
    "        features = [col for col in train.columns if col not in\n",
    "                        ['fecha', 'target']]\n",
    "        target = 'target'\n",
    "\n",
    "        X_train = pd.concat([train[features], eval_data[features]]) # [train + eval] + [eval] -> [test] \n",
    "        y_train = pd.concat([train[target], eval_data[target]])\n",
    "\n",
    "        X_train_alone = train[features]\n",
    "        y_train_alone = train[target]\n",
    "\n",
    "        X_eval = eval_data[features]\n",
    "        y_eval = eval_data[target]\n",
    "\n",
    "        X_test = test[features]\n",
    "        y_test = test[target]\n",
    "\n",
    "        X_train_final = pd.concat([train[features], eval_data[features], test[features]])\n",
    "        y_train_final = pd.concat([train[target], eval_data[target], test[target]])\n",
    "\n",
    "        X_kaggle = kaggle_pred[features]\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_train_alone\": X_train_alone,\n",
    "            \"y_train_alone\": y_train_alone,\n",
    "            \"X_eval\": X_eval,\n",
    "            \"y_eval\": y_eval,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "            \"X_train_final\": X_train_final,\n",
    "            \"y_train_final\": y_train_final,\n",
    "            \"X_kaggle\": X_kaggle\n",
    "        }\n",
    "        \n",
    "\n",
    "# ScaleFeatureStep\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "class PipelineScaler(ABC):\n",
    "    def __init__(self, column: str):\n",
    "        self.column = column\n",
    "        self.scaler_data = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        pass\n",
    "\n",
    "# TODO: hacer transformacion log1p si es necesario\n",
    "# TODO: debuggear, por alguna razon da mal\n",
    "\n",
    "class PipelineRobustScaler(PipelineScaler):\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])[self.column]  # SeriesGroupBy\n",
    "        median = grouped.median()\n",
    "        q1 = grouped.apply(lambda x: x.quantile(0.25))\n",
    "        q3 = grouped.apply(lambda x: x.quantile(0.75))\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        agg = pd.DataFrame({\n",
    "            f'{self.column}_median_scaler': median,\n",
    "            f'{self.column}_iqr_scaler': iqr\n",
    "        })\n",
    "        print(agg.head())\n",
    "        self.scaler_data = agg\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        original_index = df.index\n",
    "        original_nans = df[self.column].isna()\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(original_index, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = (df[self.column] - df[f'{self.column}_median_scaler']) / (df[f'{self.column}_iqr_scaler'])\n",
    "        # replace inf and -inf with NaN\n",
    "        df[f'{self.column}_scaled'].replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].fillna(0)\n",
    "        # original nans\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].where(~original_nans, other=pd.NA)\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_median_scaler', f'{self.column}_iqr_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}_scaled\"]\n",
    "\n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        return self.fit(df).transform(df)\n",
    "\n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "\n",
    "        # agrego columnas temporales\n",
    "        df_index = df.index\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        # reconstruyo los indices originales\n",
    "        df.set_index(df_index, inplace=True)\n",
    "        df[f\"{self.column}\"] = (df[f'{self.column}'] * (df[f'{self.column}_iqr_scaler'])) + df[f'{self.column}_median_scaler']\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_median_scaler', f'{self.column}_iqr_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}\"]\n",
    "\n",
    "\n",
    "class PipelineStandarScaler(PipelineScaler):\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        agg = df.groupby(['product_id', 'customer_id'])[self.column].agg(['mean', 'std']).rename(\n",
    "            columns={'mean': f'{self.column}_mean_scaler', 'std': f'{self.column}_std_scaler'})\n",
    "        self.scaler_data = agg\n",
    "        #self.scaler_data.fillna(0, inplace=True)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        original_index = df.index\n",
    "        original_nans = df[self.column].isna()\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(original_index, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = (df[self.column] - df[f'{self.column}_mean_scaler']) / (df[f'{self.column}_std_scaler'])\n",
    "        # replace inf and -inf with NaN\n",
    "        df[f'{self.column}_scaled'].replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "        # original nans\n",
    "        # hago un fill nan de las rows que no eran nan en la serie original\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].fillna(0)\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].where(~original_nans, other=pd.NA)\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_mean_scaler', f'{self.column}_std_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}_scaled\"]\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        return self.fit(df).transform(df)\n",
    "    \n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "\n",
    "        # agrego columnas temporales\n",
    "        df_index = df.index\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        # reconstruyo los indices originales\n",
    "        df.set_index(df_index, inplace=True)\n",
    "        df[f\"{self.column}\"] = (df[f'{self.column}'] * (df[f'{self.column}_std_scaler'])) + df[f'{self.column}_mean_scaler']\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_mean_scaler', f'{self.column}_std_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}\"]\n",
    "    \n",
    "\n",
    "class PipelineMinMaxScaler(PipelineScaler):\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        agg = df.groupby(['product_id', 'customer_id'])[self.column].agg(['min', 'max']).rename(\n",
    "            columns={'min': f'{self.column}_min_scaler', 'max': f'{self.column}_max_scaler'})\n",
    "        # seteo el minimo con 0 asi queda estandarlizado en todas las series\n",
    "        agg[f'{self.column}_min_scaler'] = 0\n",
    "        self.scaler_data = agg\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        original_index = df.index\n",
    "        original_nans = df[self.column].isna()\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(original_index, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = (df[self.column] - df[f'{self.column}_min_scaler']) / (df[f'{self.column}_max_scaler'] - df[f'{self.column}_min_scaler'])\n",
    "        df[f'{self.column}_scaled'].replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "        # original nans\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].fillna(0)\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].where(~original_nans, other=pd.NA)\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_min_scaler', f'{self.column}_max_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}_scaled\"]\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        return self.fit(df).transform(df)\n",
    "    \n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        df_index = df.index\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(df_index, inplace=True)\n",
    "\n",
    "        df[f\"{self.column}\"] = (df[f'{self.column}'] * (df[f'{self.column}_max_scaler'] - df[f'{self.column}_min_scaler'])) + df[f'{self.column}_min_scaler']\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_min_scaler', f'{self.column}_max_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}\"]\n",
    "    \n",
    "\n",
    "class ScaleFeatureStep(PipelineStep):\n",
    "    def __init__(self, column: str, regex=False, override=False, scaler=PipelineStandarScaler, name = None,):\n",
    "        super().__init__(name)\n",
    "        self.column = column\n",
    "        self.scaler_cls = scaler\n",
    "        self.regex = regex\n",
    "        self.override = override\n",
    "\n",
    "    def execute(self, df: pd.DataFrame, train_index) -> Dict:\n",
    "        # si regex es True, busco todas las columnas que coincidan con el regex\n",
    "        if self.regex:\n",
    "            columns = df.filter(regex=self.column, axis=1).columns.tolist()\n",
    "            print(f\"Columns found matching regex '{self.column}': {columns}\")\n",
    "            if not columns:\n",
    "                raise ValueError(f\"No columns found matching regex '{self.column}'\")\n",
    "        else:\n",
    "            columns = [self.column]\n",
    "        scalers = {}\n",
    "        for column in columns:\n",
    "            scaler = self.scaler_cls(\n",
    "                column=column,\n",
    "            )\n",
    "            if self.override:\n",
    "                column_scaled = column\n",
    "            else:\n",
    "                column_scaled = f\"{column}_scaled\"\n",
    "            scaler.fit(df[[\"product_id\", \"customer_id\", column]])\n",
    "            df[column_scaled] = scaler.transform(df[[\"product_id\", \"customer_id\", column]])\n",
    "            scalers[f\"scaler_{column_scaled}\"] = scaler\n",
    "        ret = {\"df\": df, **scalers}\n",
    "        return ret\n",
    "    \n",
    "\n",
    "class InverseScalePredictionsStep(PipelineStep):\n",
    "    def execute(self, predictions, df, test_index, scaler_target=None) -> Dict:\n",
    "        \"\"\"\n",
    "        Inverse scale the predictions using the provided grouped scaler.\n",
    "        \"\"\"\n",
    "        if not scaler_target:\n",
    "            return\n",
    "\n",
    "        # creo un df predictions_df que tiene predictions, product_id y customer_id de df para los indices de predictions\n",
    "        predictions_df = pd.DataFrame(predictions, index=predictions.index)\n",
    "        predictions_df[\"product_id\"] = df[\"product_id\"]\n",
    "        predictions_df[\"customer_id\"] = df[\"customer_id\"]\n",
    "        predictions_df.columns = [\"target\", \"product_id\", \"customer_id\"]\n",
    "        predictions = scaler_target.inverse_transform(predictions_df)\n",
    "        predictions = pd.Series(predictions, name=\"predictions\")\n",
    "        predictions.index = test_index\n",
    "        predictions.fillna(0, inplace=True)\n",
    "\n",
    "        df[\"target\"] = scaler_target.inverse_transform(df[[\"target\", \"product_id\", \"customer_id\"]])    \n",
    " \n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"df\": df\n",
    "        }\n",
    "\n",
    "# ReduceMemoryUsageStep\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "\n",
    "\n",
    "class CreateSerieIdStep(PipelineStep):\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Creo una nueva columna serieID que es la combinacion entre serie_id y customer_id\n",
    "        \"\"\"\n",
    "        df['serieID'] = df['product_id'].astype(str) + df['customer_id'].astype(str)\n",
    "        df['serieID'] = df['serieID'].astype('uint64')\n",
    "        # le resto el valor menimo asi empieza en 1\n",
    "        df[\"serieID\"] = df[\"serieID\"] - df[\"serieID\"].min() + 1\n",
    "        # la paso a uint32\n",
    "        df[\"serieID\"] = df[\"serieID\"].astype(\"uint32\")\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "\n",
    "class DropMinSerieMonthStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, months: int = 3):\n",
    "        super().__init__(name)\n",
    "        self.months = months\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Agrupo las series por customer_id y product_id, cuento el largo de cada serie y elimino las series que tienen menos de self.months meses\n",
    "        \"\"\"\n",
    "        # Agrupo por serieID y cuento los meses únicos\n",
    "        series_counts = df.groupby('serieID')['mes'].nunique()\n",
    "        \n",
    "        # Filtrar series con menos de self.months meses\n",
    "        valid_series = series_counts[series_counts >= self.months].index\n",
    "        \n",
    "        # Filtrar el DataFrame original\n",
    "        df = df[df['serieID'].isin(valid_series)]\n",
    "        print(f\"Number of series dropped : {len(series_counts) - len(valid_series)}\")\n",
    "        \n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class FilterProductsIDStep(PipelineStep):\n",
    "    def __init__(self, product_file = \"product_id_apredecir201912.txt\", dfs=[\"df\"], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.file = product_file\n",
    "        self.dfs = dfs\n",
    "\n",
    "    def execute(self, pipeline: Pipeline) -> None:\n",
    "        \"\"\" el txt es un csv que tiene columna product_id separado por tabulaciones \"\"\"\n",
    "        converted_dfs = {}\n",
    "        for df_key in self.dfs:\n",
    "            df = pipeline.get_artifact(df_key)\n",
    "            product_ids = pd.read_csv(self.file, sep=\"\\t\")[\"product_id\"].tolist()\n",
    "            df = df[df[\"product_id\"].isin(product_ids)]\n",
    "            converted_dfs[df_key] = df\n",
    "            print(f\"Filtered DataFrame {df_key} shape: {df.shape}\")\n",
    "        return converted_dfs\n",
    "    \n",
    "\n",
    "class FilterProductForTestingStep(PipelineStep):\n",
    "    def __init__(self, total_products_ids: int = 100, name: Optional[str] = None, random=True):\n",
    "        super().__init__(name)\n",
    "        self.total_products_ids = total_products_ids\n",
    "        self.random = random\n",
    "        \n",
    "    def execute(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" Filtra el DataFrame para que contenga solo los primeros total_products_ids productos \"\"\"\n",
    "        unique_products = df['product_id'].unique()\n",
    "        if len(unique_products) > self.total_products_ids:\n",
    "            if self.random:\n",
    "                products = np.random.choice(unique_products, size=self.total_products_ids, replace=False)\n",
    "            else:\n",
    "                products = unique_products[:self.total_products_ids]\n",
    "            df = df[df['product_id'].isin(products)]\n",
    "        print(f\"Filtered DataFrame shape: {df.shape}\")\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class CastDataTypesStep(PipelineStep):\n",
    "    def __init__(self, dtypes: Dict[str, str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.dtypes = dtypes\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> None:\n",
    "        for col, dtype in self.dtypes.items():\n",
    "            df[col] = df[col].astype(dtype)\n",
    "        print(df.info())\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class ReduceMemoryUsageStep(PipelineStep):\n",
    "\n",
    "    def execute(self, df):\n",
    "        initial_mem_usage = df.memory_usage().sum() / 1024**2\n",
    "        for col in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if pd.api.types.is_float_dtype(df[col]):\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                elif pd.api.types.is_integer_dtype(df[col]):\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "        \n",
    "        final_mem_usage = df.memory_usage().sum() / 1024**2\n",
    "        print('--- Memory usage before: {:.2f} MB'.format(initial_mem_usage))\n",
    "        print('--- Memory usage after: {:.2f} MB'.format(final_mem_usage))\n",
    "        print('--- Decreased memory usage by {:.1f}%\\n'.format(100 * (initial_mem_usage - final_mem_usage) / initial_mem_usage))\n",
    "        return {\"df\": df}      \n",
    "\n",
    "        \n",
    "class ChangeDataTypesStep(PipelineStep):\n",
    "    def __init__(self, dtypes: Dict[str, str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.dtypes = dtypes\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        for original_dtype, dtype in self.dtypes.items():\n",
    "            for col in df.select_dtypes(include=[original_dtype]).columns:\n",
    "                df[col] = df[col].astype(dtype)\n",
    "        print(df.info())\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class FilterFirstDateStep(PipelineStep):\n",
    "    def __init__(self, first_date: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.first_date = first_date\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        df = df[df[\"fecha\"] >= self.first_date]\n",
    "        print(f\"Filtered DataFrame shape: {df.shape}\")\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class FeatureEngineeringLagStep(PipelineStep):\n",
    "    def __init__(self, lags: List[int], columns: List, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.lags = lags\n",
    "        self.columns = columns\n",
    "        self.all = all\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> dict:\n",
    "        # Ordenar por grupo y fecha para que los lags sean correctos\n",
    "\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        \n",
    "        # Crear lags usando groupby y shift (vectorizado)\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for column in self.columns:\n",
    "            for lag in self.lags:\n",
    "                df[f\"{column}_lag_{lag}\"] = grouped[column].shift(lag)\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class RollingMeanFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).mean()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class RollingStdFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_std_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).std()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class RollingSkewFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_skew_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).skew()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class RollingKurtosisFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_kurtosis_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).kurtosis()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class RollingZscoreFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            rolling_mean = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).mean()\n",
    "            )\n",
    "            rolling_std = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).std()\n",
    "            )\n",
    "            df[f'{col}_rolling_zscore_{self.window}'] = (df[col] - rolling_mean) / (rolling_std + 1e-6)\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class RollingAutocorrelationFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], lags: List[int], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "        self.lags = lags\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        \n",
    "        for col in self.columns:\n",
    "            for lag in self.lags:\n",
    "                df[f'{col}_rolling_autocorr_{lag}_{self.window}'] = grouped[col].transform(\n",
    "                    lambda x: x.rolling(self.window, min_periods=1).apply(\n",
    "                        lambda y: y.autocorr(lag=lag), raw=False\n",
    "                    )\n",
    "                )\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class RollingMaxFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_max_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).max()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class RollingMinFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_min_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).min()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class RollingStdFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_std_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).std()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class ExponentialMovingAverageStep(PipelineStep):\n",
    "    def __init__(self, span: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.span = span\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_ema_{self.span}'] = grouped[col].transform(\n",
    "                lambda x: x.ewm(span=self.span, adjust=False).mean()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class TrendFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        \n",
    "        def calculate_trend(series):\n",
    "            return series.rolling(self.window).apply(\n",
    "                lambda x: linregress(np.arange(len(x)), x)[0], raw=False\n",
    "            )\n",
    "        \n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_trend_{self.window}'] = grouped[col].transform(calculate_trend)\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class DiffFeatureStep(PipelineStep):\n",
    "    def __init__(self, periods: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.periods = periods\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_diff_{self.periods}'] = grouped[col].diff(self.periods)\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class RollingMedianFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_median_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).median()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class CreateTotalCategoryStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, cat: str = \"cat1\", tn: str = \"tn\"):\n",
    "        super().__init__(name)\n",
    "        self.cat = cat\n",
    "        self.tn = tn\n",
    "    \n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(['fecha', self.cat])\n",
    "        df[f\"{self.tn}_{self.cat}_vendidas\"] = (\n",
    "            df.groupby(['fecha', self.cat])[self.tn]\n",
    "              .transform('sum')\n",
    "        )\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class CreateWeightByCustomerStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        # Aseguramos orden estable (opcional, mejora legibilidad)\n",
    "        df = df.sort_values(['fecha', 'customer_id'])\n",
    "        \n",
    "        # 1) Sumatoria de 'tn' por (fecha, customer_id) directamente en cada fila\n",
    "        df['tn_customer_vendidas'] = (\n",
    "            df.groupby(['fecha', 'customer_id'])['tn']\n",
    "              .transform('sum')\n",
    "        )\n",
    "        # 2) Sumatoria total de 'tn' por fecha\n",
    "        df['tn_total_vendidas'] = (\n",
    "            df.groupby('fecha')['tn']\n",
    "              .transform('sum')\n",
    "        )\n",
    "        # 3) Ratio\n",
    "        df['customer_weight'] = df['tn_customer_vendidas'] / df['tn_total_vendidas']\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class CreateWeightByProductStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        # Aseguramos orden estable (opcional, mejora legibilidad)\n",
    "        df = df.sort_values(['fecha', 'product_id'])\n",
    "        # 1) Sumatoria de 'tn' por (fecha, product_id) directamente en cada fila\n",
    "        df['tn_product_vendidas'] = (\n",
    "            df.groupby(['fecha', 'product_id'])['tn']\n",
    "              .transform('sum')\n",
    "        )\n",
    "        # 2) Sumatoria total de 'tn' por fecha\n",
    "        df['tn_total_vendidas'] = (\n",
    "            df.groupby('fecha')['tn']\n",
    "              .transform('sum')\n",
    "        )\n",
    "        # 3) Ratio\n",
    "        df['product_weight'] = df['tn_product_vendidas'] / df['tn_total_vendidas']\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class FeatureDivInteractionStep(PipelineStep):\n",
    "    def __init__(self, columns: List[Tuple[str, str]], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        for col1, col2 in self.columns:\n",
    "            df[f\"{col1}_div_{col2}\"] = df[col1] / (df[col2] + 1e-6)  # Evitar división por cero\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class FeatureProdInteractionStep(PipelineStep):\n",
    "    def __init__(self, columns: List[Tuple[str, str]], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        for col1, col2 in self.columns:\n",
    "            df[f\"{col1}_prod_{col2}\"] = df[col1] * df[col2]\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class DateRelatedFeaturesStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        df[\"year\"] = df[\"fecha\"].dt.year\n",
    "        df[\"mes\"] = df[\"fecha\"].dt.month\n",
    "        return {\"df\": df}\n",
    "\n",
    "# PrepareXYStep\n",
    "from typing import Optional, Dict\n",
    "import pandas as pd\n",
    "\n",
    "class SplitDataFrameStep(PipelineStep):\n",
    "    def __init__(\n",
    "            self, \n",
    "            test_date=\"2019-12\", \n",
    "            df=\"df\", \n",
    "            gap=0,\n",
    "            name: Optional[str] = None\n",
    "        ):\n",
    "        super().__init__(name)\n",
    "        self.test_date = test_date\n",
    "        self.df = df\n",
    "        self.gap = gap \n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        df = pipeline.get_artifact(self.df)\n",
    "        test_df = df[df[\"fecha\"] == self.test_date]\n",
    "        train_df = df[df[\"fecha\"] < self.test_date]\n",
    "        last_train_date = train_df[\"fecha\"].max()\n",
    "        if isinstance(last_train_date, pd.Period):\n",
    "            last_train_date = last_train_date.to_timestamp()\n",
    "        gap_date = pd.to_datetime(last_train_date) - pd.DateOffset(months=self.gap)\n",
    "        # Convert gap_date to Period with same freq as fecha\n",
    "        if pd.api.types.is_period_dtype(df[\"fecha\"]):\n",
    "            gap_date = pd.Period(gap_date, freq=df[\"fecha\"].dt.freq)\n",
    "        train_df = train_df[train_df[\"fecha\"] < gap_date]\n",
    "        return {\n",
    "            \"train_index\": train_df.index,\n",
    "            \"test_index\": test_df.index\n",
    "        }\n",
    "\n",
    "\n",
    "class PrepareXYStep(PipelineStep):\n",
    "    def execute(self, df, train_index, test_index) -> None:\n",
    "        columns = df.columns\n",
    "        #features = [col for col in columns if col != \"fecha\" and \"target\" not in col]\n",
    "        features = [col for col in columns if col != \"fecha\" and \"target\" not in col]\n",
    "        targets = [col for col in columns if \"target\" in col]\n",
    "        X_train = df.loc[train_index][features]\n",
    "        y_train = df.loc[train_index][targets]\n",
    "        X_test = df.loc[test_index][features]\n",
    "        y_test = df.loc[test_index][targets]\n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"targets\": targets,\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "\n",
    "\n",
    "class CreateTargetColumStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2)    \n",
    "        return {\"df\": df, \"target_col\": self.target_col}\n",
    "    \n",
    "\n",
    "class CreateMultiDiffTargetColumStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target_1'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-1) - df[self.target_col]\n",
    "        df['target_2'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2) - df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-1)\n",
    "        return {\n",
    "            \"df\": df, \n",
    "            \"target_col\": self.target_col,\n",
    "            \"needs_integration\": True,\n",
    "            #\"integration_function\": lambda x: x[self.target] + x['target_1'] + x['target_2']\n",
    "        }\n",
    "\n",
    "\n",
    "class CreateTargetColumDiffStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df.drop(columns=[\"target\"], inplace=True, errors='ignore')\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2) - df[self.target_col]\n",
    "        return {\n",
    "            \"df\": df, \n",
    "            \"target_col\": self.target_col,\n",
    "            \"needs_integration\": True,\n",
    "            #\"integration_function\": lambda x: x[self.target] + x['target']\n",
    "        }\n",
    "    \n",
    "\n",
    "class PredictStep(PipelineStep):\n",
    "    def execute(self, df, test_index, model, features) -> None:\n",
    "        X_predict = df.loc[test_index][features]\n",
    "        predictions = model.predict(X_predict)\n",
    "        return {\"predictions\": predictions}\n",
    "\n",
    "\n",
    "class IntegratePredictionsStep(PipelineStep):\n",
    "    def execute(self, df, predictions, test_index, target_col, needs_integration=False) -> Dict:\n",
    "        if not needs_integration:\n",
    "            return {\n",
    "                \"y_test\": df.loc[test_index, [\"target\"]]\n",
    "            }\n",
    "        # crea un nuevo dataframe que es la suma de todas las columnas de predicciones\n",
    "        if predictions.ndim == 1:\n",
    "            predictions_sum = pd.Series(predictions, index=test_index, name='predictions')\n",
    "        else:\n",
    "            predictions_sum = predictions.sum(axis=1)\n",
    "        final_predictions = predictions_sum + df.loc[test_index, target_col]\n",
    "        predictions = pd.Series(final_predictions, index=test_index, name='predictions')\n",
    "        target_columns = [col for col in df.columns if 'target' in col]\n",
    "        test_sum = df.loc[test_index, target_columns].sum(axis=1)\n",
    "        y_test = test_sum + df.loc[test_index, target_col]\n",
    "        y_test = pd.DataFrame(y_test, index=test_index, columns=[\"target\"])\n",
    "        \n",
    "        # nuevo approach, uso integration_function\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"y_test\": y_test\n",
    "        }\n",
    "    \n",
    "\n",
    "## legacy code\n",
    "class IntegratePredictionsStepOld(PipelineStep):\n",
    "    def execute(self, pipeline, predict_set, predictions, target_col, test) -> Dict:\n",
    "        \"\"\"\n",
    "        Integra las predicciones al DataFrame de test.\n",
    "        Si el target_col es una diferencia, se suma el último valor de target_col al target.\n",
    "        \"\"\"\n",
    "        pred_original_df = pipeline.get_artifact(predict_set)\n",
    "        predictions[\"predictions\"] = predictions[\"predictions\"] + pred_original_df[target_col]\n",
    "        test[\"target\"] = test[\"target\"] + test[target_col]\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"test\": test\n",
    "        } \n",
    "\n",
    "\n",
    "\n",
    "class SplitDataFrameStepOld(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        sorted_dated = sorted(df[\"fecha\"].unique())\n",
    "        last_date = sorted_dated[-1] # es 12-2019\n",
    "        last_test_date = sorted_dated[-3] # needs a gap because forecast moth+2\n",
    "        last_train_date = sorted_dated[-4] #\n",
    "\n",
    "        kaggle_pred = df[df[\"fecha\"] == last_date]\n",
    "        test = df[df[\"fecha\"] == last_test_date]\n",
    "        eval_data = df[df[\"fecha\"] == last_train_date]\n",
    "        train = df[(df[\"fecha\"] < last_train_date)]\n",
    "        return {\n",
    "            \"train\": train,\n",
    "            \"eval_data\": eval_data,\n",
    "            \"test\": test,\n",
    "            \"kaggle_pred\": kaggle_pred\n",
    "        }\n",
    "    \n",
    "\n",
    "class PrepareXYStepOld(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, train, eval_data, test, kaggle_pred) -> None:\n",
    "        features = [col for col in train.columns if col not in\n",
    "                        ['fecha', 'target']]\n",
    "        target = 'target'\n",
    "\n",
    "        X_train = pd.concat([train[features], eval_data[features]]) # [train + eval] + [eval] -> [test] \n",
    "        y_train = pd.concat([train[target], eval_data[target]])\n",
    "\n",
    "        X_train_alone = train[features]\n",
    "        y_train_alone = train[target]\n",
    "\n",
    "        X_eval = eval_data[features]\n",
    "        y_eval = eval_data[target]\n",
    "\n",
    "        X_test = test[features]\n",
    "        y_test = test[target]\n",
    "\n",
    "        X_train_final = pd.concat([train[features], eval_data[features], test[features]])\n",
    "        y_train_final = pd.concat([train[target], eval_data[target], test[target]])\n",
    "\n",
    "        X_kaggle = kaggle_pred[features]\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_train_alone\": X_train_alone,\n",
    "            \"y_train_alone\": y_train_alone,\n",
    "            \"X_eval\": X_eval,\n",
    "            \"y_eval\": y_eval,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "            \"X_train_final\": X_train_final,\n",
    "            \"y_train_final\": y_train_final,\n",
    "            \"X_kaggle\": X_kaggle\n",
    "        }\n",
    "        \n",
    "\n",
    "# TrainModelStep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from typing import Dict, Optional\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "LGB_DEFAULT_PARAMS = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.7,\n",
    "    \"bagging_fraction\": 0.7,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"n_estimators\": 1500,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "class TotalErrorMetric:\n",
    "    def __init__(self, df_eval):\n",
    "        self.df_eval = df_eval\n",
    "\n",
    "    def __call__(self, preds, train_data):\n",
    "        labels = train_data.get_label()\n",
    "        df_temp = self.df_eval.copy()\n",
    "        df_temp['preds'] = preds\n",
    "        df_temp['labels'] = labels\n",
    "        # Agrupar por product_id y calcular el error\n",
    "        por_producto = df_temp.groupby(\"product_id\").agg({'labels': 'sum', 'preds': 'sum'})\n",
    "        # Calcular el error personalizado\n",
    "        error = np.sum(np.abs(por_producto['labels'] - por_producto['preds'])) / np.sum(np.abs(por_producto['labels']))\n",
    "        # LightGBM espera que el segundo valor sea mayor cuando el modelo es mejor\n",
    "        return 'total_error', error, False\n",
    "    \n",
    "\n",
    "class CustomMetricAutoML:\n",
    "    def __init__(self, df_eval):\n",
    "        self.df_eval = df_eval\n",
    "\n",
    "    def __call__(self, X_val, y_val, estimator, *args, **kwargs):\n",
    "        df_temp = X_val.copy()\n",
    "        df_temp['preds'] = estimator.predict(X_val)\n",
    "        df_temp['labels'] = y_val\n",
    "\n",
    "        por_producto = df_temp.groupby(\"product_id\").agg({'labels': 'sum', 'preds': 'sum'})\n",
    "        \n",
    "        error = np.sum(np.abs(por_producto['labels'] - por_producto['preds'])) / np.sum(por_producto['labels'])\n",
    "        \n",
    "        return error, {\"total_error\": error}\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "# import deepclone\n",
    "from copy import deepcopy\n",
    "\n",
    "class EnsambleKFoldWrapper:\n",
    "    # esta funcion se inicializa con un modelo, los clona y entra N modelos, uno por cada kfold.\n",
    "    # cuando hace la prediccion, promedia las predicciones de los N modelos.\n",
    "    def __init__(self, model, n_splits=5):\n",
    "        self.model = model\n",
    "        self.n_splits = n_splits\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y, X_val, y_val):\n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train = X.iloc[train_index]\n",
    "            y_train = y.iloc[train_index]\n",
    "            model_clone = deepcopy(self.model)  # Clona el modelo para cada fold\n",
    "            model_clone.fit(X_train, y_train, X_val, y_val)\n",
    "            self.models.append(model_clone)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models have been trained yet.\")\n",
    "        predictions = np.mean([model.predict(X) for model in self.models], axis=0)\n",
    "        return pd.Series(predictions, index=X.index, name='predictions')\n",
    "\n",
    "\n",
    "class XGBOOSTPipelineModel:\n",
    "    def __init__(self, params: Dict = None):\n",
    "        self.params = params or {}\n",
    "        self.model = None\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.params.update(params)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_eval, y_eval):\n",
    "        from xgboost import XGBRegressor\n",
    "        if isinstance(y_train, pd.DataFrame) and y_train.shape[1] > 1:\n",
    "            raise ValueError(\"y_train must be a Series for single-target regression.\")\n",
    "        self.model = XGBRegressor(**self.params, enable_categorical=True)\n",
    "        eval_sets = []\n",
    "        y_eval = y_eval.dropna()\n",
    "        if not y_eval.empty:\n",
    "            X_eval = X_eval.loc[y_eval.index]\n",
    "            eval_sets = [(X_eval, y_eval)]\n",
    "        y_train = y_train.dropna()\n",
    "        X_train = X_train.loc[y_train.index]\n",
    "        self.model.fit(X_train, y_train, eval_set=eval_sets, verbose=True)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        return pd.Series(self.model.predict(X), index=X.index, name='predictions')\n",
    "\n",
    "\n",
    "class LGBPipelineModel:\n",
    "    def __init__(self, params: Dict = LGB_DEFAULT_PARAMS):\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.params.update(params)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_eval, y_eval):\n",
    "        # si y_train tiene mas de una collumna uso mulltitarget\n",
    "        if isinstance(y_train, pd.DataFrame) and y_train.shape[1] > 1:\n",
    "            self.model = LGBMultiTargetPipelineModel(self.params).fit(X_train, y_train, X_eval, y_eval)\n",
    "        else:\n",
    "            self.model = LGBPipelineSingleTargetModel(self.params).fit(X_train, y_train, X_eval, y_eval)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def plot_importance(self, max_num_features=20):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        lgb.plot_importance(self.model.model, max_num_features=max_num_features)\n",
    "\n",
    "class LGBBase:\n",
    "    def set_params(self, **params):\n",
    "        self.params.update(params)\n",
    "\n",
    "    def _make_datasets(self, X_train, y_train, X_eval, y_eval):\n",
    "        # droppeo los indices de X_train donde y_train es nan\n",
    "        y_train = y_train.dropna()\n",
    "        X_train = X_train.loc[y_train.index]\n",
    "        cat_features = [col for col in X_train.columns if X_train[col].dtype.name == 'category']\n",
    "        train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features)\n",
    "        y_eval = y_eval.dropna()\n",
    "        # si y_val esta vacio, no creo el dataset de evaluacion\n",
    "        print(f\"Validation set size: {len(y_eval)}\")\n",
    "        if y_eval.empty:\n",
    "            return train_data, None\n",
    "        del X_train, y_train  # borro X_train, y_train para liberar memoria\n",
    "        X_eval = X_eval.loc[y_eval.index]\n",
    "        print(f\"X_eval first 5 rows:\\n{X_eval.head()}\")\n",
    "        print(f\"y_eval first 5 rows:\\n{y_eval.head()}\")\n",
    "        eval_data =lgb.Dataset(X_eval, label=y_eval, reference=train_data, categorical_feature=cat_features)\n",
    "        return train_data, eval_data\n",
    "      \n",
    "    def _train_model(self, train_data, eval_data=None):\n",
    "        if eval_data is None:\n",
    "            eval_params = {}\n",
    "\n",
    "        else:\n",
    "            eval_params = {\n",
    "                \"valid_sets\": [eval_data],\n",
    "                \"feval\": TotalErrorMetric(eval_data.data)\n",
    "            }\n",
    "        callbacks = [\n",
    "            lgb.log_evaluation(100),\n",
    "            #lgb.early_stopping(50),\n",
    "        ]\n",
    "        model = lgb.train(\n",
    "            self.params,\n",
    "            train_data,\n",
    "            callbacks=callbacks,\n",
    "            **eval_params\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "\n",
    "class LGBMultiTargetPipelineModel(LGBBase):\n",
    "    def __init__(self, params: Dict = LGB_DEFAULT_PARAMS):\n",
    "        self.params = params\n",
    "        self.models = {}\n",
    "\n",
    "    def fit(self, X_train, y_train, X_eval, y_eval):\n",
    "        if not isinstance(y_train, pd.DataFrame):\n",
    "            raise ValueError(\"y_train must be a DataFrame for multi-target regression.\")\n",
    "        for target in y_train.columns:\n",
    "            print(f\"Training model for target: {target}\")\n",
    "            train_data, eval_data = self._make_datasets(X_train, y_train[target], X_eval, y_eval[target])\n",
    "            model = self._train_model(train_data, eval_data)\n",
    "            self.models[target] = model\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models have been trained yet.\")\n",
    "        predictions = {}\n",
    "        for target, model in self.models.items():\n",
    "            predictions[target] = model.predict(X)\n",
    "        return pd.DataFrame(predictions, index=X.index)\n",
    "\n",
    "\n",
    "class LGBPipelineSingleTargetModel(LGBBase):\n",
    "    def __init__(self, params: Dict = LGB_DEFAULT_PARAMS):\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train, X_eval, y_eval):\n",
    "        if isinstance(y_train, pd.DataFrame) and y_train.shape[1] > 1:\n",
    "            raise ValueError(\"y_train must be a Series for single-target regression.\")\n",
    "        train_data, eval_data = self._make_datasets(X_train, y_train, X_eval, y_eval)\n",
    "        # borro X_train, y _train, X_eval, y_eval para liberar memoria\n",
    "        del X_train, y_train, X_eval, y_eval\n",
    "        self.model = self._train_model(train_data, eval_data)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "        return pd.Series(self.model.predict(X), index=X.index, name='predictions')\n",
    "    \n",
    "    \n",
    "class TrainModelStep(PipelineStep):\n",
    "    def __init__(self, model_cls = LGBPipelineModel, name: Optional[str] = None, params={}, folds=0):\n",
    "        super().__init__(name)\n",
    "        self.model_cls = model_cls\n",
    "        self.params = params\n",
    "        self.folds = folds  # Number of folds for cross-validation, if applicable\n",
    "\n",
    "    def execute(self, X_test, y_test, X_train, y_train, params={}) -> None:\n",
    "        params = params or self.params\n",
    "        if self.folds > 1:\n",
    "            model = self.model_cls()\n",
    "            model.set_params(**params)\n",
    "            model = EnsambleKFoldWrapper(model, n_splits=self.folds)\n",
    "            model.fit(X_train, y_train, X_test, y_test)\n",
    "        else:\n",
    "            model = self.model_cls()\n",
    "            model.set_params(**params)\n",
    "            model.fit(X_train, y_train, X_test, y_test)\n",
    "        return {\"model\": model}\n",
    "    \n",
    "\n",
    "## LEGACY\n",
    "class TrainModelLGBStep(PipelineStep):\n",
    "    def __init__(self, params: Dict = LGB_DEFAULT_PARAMS, train_eval_sets = {}, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        if not params:\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"boosting_type\": \"gbdt\",\n",
    "                \"num_leaves\": 31,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"feature_fraction\": 0.9,\n",
    "                \"bagging_fraction\": 0.8,\n",
    "                \"bagging_freq\": 5,\n",
    "                \"n_estimators\": 1000,\n",
    "                \"verbose\": -1\n",
    "            }\n",
    "        if not train_eval_sets:\n",
    "            train_eval_sets = {\n",
    "                \"X_train\": \"X_train\",\n",
    "                \"y_train\": \"y_train\",\n",
    "                \"X_eval\": \"X_eval\",\n",
    "                \"y_eval\": \"y_eval\",\n",
    "                \"eval_data\": \"eval_data\",\n",
    "            }\n",
    "        self.params = params\n",
    "        self.train_eval_sets = train_eval_sets\n",
    "\n",
    "    def execute(self, pipeline: Pipeline, params=None) -> None:\n",
    "        X_train = pipeline.get_artifact(self.train_eval_sets[\"X_train\"])\n",
    "        y_train = pipeline.get_artifact(self.train_eval_sets[\"y_train\"])\n",
    "        X_eval = pipeline.get_artifact(self.train_eval_sets[\"X_eval\"])\n",
    "        y_eval = pipeline.get_artifact(self.train_eval_sets[\"y_eval\"])\n",
    "        df_eval = pipeline.get_artifact(self.train_eval_sets[\"eval_data\"])\n",
    "\n",
    "        cat_features = [col for col in X_train.columns if X_train[col].dtype.name == 'category']\n",
    "\n",
    "        \n",
    "        params = params or self.params\n",
    "        weight = X_train['weight'] if 'weight' in X_train.columns else None\n",
    "        weight_eval = X_eval['weight'] if 'weight' in X_eval.columns else None\n",
    "        train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features, weight=weight)\n",
    "        eval_data = lgb.Dataset(X_eval, label=y_eval, reference=train_data, categorical_feature=cat_features, weight=weight_eval)\n",
    "        custom_metric = CustomMetric(df_eval, product_id_col='product_id')\n",
    "        callbacks = [\n",
    "            #lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(100),\n",
    "        ]\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            #num_boost_round=1200,\n",
    "            #num_boost_round=50, # test\n",
    "            valid_sets=[eval_data],\n",
    "            feval=custom_metric,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        return {\"model\": model}\n",
    "    \n",
    "\n",
    "class MLPPipelineModel:\n",
    "    def __init__(self, params: Dict = None):\n",
    "        self.params = params or {\n",
    "            \"hidden_layer_sizes\": (256, 128, 64, 32),\n",
    "            \"activation\": \"relu\",\n",
    "            \"solver\": \"adam\",\n",
    "            \"alpha\": 0.0001,\n",
    "            \"batch_size\": \"auto\",\n",
    "            \"learning_rate\": \"adaptive\",\n",
    "            \"learning_rate_init\": 0.001,\n",
    "            \"max_iter\": 500,\n",
    "            \"early_stopping\": True,\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": True,\n",
    "        }\n",
    "        self.model = None\n",
    "        self.feature_columns = None  # Para guardar las columnas después del one-hot\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.params.update(params)\n",
    "\n",
    "    def _preprocess(self, X):\n",
    "        # Convierte variables categóricas a dummies\n",
    "        X_proc = pd.get_dummies(X, drop_first=True)\n",
    "        # Si ya entrenamos, aseguramos que las columnas coincidan\n",
    "        if self.feature_columns is not None:\n",
    "            for col in self.feature_columns:\n",
    "                if col not in X_proc:\n",
    "                    X_proc[col] = 0\n",
    "            X_proc = X_proc[self.feature_columns]\n",
    "        # Reemplaza NaN por 0\n",
    "        X_proc = X_proc.fillna(0)\n",
    "        return X_proc\n",
    "\n",
    "    def fit(self, X_train, y_train, X_eval=None, y_eval=None):\n",
    "        if isinstance(y_train, pd.DataFrame) and y_train.shape[1] > 1:\n",
    "            raise ValueError(\"y_train debe ser una Serie para regresión single-target.\")\n",
    "        y_train = y_train.dropna()\n",
    "        X_train = X_train.loc[y_train.index]\n",
    "        X_train_proc = pd.get_dummies(X_train, drop_first=True)\n",
    "        X_train_proc = X_train_proc.fillna(0)  # <--- asegurate de esto\n",
    "        self.feature_columns = X_train_proc.columns.tolist()\n",
    "        self.model = MLPRegressor(**self.params)\n",
    "        self.model.fit(X_train_proc, y_train)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been entrenado aún.\")\n",
    "        X_proc = self._preprocess(X)\n",
    "        return pd.Series(self.model.predict(X_proc), index=X.index, name='predictions')\n",
    "\n",
    "# PredictStep\n",
    "from typing import Optional, Dict\n",
    "import pandas as pd\n",
    "\n",
    "class SplitDataFrameStep(PipelineStep):\n",
    "    def __init__(\n",
    "            self, \n",
    "            test_date=\"2019-12\", \n",
    "            df=\"df\", \n",
    "            gap=0,\n",
    "            name: Optional[str] = None\n",
    "        ):\n",
    "        super().__init__(name)\n",
    "        self.test_date = test_date\n",
    "        self.df = df\n",
    "        self.gap = gap \n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        df = pipeline.get_artifact(self.df)\n",
    "        test_df = df[df[\"fecha\"] == self.test_date]\n",
    "        train_df = df[df[\"fecha\"] < self.test_date]\n",
    "        last_train_date = train_df[\"fecha\"].max()\n",
    "        if isinstance(last_train_date, pd.Period):\n",
    "            last_train_date = last_train_date.to_timestamp()\n",
    "        gap_date = pd.to_datetime(last_train_date) - pd.DateOffset(months=self.gap)\n",
    "        # Convert gap_date to Period with same freq as fecha\n",
    "        if pd.api.types.is_period_dtype(df[\"fecha\"]):\n",
    "            gap_date = pd.Period(gap_date, freq=df[\"fecha\"].dt.freq)\n",
    "        train_df = train_df[train_df[\"fecha\"] < gap_date]\n",
    "        return {\n",
    "            \"train_index\": train_df.index,\n",
    "            \"test_index\": test_df.index\n",
    "        }\n",
    "\n",
    "\n",
    "class PrepareXYStep(PipelineStep):\n",
    "    def execute(self, df, train_index, test_index) -> None:\n",
    "        columns = df.columns\n",
    "        #features = [col for col in columns if col != \"fecha\" and \"target\" not in col]\n",
    "        features = [col for col in columns if col != \"fecha\" and \"target\" not in col]\n",
    "        targets = [col for col in columns if \"target\" in col]\n",
    "        X_train = df.loc[train_index][features]\n",
    "        y_train = df.loc[train_index][targets]\n",
    "        X_test = df.loc[test_index][features]\n",
    "        y_test = df.loc[test_index][targets]\n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"targets\": targets,\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "\n",
    "\n",
    "class CreateTargetColumStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2)    \n",
    "        return {\"df\": df, \"target_col\": self.target_col}\n",
    "    \n",
    "\n",
    "class CreateMultiDiffTargetColumStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target_1'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-1) - df[self.target_col]\n",
    "        df['target_2'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2) - df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-1)\n",
    "        return {\n",
    "            \"df\": df, \n",
    "            \"target_col\": self.target_col,\n",
    "            \"needs_integration\": True,\n",
    "            #\"integration_function\": lambda x: x[self.target] + x['target_1'] + x['target_2']\n",
    "        }\n",
    "\n",
    "\n",
    "class CreateTargetColumDiffStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df.drop(columns=[\"target\"], inplace=True, errors='ignore')\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2) - df[self.target_col]\n",
    "        return {\n",
    "            \"df\": df, \n",
    "            \"target_col\": self.target_col,\n",
    "            \"needs_integration\": True,\n",
    "            #\"integration_function\": lambda x: x[self.target] + x['target']\n",
    "        }\n",
    "    \n",
    "\n",
    "class PredictStep(PipelineStep):\n",
    "    def execute(self, df, test_index, model, features) -> None:\n",
    "        X_predict = df.loc[test_index][features]\n",
    "        predictions = model.predict(X_predict)\n",
    "        return {\"predictions\": predictions}\n",
    "\n",
    "\n",
    "class IntegratePredictionsStep(PipelineStep):\n",
    "    def execute(self, df, predictions, test_index, target_col, needs_integration=False) -> Dict:\n",
    "        if not needs_integration:\n",
    "            return {\n",
    "                \"y_test\": df.loc[test_index, [\"target\"]]\n",
    "            }\n",
    "        # crea un nuevo dataframe que es la suma de todas las columnas de predicciones\n",
    "        if predictions.ndim == 1:\n",
    "            predictions_sum = pd.Series(predictions, index=test_index, name='predictions')\n",
    "        else:\n",
    "            predictions_sum = predictions.sum(axis=1)\n",
    "        final_predictions = predictions_sum + df.loc[test_index, target_col]\n",
    "        predictions = pd.Series(final_predictions, index=test_index, name='predictions')\n",
    "        target_columns = [col for col in df.columns if 'target' in col]\n",
    "        test_sum = df.loc[test_index, target_columns].sum(axis=1)\n",
    "        y_test = test_sum + df.loc[test_index, target_col]\n",
    "        y_test = pd.DataFrame(y_test, index=test_index, columns=[\"target\"])\n",
    "        \n",
    "        # nuevo approach, uso integration_function\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"y_test\": y_test\n",
    "        }\n",
    "    \n",
    "\n",
    "## legacy code\n",
    "class IntegratePredictionsStepOld(PipelineStep):\n",
    "    def execute(self, pipeline, predict_set, predictions, target_col, test) -> Dict:\n",
    "        \"\"\"\n",
    "        Integra las predicciones al DataFrame de test.\n",
    "        Si el target_col es una diferencia, se suma el último valor de target_col al target.\n",
    "        \"\"\"\n",
    "        pred_original_df = pipeline.get_artifact(predict_set)\n",
    "        predictions[\"predictions\"] = predictions[\"predictions\"] + pred_original_df[target_col]\n",
    "        test[\"target\"] = test[\"target\"] + test[target_col]\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"test\": test\n",
    "        } \n",
    "\n",
    "\n",
    "\n",
    "class SplitDataFrameStepOld(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        sorted_dated = sorted(df[\"fecha\"].unique())\n",
    "        last_date = sorted_dated[-1] # es 12-2019\n",
    "        last_test_date = sorted_dated[-3] # needs a gap because forecast moth+2\n",
    "        last_train_date = sorted_dated[-4] #\n",
    "\n",
    "        kaggle_pred = df[df[\"fecha\"] == last_date]\n",
    "        test = df[df[\"fecha\"] == last_test_date]\n",
    "        eval_data = df[df[\"fecha\"] == last_train_date]\n",
    "        train = df[(df[\"fecha\"] < last_train_date)]\n",
    "        return {\n",
    "            \"train\": train,\n",
    "            \"eval_data\": eval_data,\n",
    "            \"test\": test,\n",
    "            \"kaggle_pred\": kaggle_pred\n",
    "        }\n",
    "    \n",
    "\n",
    "class PrepareXYStepOld(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, train, eval_data, test, kaggle_pred) -> None:\n",
    "        features = [col for col in train.columns if col not in\n",
    "                        ['fecha', 'target']]\n",
    "        target = 'target'\n",
    "\n",
    "        X_train = pd.concat([train[features], eval_data[features]]) # [train + eval] + [eval] -> [test] \n",
    "        y_train = pd.concat([train[target], eval_data[target]])\n",
    "\n",
    "        X_train_alone = train[features]\n",
    "        y_train_alone = train[target]\n",
    "\n",
    "        X_eval = eval_data[features]\n",
    "        y_eval = eval_data[target]\n",
    "\n",
    "        X_test = test[features]\n",
    "        y_test = test[target]\n",
    "\n",
    "        X_train_final = pd.concat([train[features], eval_data[features], test[features]])\n",
    "        y_train_final = pd.concat([train[target], eval_data[target], test[target]])\n",
    "\n",
    "        X_kaggle = kaggle_pred[features]\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_train_alone\": X_train_alone,\n",
    "            \"y_train_alone\": y_train_alone,\n",
    "            \"X_eval\": X_eval,\n",
    "            \"y_eval\": y_eval,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "            \"X_train_final\": X_train_final,\n",
    "            \"y_train_final\": y_train_final,\n",
    "            \"X_kaggle\": X_kaggle\n",
    "        }\n",
    "        \n",
    "\n",
    "# InverseScalePredictionsStep\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "class PipelineScaler(ABC):\n",
    "    def __init__(self, column: str):\n",
    "        self.column = column\n",
    "        self.scaler_data = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        pass\n",
    "\n",
    "# TODO: hacer transformacion log1p si es necesario\n",
    "# TODO: debuggear, por alguna razon da mal\n",
    "\n",
    "class PipelineRobustScaler(PipelineScaler):\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])[self.column]  # SeriesGroupBy\n",
    "        median = grouped.median()\n",
    "        q1 = grouped.apply(lambda x: x.quantile(0.25))\n",
    "        q3 = grouped.apply(lambda x: x.quantile(0.75))\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        agg = pd.DataFrame({\n",
    "            f'{self.column}_median_scaler': median,\n",
    "            f'{self.column}_iqr_scaler': iqr\n",
    "        })\n",
    "        print(agg.head())\n",
    "        self.scaler_data = agg\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        original_index = df.index\n",
    "        original_nans = df[self.column].isna()\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(original_index, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = (df[self.column] - df[f'{self.column}_median_scaler']) / (df[f'{self.column}_iqr_scaler'])\n",
    "        # replace inf and -inf with NaN\n",
    "        df[f'{self.column}_scaled'].replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].fillna(0)\n",
    "        # original nans\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].where(~original_nans, other=pd.NA)\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_median_scaler', f'{self.column}_iqr_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}_scaled\"]\n",
    "\n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        return self.fit(df).transform(df)\n",
    "\n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "\n",
    "        # agrego columnas temporales\n",
    "        df_index = df.index\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        # reconstruyo los indices originales\n",
    "        df.set_index(df_index, inplace=True)\n",
    "        df[f\"{self.column}\"] = (df[f'{self.column}'] * (df[f'{self.column}_iqr_scaler'])) + df[f'{self.column}_median_scaler']\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_median_scaler', f'{self.column}_iqr_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}\"]\n",
    "\n",
    "\n",
    "class PipelineStandarScaler(PipelineScaler):\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        agg = df.groupby(['product_id', 'customer_id'])[self.column].agg(['mean', 'std']).rename(\n",
    "            columns={'mean': f'{self.column}_mean_scaler', 'std': f'{self.column}_std_scaler'})\n",
    "        self.scaler_data = agg\n",
    "        #self.scaler_data.fillna(0, inplace=True)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        original_index = df.index\n",
    "        original_nans = df[self.column].isna()\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(original_index, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = (df[self.column] - df[f'{self.column}_mean_scaler']) / (df[f'{self.column}_std_scaler'])\n",
    "        # replace inf and -inf with NaN\n",
    "        df[f'{self.column}_scaled'].replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "        # original nans\n",
    "        # hago un fill nan de las rows que no eran nan en la serie original\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].fillna(0)\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].where(~original_nans, other=pd.NA)\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_mean_scaler', f'{self.column}_std_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}_scaled\"]\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        return self.fit(df).transform(df)\n",
    "    \n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "\n",
    "        # agrego columnas temporales\n",
    "        df_index = df.index\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        # reconstruyo los indices originales\n",
    "        df.set_index(df_index, inplace=True)\n",
    "        df[f\"{self.column}\"] = (df[f'{self.column}'] * (df[f'{self.column}_std_scaler'])) + df[f'{self.column}_mean_scaler']\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_mean_scaler', f'{self.column}_std_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}\"]\n",
    "    \n",
    "\n",
    "class PipelineMinMaxScaler(PipelineScaler):\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        agg = df.groupby(['product_id', 'customer_id'])[self.column].agg(['min', 'max']).rename(\n",
    "            columns={'min': f'{self.column}_min_scaler', 'max': f'{self.column}_max_scaler'})\n",
    "        # seteo el minimo con 0 asi queda estandarlizado en todas las series\n",
    "        agg[f'{self.column}_min_scaler'] = 0\n",
    "        self.scaler_data = agg\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        original_index = df.index\n",
    "        original_nans = df[self.column].isna()\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(original_index, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = (df[self.column] - df[f'{self.column}_min_scaler']) / (df[f'{self.column}_max_scaler'] - df[f'{self.column}_min_scaler'])\n",
    "        df[f'{self.column}_scaled'].replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "        # original nans\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].fillna(0)\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].where(~original_nans, other=pd.NA)\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_min_scaler', f'{self.column}_max_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}_scaled\"]\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        return self.fit(df).transform(df)\n",
    "    \n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        df_index = df.index\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(df_index, inplace=True)\n",
    "\n",
    "        df[f\"{self.column}\"] = (df[f'{self.column}'] * (df[f'{self.column}_max_scaler'] - df[f'{self.column}_min_scaler'])) + df[f'{self.column}_min_scaler']\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_min_scaler', f'{self.column}_max_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}\"]\n",
    "    \n",
    "\n",
    "class ScaleFeatureStep(PipelineStep):\n",
    "    def __init__(self, column: str, regex=False, override=False, scaler=PipelineStandarScaler, name = None,):\n",
    "        super().__init__(name)\n",
    "        self.column = column\n",
    "        self.scaler_cls = scaler\n",
    "        self.regex = regex\n",
    "        self.override = override\n",
    "\n",
    "    def execute(self, df: pd.DataFrame, train_index) -> Dict:\n",
    "        # si regex es True, busco todas las columnas que coincidan con el regex\n",
    "        if self.regex:\n",
    "            columns = df.filter(regex=self.column, axis=1).columns.tolist()\n",
    "            print(f\"Columns found matching regex '{self.column}': {columns}\")\n",
    "            if not columns:\n",
    "                raise ValueError(f\"No columns found matching regex '{self.column}'\")\n",
    "        else:\n",
    "            columns = [self.column]\n",
    "        scalers = {}\n",
    "        for column in columns:\n",
    "            scaler = self.scaler_cls(\n",
    "                column=column,\n",
    "            )\n",
    "            if self.override:\n",
    "                column_scaled = column\n",
    "            else:\n",
    "                column_scaled = f\"{column}_scaled\"\n",
    "            scaler.fit(df[[\"product_id\", \"customer_id\", column]])\n",
    "            df[column_scaled] = scaler.transform(df[[\"product_id\", \"customer_id\", column]])\n",
    "            scalers[f\"scaler_{column_scaled}\"] = scaler\n",
    "        ret = {\"df\": df, **scalers}\n",
    "        return ret\n",
    "    \n",
    "\n",
    "class InverseScalePredictionsStep(PipelineStep):\n",
    "    def execute(self, predictions, df, test_index, scaler_target=None) -> Dict:\n",
    "        \"\"\"\n",
    "        Inverse scale the predictions using the provided grouped scaler.\n",
    "        \"\"\"\n",
    "        if not scaler_target:\n",
    "            return\n",
    "\n",
    "        # creo un df predictions_df que tiene predictions, product_id y customer_id de df para los indices de predictions\n",
    "        predictions_df = pd.DataFrame(predictions, index=predictions.index)\n",
    "        predictions_df[\"product_id\"] = df[\"product_id\"]\n",
    "        predictions_df[\"customer_id\"] = df[\"customer_id\"]\n",
    "        predictions_df.columns = [\"target\", \"product_id\", \"customer_id\"]\n",
    "        predictions = scaler_target.inverse_transform(predictions_df)\n",
    "        predictions = pd.Series(predictions, name=\"predictions\")\n",
    "        predictions.index = test_index\n",
    "        predictions.fillna(0, inplace=True)\n",
    "\n",
    "        df[\"target\"] = scaler_target.inverse_transform(df[[\"target\", \"product_id\", \"customer_id\"]])    \n",
    " \n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"df\": df\n",
    "        }\n",
    "\n",
    "# IntegratePredictionsStep\n",
    "from typing import Optional, Dict\n",
    "import pandas as pd\n",
    "\n",
    "class SplitDataFrameStep(PipelineStep):\n",
    "    def __init__(\n",
    "            self, \n",
    "            test_date=\"2019-12\", \n",
    "            df=\"df\", \n",
    "            gap=0,\n",
    "            name: Optional[str] = None\n",
    "        ):\n",
    "        super().__init__(name)\n",
    "        self.test_date = test_date\n",
    "        self.df = df\n",
    "        self.gap = gap \n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        df = pipeline.get_artifact(self.df)\n",
    "        test_df = df[df[\"fecha\"] == self.test_date]\n",
    "        train_df = df[df[\"fecha\"] < self.test_date]\n",
    "        last_train_date = train_df[\"fecha\"].max()\n",
    "        if isinstance(last_train_date, pd.Period):\n",
    "            last_train_date = last_train_date.to_timestamp()\n",
    "        gap_date = pd.to_datetime(last_train_date) - pd.DateOffset(months=self.gap)\n",
    "        # Convert gap_date to Period with same freq as fecha\n",
    "        if pd.api.types.is_period_dtype(df[\"fecha\"]):\n",
    "            gap_date = pd.Period(gap_date, freq=df[\"fecha\"].dt.freq)\n",
    "        train_df = train_df[train_df[\"fecha\"] < gap_date]\n",
    "        return {\n",
    "            \"train_index\": train_df.index,\n",
    "            \"test_index\": test_df.index\n",
    "        }\n",
    "\n",
    "\n",
    "class PrepareXYStep(PipelineStep):\n",
    "    def execute(self, df, train_index, test_index) -> None:\n",
    "        columns = df.columns\n",
    "        #features = [col for col in columns if col != \"fecha\" and \"target\" not in col]\n",
    "        features = [col for col in columns if col != \"fecha\" and \"target\" not in col]\n",
    "        targets = [col for col in columns if \"target\" in col]\n",
    "        X_train = df.loc[train_index][features]\n",
    "        y_train = df.loc[train_index][targets]\n",
    "        X_test = df.loc[test_index][features]\n",
    "        y_test = df.loc[test_index][targets]\n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"targets\": targets,\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "\n",
    "\n",
    "class CreateTargetColumStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2)    \n",
    "        return {\"df\": df, \"target_col\": self.target_col}\n",
    "    \n",
    "\n",
    "class CreateMultiDiffTargetColumStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target_1'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-1) - df[self.target_col]\n",
    "        df['target_2'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2) - df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-1)\n",
    "        return {\n",
    "            \"df\": df, \n",
    "            \"target_col\": self.target_col,\n",
    "            \"needs_integration\": True,\n",
    "            #\"integration_function\": lambda x: x[self.target] + x['target_1'] + x['target_2']\n",
    "        }\n",
    "\n",
    "\n",
    "class CreateTargetColumDiffStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df.drop(columns=[\"target\"], inplace=True, errors='ignore')\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2) - df[self.target_col]\n",
    "        return {\n",
    "            \"df\": df, \n",
    "            \"target_col\": self.target_col,\n",
    "            \"needs_integration\": True,\n",
    "            #\"integration_function\": lambda x: x[self.target] + x['target']\n",
    "        }\n",
    "    \n",
    "\n",
    "class PredictStep(PipelineStep):\n",
    "    def execute(self, df, test_index, model, features) -> None:\n",
    "        X_predict = df.loc[test_index][features]\n",
    "        predictions = model.predict(X_predict)\n",
    "        return {\"predictions\": predictions}\n",
    "\n",
    "\n",
    "class IntegratePredictionsStep(PipelineStep):\n",
    "    def execute(self, df, predictions, test_index, target_col, needs_integration=False) -> Dict:\n",
    "        if not needs_integration:\n",
    "            return {\n",
    "                \"y_test\": df.loc[test_index, [\"target\"]]\n",
    "            }\n",
    "        # crea un nuevo dataframe que es la suma de todas las columnas de predicciones\n",
    "        if predictions.ndim == 1:\n",
    "            predictions_sum = pd.Series(predictions, index=test_index, name='predictions')\n",
    "        else:\n",
    "            predictions_sum = predictions.sum(axis=1)\n",
    "        final_predictions = predictions_sum + df.loc[test_index, target_col]\n",
    "        predictions = pd.Series(final_predictions, index=test_index, name='predictions')\n",
    "        target_columns = [col for col in df.columns if 'target' in col]\n",
    "        test_sum = df.loc[test_index, target_columns].sum(axis=1)\n",
    "        y_test = test_sum + df.loc[test_index, target_col]\n",
    "        y_test = pd.DataFrame(y_test, index=test_index, columns=[\"target\"])\n",
    "        \n",
    "        # nuevo approach, uso integration_function\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"y_test\": y_test\n",
    "        }\n",
    "    \n",
    "\n",
    "## legacy code\n",
    "class IntegratePredictionsStepOld(PipelineStep):\n",
    "    def execute(self, pipeline, predict_set, predictions, target_col, test) -> Dict:\n",
    "        \"\"\"\n",
    "        Integra las predicciones al DataFrame de test.\n",
    "        Si el target_col es una diferencia, se suma el último valor de target_col al target.\n",
    "        \"\"\"\n",
    "        pred_original_df = pipeline.get_artifact(predict_set)\n",
    "        predictions[\"predictions\"] = predictions[\"predictions\"] + pred_original_df[target_col]\n",
    "        test[\"target\"] = test[\"target\"] + test[target_col]\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"test\": test\n",
    "        } \n",
    "\n",
    "\n",
    "\n",
    "class SplitDataFrameStepOld(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        sorted_dated = sorted(df[\"fecha\"].unique())\n",
    "        last_date = sorted_dated[-1] # es 12-2019\n",
    "        last_test_date = sorted_dated[-3] # needs a gap because forecast moth+2\n",
    "        last_train_date = sorted_dated[-4] #\n",
    "\n",
    "        kaggle_pred = df[df[\"fecha\"] == last_date]\n",
    "        test = df[df[\"fecha\"] == last_test_date]\n",
    "        eval_data = df[df[\"fecha\"] == last_train_date]\n",
    "        train = df[(df[\"fecha\"] < last_train_date)]\n",
    "        return {\n",
    "            \"train\": train,\n",
    "            \"eval_data\": eval_data,\n",
    "            \"test\": test,\n",
    "            \"kaggle_pred\": kaggle_pred\n",
    "        }\n",
    "    \n",
    "\n",
    "class PrepareXYStepOld(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, train, eval_data, test, kaggle_pred) -> None:\n",
    "        features = [col for col in train.columns if col not in\n",
    "                        ['fecha', 'target']]\n",
    "        target = 'target'\n",
    "\n",
    "        X_train = pd.concat([train[features], eval_data[features]]) # [train + eval] + [eval] -> [test] \n",
    "        y_train = pd.concat([train[target], eval_data[target]])\n",
    "\n",
    "        X_train_alone = train[features]\n",
    "        y_train_alone = train[target]\n",
    "\n",
    "        X_eval = eval_data[features]\n",
    "        y_eval = eval_data[target]\n",
    "\n",
    "        X_test = test[features]\n",
    "        y_test = test[target]\n",
    "\n",
    "        X_train_final = pd.concat([train[features], eval_data[features], test[features]])\n",
    "        y_train_final = pd.concat([train[target], eval_data[target], test[target]])\n",
    "\n",
    "        X_kaggle = kaggle_pred[features]\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_train_alone\": X_train_alone,\n",
    "            \"y_train_alone\": y_train_alone,\n",
    "            \"X_eval\": X_eval,\n",
    "            \"y_eval\": y_eval,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "            \"X_train_final\": X_train_final,\n",
    "            \"y_train_final\": y_train_final,\n",
    "            \"X_kaggle\": X_kaggle\n",
    "        }\n",
    "        \n",
    "\n",
    "# EvaluatePredictionsSteps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import pickle\n",
    "from typing import Optional\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EvaluatePredictionsSteps(PipelineStep):\n",
    "\n",
    "    def execute(self, df, y_test, predictions, test_index) -> None:\n",
    "        \n",
    "        eval_df_total = pd.DataFrame({\n",
    "            \"product_id\": df.loc[test_index, \"product_id\"],\n",
    "            \"customer_id\": df.loc[test_index, \"customer_id\"],\n",
    "            \"target\": y_test[\"target\"].values,\n",
    "            \"predictions\": predictions.values\n",
    "        })\n",
    "\n",
    "        eval_df = eval_df_total.groupby([\"product_id\"]).agg({\n",
    "            \"target\": \"sum\",\n",
    "            \"predictions\": \"sum\"\n",
    "        }).reset_index()\n",
    "\n",
    "        eval_df['tn_real'] = eval_df['target']\n",
    "        eval_df['tn_pred'] = eval_df['predictions']\n",
    "\n",
    "        total_error = np.sum(np.abs(eval_df['tn_real'] - eval_df['tn_pred'])) / np.sum(eval_df['tn_real'])\n",
    "        print(f\"Error en test: {total_error:.4f}\")\n",
    "        print(\"\\nTop 5 productos con mayor error absoluto:\")\n",
    "        eval_df['error_absoluto'] = np.abs(eval_df['tn_real'] - eval_df['tn_pred'])\n",
    "        print(eval_df.sort_values('error_absoluto', ascending=False).head())\n",
    "        return {\n",
    "            \"eval_df\": eval_df,\n",
    "            \"eval_df_total\": eval_df_total,\n",
    "            \"total_error\": total_error\n",
    "        }\n",
    "\n",
    "\n",
    "class PlotFeatureImportanceStep(PipelineStep):\n",
    "    def execute(self, model) -> None:\n",
    "        importance = pd.DataFrame()\n",
    "        model.plot_importance(max_num_features=20)\n",
    "\n",
    "\n",
    "class KaggleSubmissionStep(PipelineStep):\n",
    "    def execute(self, df, test_index, predictions) -> None:\n",
    "        submission_aux_df = pd.DataFrame({\n",
    "            \"product_id\": df.loc[test_index, \"product_id\"],\n",
    "            \"customer_id\": df.loc[test_index, \"customer_id\"],\n",
    "            \"predictions\": predictions.values\n",
    "        })\n",
    "        submission = submission_aux_df.groupby(\"product_id\")[\"predictions\"].sum().reset_index()\n",
    "        submission.columns = [\"product_id\", \"tn\"]\n",
    "        return {\"submission\": submission}\n",
    "    \n",
    "\n",
    "class SaveSubmissionStep(PipelineStep):\n",
    "    def __init__(self, exp_name: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.exp_name = exp_name\n",
    "\n",
    "    def execute(self, submission, total_error) -> None:\n",
    "        # Create the experiment directory\n",
    "        exp_name = f\"{str(datetime.datetime.now())}_{self.exp_name}\"\n",
    "        exp_dir = f\"experiments/{exp_name}\"\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "        # Save the submission file\n",
    "        submission.to_csv(os.path.join(exp_dir, f\"submission_{self.exp_name}_{total_error}.csv\"), index=False)\n",
    "        \n",
    "\n",
    "class SaveExperimentStep(PipelineStep):\n",
    "    def __init__(self, exp_name: str, save_dataframes=False, name = None):\n",
    "        super().__init__(name)\n",
    "        self.exp_name = exp_name\n",
    "        self.save_dataframes = save_dataframes\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "\n",
    "        # Create the experiment directory\n",
    "        exp_dir = f\"experiments/{self.exp_name}\"\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "        # obtengo el model\n",
    "        model = pipeline.get_artifact(\"model\")\n",
    "        # Save the model as a pickle file\n",
    "        with open(os.path.join(exp_dir, \"model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        # guardo el error total de test\n",
    "        total_error = pipeline.get_artifact(\"total_error\")\n",
    "        with open(os.path.join(exp_dir, \"total_error.txt\"), \"w\") as f:\n",
    "            f.write(str(total_error))\n",
    "\n",
    "        # Save the submission file\n",
    "        submission = pipeline.get_artifact(\"submission\")\n",
    "        submission.to_csv(os.path.join(exp_dir, f\"submission_{self.exp_name}_{total_error:.4f}.csv\"), index=False)\n",
    "\n",
    "        # borro submission model y error de los artifacts\n",
    "        pipeline.del_artifact(\"submission\")\n",
    "        \n",
    "        # Guardo los artifacts restantes que son dataframes como csvs\n",
    "        if self.save_dataframes:\n",
    "            for artifact_name, artifact in pipeline.artifacts.items():\n",
    "                if isinstance(artifact, pd.DataFrame):\n",
    "                    artifact.to_csv(os.path.join(exp_dir, f\"{artifact_name}.csv\"), index=False)\n",
    "\n",
    "\n",
    "        # Save a copy of the notebook\n",
    "        #notebook_path = fallback_latest_notebook()\n",
    "        #shutil.copy(notebook_path, os.path.join(exp_dir, f\"notebook_{self.exp_name}.ipynb\"))\n",
    "\n",
    "\n",
    "class SaveDataFrameStep(PipelineStep):\n",
    "    def __init__(self, df_name: str, file_name: str, ext = \"pickle\", name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.df_name = df_name\n",
    "        self.file_name = file_name\n",
    "        self.ext = ext\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        df = pipeline.get_artifact(self.df_name)\n",
    "        if self.ext == \"pickle\":\n",
    "            df.to_pickle(self.file_name)\n",
    "        elif self.ext == \"parquet\":\n",
    "            df.to_parquet(f\"{self.file_name}.parquet\", index=False)\n",
    "        elif self.ext == \"csv\":\n",
    "            df.to_csv(f\"{self.file_name}.csv\", index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {self.ext}\")\n",
    "\n",
    "\n",
    "class SaveScalerStep(PipelineStep):\n",
    "    def __init__(self, scaler_name: str, file_name: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.scaler_name = scaler_name\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        scaler = pipeline.get_artifact(self.scaler_name)\n",
    "        with open(self.file_name, \"wb\") as f:\n",
    "            pickle.dump(scaler, f)\n",
    "\n",
    "# PlotFeatureImportanceStep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import pickle\n",
    "from typing import Optional\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EvaluatePredictionsSteps(PipelineStep):\n",
    "\n",
    "    def execute(self, df, y_test, predictions, test_index) -> None:\n",
    "        \n",
    "        eval_df_total = pd.DataFrame({\n",
    "            \"product_id\": df.loc[test_index, \"product_id\"],\n",
    "            \"customer_id\": df.loc[test_index, \"customer_id\"],\n",
    "            \"target\": y_test[\"target\"].values,\n",
    "            \"predictions\": predictions.values\n",
    "        })\n",
    "\n",
    "        eval_df = eval_df_total.groupby([\"product_id\"]).agg({\n",
    "            \"target\": \"sum\",\n",
    "            \"predictions\": \"sum\"\n",
    "        }).reset_index()\n",
    "\n",
    "        eval_df['tn_real'] = eval_df['target']\n",
    "        eval_df['tn_pred'] = eval_df['predictions']\n",
    "\n",
    "        total_error = np.sum(np.abs(eval_df['tn_real'] - eval_df['tn_pred'])) / np.sum(eval_df['tn_real'])\n",
    "        print(f\"Error en test: {total_error:.4f}\")\n",
    "        print(\"\\nTop 5 productos con mayor error absoluto:\")\n",
    "        eval_df['error_absoluto'] = np.abs(eval_df['tn_real'] - eval_df['tn_pred'])\n",
    "        print(eval_df.sort_values('error_absoluto', ascending=False).head())\n",
    "        return {\n",
    "            \"eval_df\": eval_df,\n",
    "            \"eval_df_total\": eval_df_total,\n",
    "            \"total_error\": total_error\n",
    "        }\n",
    "\n",
    "\n",
    "class PlotFeatureImportanceStep(PipelineStep):\n",
    "    def execute(self, model) -> None:\n",
    "        importance = pd.DataFrame()\n",
    "        model.plot_importance(max_num_features=20)\n",
    "\n",
    "\n",
    "class KaggleSubmissionStep(PipelineStep):\n",
    "    def execute(self, df, test_index, predictions) -> None:\n",
    "        submission_aux_df = pd.DataFrame({\n",
    "            \"product_id\": df.loc[test_index, \"product_id\"],\n",
    "            \"customer_id\": df.loc[test_index, \"customer_id\"],\n",
    "            \"predictions\": predictions.values\n",
    "        })\n",
    "        submission = submission_aux_df.groupby(\"product_id\")[\"predictions\"].sum().reset_index()\n",
    "        submission.columns = [\"product_id\", \"tn\"]\n",
    "        return {\"submission\": submission}\n",
    "    \n",
    "\n",
    "class SaveSubmissionStep(PipelineStep):\n",
    "    def __init__(self, exp_name: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.exp_name = exp_name\n",
    "\n",
    "    def execute(self, submission, total_error) -> None:\n",
    "        # Create the experiment directory\n",
    "        exp_name = f\"{str(datetime.datetime.now())}_{self.exp_name}\"\n",
    "        exp_dir = f\"experiments/{exp_name}\"\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "        # Save the submission file\n",
    "        submission.to_csv(os.path.join(exp_dir, f\"submission_{self.exp_name}_{total_error}.csv\"), index=False)\n",
    "        \n",
    "\n",
    "class SaveExperimentStep(PipelineStep):\n",
    "    def __init__(self, exp_name: str, save_dataframes=False, name = None):\n",
    "        super().__init__(name)\n",
    "        self.exp_name = exp_name\n",
    "        self.save_dataframes = save_dataframes\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "\n",
    "        # Create the experiment directory\n",
    "        exp_dir = f\"experiments/{self.exp_name}\"\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "        # obtengo el model\n",
    "        model = pipeline.get_artifact(\"model\")\n",
    "        # Save the model as a pickle file\n",
    "        with open(os.path.join(exp_dir, \"model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        # guardo el error total de test\n",
    "        total_error = pipeline.get_artifact(\"total_error\")\n",
    "        with open(os.path.join(exp_dir, \"total_error.txt\"), \"w\") as f:\n",
    "            f.write(str(total_error))\n",
    "\n",
    "        # Save the submission file\n",
    "        submission = pipeline.get_artifact(\"submission\")\n",
    "        submission.to_csv(os.path.join(exp_dir, f\"submission_{self.exp_name}_{total_error:.4f}.csv\"), index=False)\n",
    "\n",
    "        # borro submission model y error de los artifacts\n",
    "        pipeline.del_artifact(\"submission\")\n",
    "        \n",
    "        # Guardo los artifacts restantes que son dataframes como csvs\n",
    "        if self.save_dataframes:\n",
    "            for artifact_name, artifact in pipeline.artifacts.items():\n",
    "                if isinstance(artifact, pd.DataFrame):\n",
    "                    artifact.to_csv(os.path.join(exp_dir, f\"{artifact_name}.csv\"), index=False)\n",
    "\n",
    "\n",
    "        # Save a copy of the notebook\n",
    "        #notebook_path = fallback_latest_notebook()\n",
    "        #shutil.copy(notebook_path, os.path.join(exp_dir, f\"notebook_{self.exp_name}.ipynb\"))\n",
    "\n",
    "\n",
    "class SaveDataFrameStep(PipelineStep):\n",
    "    def __init__(self, df_name: str, file_name: str, ext = \"pickle\", name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.df_name = df_name\n",
    "        self.file_name = file_name\n",
    "        self.ext = ext\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        df = pipeline.get_artifact(self.df_name)\n",
    "        if self.ext == \"pickle\":\n",
    "            df.to_pickle(self.file_name)\n",
    "        elif self.ext == \"parquet\":\n",
    "            df.to_parquet(f\"{self.file_name}.parquet\", index=False)\n",
    "        elif self.ext == \"csv\":\n",
    "            df.to_csv(f\"{self.file_name}.csv\", index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {self.ext}\")\n",
    "\n",
    "\n",
    "class SaveScalerStep(PipelineStep):\n",
    "    def __init__(self, scaler_name: str, file_name: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.scaler_name = scaler_name\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        scaler = pipeline.get_artifact(self.scaler_name)\n",
    "        with open(self.file_name, \"wb\") as f:\n",
    "            pickle.dump(scaler, f)\n",
    "\n",
    "# KaggleSubmissionStep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import pickle\n",
    "from typing import Optional\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EvaluatePredictionsSteps(PipelineStep):\n",
    "\n",
    "    def execute(self, df, y_test, predictions, test_index) -> None:\n",
    "        \n",
    "        eval_df_total = pd.DataFrame({\n",
    "            \"product_id\": df.loc[test_index, \"product_id\"],\n",
    "            \"customer_id\": df.loc[test_index, \"customer_id\"],\n",
    "            \"target\": y_test[\"target\"].values,\n",
    "            \"predictions\": predictions.values\n",
    "        })\n",
    "\n",
    "        eval_df = eval_df_total.groupby([\"product_id\"]).agg({\n",
    "            \"target\": \"sum\",\n",
    "            \"predictions\": \"sum\"\n",
    "        }).reset_index()\n",
    "\n",
    "        eval_df['tn_real'] = eval_df['target']\n",
    "        eval_df['tn_pred'] = eval_df['predictions']\n",
    "\n",
    "        total_error = np.sum(np.abs(eval_df['tn_real'] - eval_df['tn_pred'])) / np.sum(eval_df['tn_real'])\n",
    "        print(f\"Error en test: {total_error:.4f}\")\n",
    "        print(\"\\nTop 5 productos con mayor error absoluto:\")\n",
    "        eval_df['error_absoluto'] = np.abs(eval_df['tn_real'] - eval_df['tn_pred'])\n",
    "        print(eval_df.sort_values('error_absoluto', ascending=False).head())\n",
    "        return {\n",
    "            \"eval_df\": eval_df,\n",
    "            \"eval_df_total\": eval_df_total,\n",
    "            \"total_error\": total_error\n",
    "        }\n",
    "\n",
    "\n",
    "class PlotFeatureImportanceStep(PipelineStep):\n",
    "    def execute(self, model) -> None:\n",
    "        importance = pd.DataFrame()\n",
    "        model.plot_importance(max_num_features=20)\n",
    "\n",
    "\n",
    "class KaggleSubmissionStep(PipelineStep):\n",
    "    def execute(self, df, test_index, predictions) -> None:\n",
    "        submission_aux_df = pd.DataFrame({\n",
    "            \"product_id\": df.loc[test_index, \"product_id\"],\n",
    "            \"customer_id\": df.loc[test_index, \"customer_id\"],\n",
    "            \"predictions\": predictions.values\n",
    "        })\n",
    "        submission = submission_aux_df.groupby(\"product_id\")[\"predictions\"].sum().reset_index()\n",
    "        submission.columns = [\"product_id\", \"tn\"]\n",
    "        return {\"submission\": submission}\n",
    "    \n",
    "\n",
    "class SaveSubmissionStep(PipelineStep):\n",
    "    def __init__(self, exp_name: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.exp_name = exp_name\n",
    "\n",
    "    def execute(self, submission, total_error) -> None:\n",
    "        # Create the experiment directory\n",
    "        exp_name = f\"{str(datetime.datetime.now())}_{self.exp_name}\"\n",
    "        exp_dir = f\"experiments/{exp_name}\"\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "        # Save the submission file\n",
    "        submission.to_csv(os.path.join(exp_dir, f\"submission_{self.exp_name}_{total_error}.csv\"), index=False)\n",
    "        \n",
    "\n",
    "class SaveExperimentStep(PipelineStep):\n",
    "    def __init__(self, exp_name: str, save_dataframes=False, name = None):\n",
    "        super().__init__(name)\n",
    "        self.exp_name = exp_name\n",
    "        self.save_dataframes = save_dataframes\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "\n",
    "        # Create the experiment directory\n",
    "        exp_dir = f\"experiments/{self.exp_name}\"\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "        # obtengo el model\n",
    "        model = pipeline.get_artifact(\"model\")\n",
    "        # Save the model as a pickle file\n",
    "        with open(os.path.join(exp_dir, \"model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        # guardo el error total de test\n",
    "        total_error = pipeline.get_artifact(\"total_error\")\n",
    "        with open(os.path.join(exp_dir, \"total_error.txt\"), \"w\") as f:\n",
    "            f.write(str(total_error))\n",
    "\n",
    "        # Save the submission file\n",
    "        submission = pipeline.get_artifact(\"submission\")\n",
    "        submission.to_csv(os.path.join(exp_dir, f\"submission_{self.exp_name}_{total_error:.4f}.csv\"), index=False)\n",
    "\n",
    "        # borro submission model y error de los artifacts\n",
    "        pipeline.del_artifact(\"submission\")\n",
    "        \n",
    "        # Guardo los artifacts restantes que son dataframes como csvs\n",
    "        if self.save_dataframes:\n",
    "            for artifact_name, artifact in pipeline.artifacts.items():\n",
    "                if isinstance(artifact, pd.DataFrame):\n",
    "                    artifact.to_csv(os.path.join(exp_dir, f\"{artifact_name}.csv\"), index=False)\n",
    "\n",
    "\n",
    "        # Save a copy of the notebook\n",
    "        #notebook_path = fallback_latest_notebook()\n",
    "        #shutil.copy(notebook_path, os.path.join(exp_dir, f\"notebook_{self.exp_name}.ipynb\"))\n",
    "\n",
    "\n",
    "class SaveDataFrameStep(PipelineStep):\n",
    "    def __init__(self, df_name: str, file_name: str, ext = \"pickle\", name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.df_name = df_name\n",
    "        self.file_name = file_name\n",
    "        self.ext = ext\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        df = pipeline.get_artifact(self.df_name)\n",
    "        if self.ext == \"pickle\":\n",
    "            df.to_pickle(self.file_name)\n",
    "        elif self.ext == \"parquet\":\n",
    "            df.to_parquet(f\"{self.file_name}.parquet\", index=False)\n",
    "        elif self.ext == \"csv\":\n",
    "            df.to_csv(f\"{self.file_name}.csv\", index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {self.ext}\")\n",
    "\n",
    "\n",
    "class SaveScalerStep(PipelineStep):\n",
    "    def __init__(self, scaler_name: str, file_name: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.scaler_name = scaler_name\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        scaler = pipeline.get_artifact(self.scaler_name)\n",
    "        with open(self.file_name, \"wb\") as f:\n",
    "            pickle.dump(scaler, f)\n",
    "\n",
    "# SaveSubmissionStep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import pickle\n",
    "from typing import Optional\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EvaluatePredictionsSteps(PipelineStep):\n",
    "\n",
    "    def execute(self, df, y_test, predictions, test_index) -> None:\n",
    "        \n",
    "        eval_df_total = pd.DataFrame({\n",
    "            \"product_id\": df.loc[test_index, \"product_id\"],\n",
    "            \"customer_id\": df.loc[test_index, \"customer_id\"],\n",
    "            \"target\": y_test[\"target\"].values,\n",
    "            \"predictions\": predictions.values\n",
    "        })\n",
    "\n",
    "        eval_df = eval_df_total.groupby([\"product_id\"]).agg({\n",
    "            \"target\": \"sum\",\n",
    "            \"predictions\": \"sum\"\n",
    "        }).reset_index()\n",
    "\n",
    "        eval_df['tn_real'] = eval_df['target']\n",
    "        eval_df['tn_pred'] = eval_df['predictions']\n",
    "\n",
    "        total_error = np.sum(np.abs(eval_df['tn_real'] - eval_df['tn_pred'])) / np.sum(eval_df['tn_real'])\n",
    "        print(f\"Error en test: {total_error:.4f}\")\n",
    "        print(\"\\nTop 5 productos con mayor error absoluto:\")\n",
    "        eval_df['error_absoluto'] = np.abs(eval_df['tn_real'] - eval_df['tn_pred'])\n",
    "        print(eval_df.sort_values('error_absoluto', ascending=False).head())\n",
    "        return {\n",
    "            \"eval_df\": eval_df,\n",
    "            \"eval_df_total\": eval_df_total,\n",
    "            \"total_error\": total_error\n",
    "        }\n",
    "\n",
    "\n",
    "class PlotFeatureImportanceStep(PipelineStep):\n",
    "    def execute(self, model) -> None:\n",
    "        importance = pd.DataFrame()\n",
    "        model.plot_importance(max_num_features=20)\n",
    "\n",
    "\n",
    "class KaggleSubmissionStep(PipelineStep):\n",
    "    def execute(self, df, test_index, predictions) -> None:\n",
    "        submission_aux_df = pd.DataFrame({\n",
    "            \"product_id\": df.loc[test_index, \"product_id\"],\n",
    "            \"customer_id\": df.loc[test_index, \"customer_id\"],\n",
    "            \"predictions\": predictions.values\n",
    "        })\n",
    "        submission = submission_aux_df.groupby(\"product_id\")[\"predictions\"].sum().reset_index()\n",
    "        submission.columns = [\"product_id\", \"tn\"]\n",
    "        return {\"submission\": submission}\n",
    "    \n",
    "\n",
    "class SaveSubmissionStep(PipelineStep):\n",
    "    def __init__(self, exp_name: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.exp_name = exp_name\n",
    "\n",
    "    def execute(self, submission, total_error) -> None:\n",
    "        # Create the experiment directory\n",
    "        exp_name = f\"{str(datetime.datetime.now())}_{self.exp_name}\"\n",
    "        exp_dir = f\"experiments/{exp_name}\"\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "        # Save the submission file\n",
    "        submission.to_csv(os.path.join(exp_dir, f\"submission_{self.exp_name}_{total_error}.csv\"), index=False)\n",
    "        \n",
    "\n",
    "class SaveExperimentStep(PipelineStep):\n",
    "    def __init__(self, exp_name: str, save_dataframes=False, name = None):\n",
    "        super().__init__(name)\n",
    "        self.exp_name = exp_name\n",
    "        self.save_dataframes = save_dataframes\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "\n",
    "        # Create the experiment directory\n",
    "        exp_dir = f\"experiments/{self.exp_name}\"\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "        # obtengo el model\n",
    "        model = pipeline.get_artifact(\"model\")\n",
    "        # Save the model as a pickle file\n",
    "        with open(os.path.join(exp_dir, \"model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        # guardo el error total de test\n",
    "        total_error = pipeline.get_artifact(\"total_error\")\n",
    "        with open(os.path.join(exp_dir, \"total_error.txt\"), \"w\") as f:\n",
    "            f.write(str(total_error))\n",
    "\n",
    "        # Save the submission file\n",
    "        submission = pipeline.get_artifact(\"submission\")\n",
    "        submission.to_csv(os.path.join(exp_dir, f\"submission_{self.exp_name}_{total_error:.4f}.csv\"), index=False)\n",
    "\n",
    "        # borro submission model y error de los artifacts\n",
    "        pipeline.del_artifact(\"submission\")\n",
    "        \n",
    "        # Guardo los artifacts restantes que son dataframes como csvs\n",
    "        if self.save_dataframes:\n",
    "            for artifact_name, artifact in pipeline.artifacts.items():\n",
    "                if isinstance(artifact, pd.DataFrame):\n",
    "                    artifact.to_csv(os.path.join(exp_dir, f\"{artifact_name}.csv\"), index=False)\n",
    "\n",
    "\n",
    "        # Save a copy of the notebook\n",
    "        #notebook_path = fallback_latest_notebook()\n",
    "        #shutil.copy(notebook_path, os.path.join(exp_dir, f\"notebook_{self.exp_name}.ipynb\"))\n",
    "\n",
    "\n",
    "class SaveDataFrameStep(PipelineStep):\n",
    "    def __init__(self, df_name: str, file_name: str, ext = \"pickle\", name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.df_name = df_name\n",
    "        self.file_name = file_name\n",
    "        self.ext = ext\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        df = pipeline.get_artifact(self.df_name)\n",
    "        if self.ext == \"pickle\":\n",
    "            df.to_pickle(self.file_name)\n",
    "        elif self.ext == \"parquet\":\n",
    "            df.to_parquet(f\"{self.file_name}.parquet\", index=False)\n",
    "        elif self.ext == \"csv\":\n",
    "            df.to_csv(f\"{self.file_name}.csv\", index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {self.ext}\")\n",
    "\n",
    "\n",
    "class SaveScalerStep(PipelineStep):\n",
    "    def __init__(self, scaler_name: str, file_name: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.scaler_name = scaler_name\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def execute(self, pipeline) -> None:\n",
    "        scaler = pipeline.get_artifact(self.scaler_name)\n",
    "        with open(self.file_name, \"wb\") as f:\n",
    "            pickle.dump(scaler, f)\n",
    "\n",
    "# PipelineMinMaxScaler\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "class PipelineScaler(ABC):\n",
    "    def __init__(self, column: str):\n",
    "        self.column = column\n",
    "        self.scaler_data = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        pass\n",
    "\n",
    "# TODO: hacer transformacion log1p si es necesario\n",
    "# TODO: debuggear, por alguna razon da mal\n",
    "\n",
    "class PipelineRobustScaler(PipelineScaler):\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])[self.column]  # SeriesGroupBy\n",
    "        median = grouped.median()\n",
    "        q1 = grouped.apply(lambda x: x.quantile(0.25))\n",
    "        q3 = grouped.apply(lambda x: x.quantile(0.75))\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        agg = pd.DataFrame({\n",
    "            f'{self.column}_median_scaler': median,\n",
    "            f'{self.column}_iqr_scaler': iqr\n",
    "        })\n",
    "        print(agg.head())\n",
    "        self.scaler_data = agg\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        original_index = df.index\n",
    "        original_nans = df[self.column].isna()\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(original_index, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = (df[self.column] - df[f'{self.column}_median_scaler']) / (df[f'{self.column}_iqr_scaler'])\n",
    "        # replace inf and -inf with NaN\n",
    "        df[f'{self.column}_scaled'].replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].fillna(0)\n",
    "        # original nans\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].where(~original_nans, other=pd.NA)\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_median_scaler', f'{self.column}_iqr_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}_scaled\"]\n",
    "\n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        return self.fit(df).transform(df)\n",
    "\n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "\n",
    "        # agrego columnas temporales\n",
    "        df_index = df.index\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        # reconstruyo los indices originales\n",
    "        df.set_index(df_index, inplace=True)\n",
    "        df[f\"{self.column}\"] = (df[f'{self.column}'] * (df[f'{self.column}_iqr_scaler'])) + df[f'{self.column}_median_scaler']\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_median_scaler', f'{self.column}_iqr_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}\"]\n",
    "\n",
    "\n",
    "class PipelineStandarScaler(PipelineScaler):\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        agg = df.groupby(['product_id', 'customer_id'])[self.column].agg(['mean', 'std']).rename(\n",
    "            columns={'mean': f'{self.column}_mean_scaler', 'std': f'{self.column}_std_scaler'})\n",
    "        self.scaler_data = agg\n",
    "        #self.scaler_data.fillna(0, inplace=True)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        original_index = df.index\n",
    "        original_nans = df[self.column].isna()\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(original_index, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = (df[self.column] - df[f'{self.column}_mean_scaler']) / (df[f'{self.column}_std_scaler'])\n",
    "        # replace inf and -inf with NaN\n",
    "        df[f'{self.column}_scaled'].replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "        # original nans\n",
    "        # hago un fill nan de las rows que no eran nan en la serie original\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].fillna(0)\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].where(~original_nans, other=pd.NA)\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_mean_scaler', f'{self.column}_std_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}_scaled\"]\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        return self.fit(df).transform(df)\n",
    "    \n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "\n",
    "        # agrego columnas temporales\n",
    "        df_index = df.index\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        # reconstruyo los indices originales\n",
    "        df.set_index(df_index, inplace=True)\n",
    "        df[f\"{self.column}\"] = (df[f'{self.column}'] * (df[f'{self.column}_std_scaler'])) + df[f'{self.column}_mean_scaler']\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_mean_scaler', f'{self.column}_std_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}\"]\n",
    "    \n",
    "\n",
    "class PipelineMinMaxScaler(PipelineScaler):\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        agg = df.groupby(['product_id', 'customer_id'])[self.column].agg(['min', 'max']).rename(\n",
    "            columns={'min': f'{self.column}_min_scaler', 'max': f'{self.column}_max_scaler'})\n",
    "        # seteo el minimo con 0 asi queda estandarlizado en todas las series\n",
    "        agg[f'{self.column}_min_scaler'] = 0\n",
    "        self.scaler_data = agg\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        original_index = df.index\n",
    "        original_nans = df[self.column].isna()\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(original_index, inplace=True)\n",
    "        df[f'{self.column}_scaled'] = (df[self.column] - df[f'{self.column}_min_scaler']) / (df[f'{self.column}_max_scaler'] - df[f'{self.column}_min_scaler'])\n",
    "        df[f'{self.column}_scaled'].replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "        # original nans\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].fillna(0)\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].where(~original_nans, other=pd.NA)\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_min_scaler', f'{self.column}_max_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}_scaled\"]\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        return self.fit(df).transform(df)\n",
    "    \n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.Series:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        df_index = df.index\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df.set_index(df_index, inplace=True)\n",
    "\n",
    "        df[f\"{self.column}\"] = (df[f'{self.column}'] * (df[f'{self.column}_max_scaler'] - df[f'{self.column}_min_scaler'])) + df[f'{self.column}_min_scaler']\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_min_scaler', f'{self.column}_max_scaler'], inplace=True, errors='ignore')\n",
    "        return df[f\"{self.column}\"]\n",
    "    \n",
    "\n",
    "class ScaleFeatureStep(PipelineStep):\n",
    "    def __init__(self, column: str, regex=False, override=False, scaler=PipelineStandarScaler, name = None,):\n",
    "        super().__init__(name)\n",
    "        self.column = column\n",
    "        self.scaler_cls = scaler\n",
    "        self.regex = regex\n",
    "        self.override = override\n",
    "\n",
    "    def execute(self, df: pd.DataFrame, train_index) -> Dict:\n",
    "        # si regex es True, busco todas las columnas que coincidan con el regex\n",
    "        if self.regex:\n",
    "            columns = df.filter(regex=self.column, axis=1).columns.tolist()\n",
    "            print(f\"Columns found matching regex '{self.column}': {columns}\")\n",
    "            if not columns:\n",
    "                raise ValueError(f\"No columns found matching regex '{self.column}'\")\n",
    "        else:\n",
    "            columns = [self.column]\n",
    "        scalers = {}\n",
    "        for column in columns:\n",
    "            scaler = self.scaler_cls(\n",
    "                column=column,\n",
    "            )\n",
    "            if self.override:\n",
    "                column_scaled = column\n",
    "            else:\n",
    "                column_scaled = f\"{column}_scaled\"\n",
    "            scaler.fit(df[[\"product_id\", \"customer_id\", column]])\n",
    "            df[column_scaled] = scaler.transform(df[[\"product_id\", \"customer_id\", column]])\n",
    "            scalers[f\"scaler_{column_scaled}\"] = scaler\n",
    "        ret = {\"df\": df, **scalers}\n",
    "        return ret\n",
    "    \n",
    "\n",
    "class InverseScalePredictionsStep(PipelineStep):\n",
    "    def execute(self, predictions, df, test_index, scaler_target=None) -> Dict:\n",
    "        \"\"\"\n",
    "        Inverse scale the predictions using the provided grouped scaler.\n",
    "        \"\"\"\n",
    "        if not scaler_target:\n",
    "            return\n",
    "\n",
    "        # creo un df predictions_df que tiene predictions, product_id y customer_id de df para los indices de predictions\n",
    "        predictions_df = pd.DataFrame(predictions, index=predictions.index)\n",
    "        predictions_df[\"product_id\"] = df[\"product_id\"]\n",
    "        predictions_df[\"customer_id\"] = df[\"customer_id\"]\n",
    "        predictions_df.columns = [\"target\", \"product_id\", \"customer_id\"]\n",
    "        predictions = scaler_target.inverse_transform(predictions_df)\n",
    "        predictions = pd.Series(predictions, name=\"predictions\")\n",
    "        predictions.index = test_index\n",
    "        predictions.fillna(0, inplace=True)\n",
    "\n",
    "        df[\"target\"] = scaler_target.inverse_transform(df[[\"target\", \"product_id\", \"customer_id\"]])    \n",
    " \n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"df\": df\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_test_grande = Pipeline(\n",
    "    steps=[\n",
    "        LoadDataFrameFromPickleStep(path=\"df_fe_big.pickle\"),\n",
    "        SplitDataFrameStep(df=\"df\", test_date=\"2019-10\", gap=1),\n",
    "        CreateTargetColumStep(target_col=\"tn\"),\n",
    "        ScaleFeatureStep(column=\".*tn.*\", override=False, regex=True),  \n",
    "        ScaleFeatureStep(column=\"target\", override=True, scaler=PipelineMinMaxScaler),\n",
    "        ReduceMemoryUsageStep(),\n",
    "        PrepareXYStep(),\n",
    "        TrainModelStep(folds=0, params={\"max_bin\":1024, 'num_leaves': 31, \"n_estimators\": 700, \"learning_rate\": 0.01}),\n",
    "        PredictStep(),\n",
    "        InverseScalePredictionsStep(),\n",
    "        IntegratePredictionsStep(),\n",
    "        EvaluatePredictionsSteps(),\n",
    "        PlotFeatureImportanceStep(),\n",
    "        LoadDataFrameFromPickleStep(path=\"df_fe_big.pickle\"),\n",
    "        SplitDataFrameStep(df=\"df\", test_date=\"2019-12\", gap=1),\n",
    "        CreateTargetColumStep(target_col=\"tn\"),\n",
    "        ScaleFeatureStep(column=\".*tn.*\", override=False, regex=True),\n",
    "        ScaleFeatureStep(column=\"target\", override=True, scaler=PipelineMinMaxScaler),\n",
    "        ReduceMemoryUsageStep(),\n",
    "\n",
    "        PrepareXYStep(),\n",
    "        TrainModelStep(folds=0, params={\"max_bin\":1024, 'num_leaves': 31, \"n_estimators\": 700, \"learning_rate\": 0.01}),\n",
    "        PredictStep(),\n",
    "        InverseScalePredictionsStep(),\n",
    "        IntegratePredictionsStep(),\n",
    "        PlotFeatureImportanceStep(),\n",
    "        KaggleSubmissionStep(),\n",
    "        SaveSubmissionStep(exp_name=\"test_new_pipeline_150features\"),\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
