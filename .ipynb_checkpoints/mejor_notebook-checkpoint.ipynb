{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, List, Optional, Callable, Tuple, Self\n",
    "import time\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "from flaml import AutoML\n",
    "import os\n",
    "from glob import glob\n",
    "import inspect\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)        \n",
    "import cloudpickle\n",
    "import hashlib\n",
    "import inspect\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "def fallback_latest_notebook():\n",
    "    notebooks = glob(\"*.ipynb\")\n",
    "    if not notebooks:\n",
    "        return None\n",
    "    notebooks = sorted(notebooks, key=os.path.getmtime, reverse=True)\n",
    "    return notebooks[0]\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "class InDiskCacheWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper class to enable in-disk caching for pipeline steps.\n",
    "    It uses the InDiskCache class to cache artifacts on disk.\n",
    "    \"\"\"\n",
    "    def __init__(self, step: \"PipelineStep\", cache_dir: str = \".cache\", execute_params: Optional[Dict[str, Any]] = None):\n",
    "        self.step = step\n",
    "        self.cache_dir = os.path.join(cache_dir, step.name)\n",
    "        if not os.path.exists(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "        self._execute_params = execute_params or {}\n",
    "\n",
    "    def execute(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"if the step has a cache, it hashes the parameters and checks if the result is already cached.\n",
    "        note that params could be any object, so it uses cloudpickle to serialize them.\n",
    "        If the result is cached, it returns the cached result.\n",
    "        If not, it executes the step and saves the result in the cache.\n",
    "        \"\"\"\n",
    "        # Bind args/kwargs to parameter names using original signature\n",
    "        bound = inspect.signature(self.step.execute).bind(*args, **kwargs)\n",
    "        #bound.apply_defaults()\n",
    "\n",
    "        # also checks que values from __init__ for the hash\n",
    "        init_params = self.step.__dict__.copy()\n",
    "        # si los parametros con los que se inicializo cambiaron entonces deberia missear el cache\n",
    "        bound.apply_defaults()\n",
    "\n",
    "        # Serialize input arguments with cloudpickle\n",
    "        try:\n",
    "            serialized = cloudpickle.dumps(bound.arguments)\n",
    "            # Include init parameters in the serialization\n",
    "            serialized += cloudpickle.dumps(init_params)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to serialize for cache: {e}\")\n",
    "\n",
    "        # Generate a hash key from inputs\n",
    "        hash_key = hashlib.sha256(serialized).hexdigest()\n",
    "        cache_file = os.path.join(self.cache_dir, f\"{hash_key}.pkl\")\n",
    "\n",
    "        # Load from cache or compute and save\n",
    "        if os.path.exists(cache_file):\n",
    "            print(f\"Loading cached result for {self.step.name} from {cache_file}\")\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            print(f\"Cache miss for {self.step.name}, executing step and saving result to {cache_file}\")\n",
    "            result = self.step.execute(*args, **kwargs)\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(result, f)\n",
    "            return result\n",
    "\n",
    "    def get_execute_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the parameters for the execute method of the wrapped step.\n",
    "        \"\"\"\n",
    "        return self._execute_params\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the name of the step.\n",
    "        \"\"\"\n",
    "        return self.step.name\n",
    "    \n",
    "\n",
    "class InMemoryCacheWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper class to enable in-memory caching for pipeline steps.\n",
    "    It uses the InMemoryCache class to cache artifacts in memory.\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    \n",
    "    def __init__(self, step: \"PipelineStep\", execute_params: Optional[Dict[str, Any]] = None):\n",
    "        self.step = step\n",
    "        self._execute_params = execute_params or {}\n",
    "\n",
    "    def execute(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Execute the step and cache the result in memory.\"\"\"\n",
    "        # Bind args/kwargs to parameter names using original signature\n",
    "        bound = inspect.signature(self.step.execute).bind(*args, **kwargs)\n",
    "\n",
    "        init_params = self.step.__dict__.copy()\n",
    "        # Merge init parameters with execute parameters\n",
    "        bound.arguments.update(init_params)\n",
    "        bound.apply_defaults()\n",
    "\n",
    "        # Serialize input arguments with cloudpickle\n",
    "        try:\n",
    "            serialized = cloudpickle.dumps(bound.arguments)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to serialize for cache: {e}\")\n",
    "\n",
    "        # Generate a hash key from inputs\n",
    "        hash_key = hashlib.sha256(serialized).hexdigest()\n",
    "\n",
    "        # Load from cache or compute and save\n",
    "        if hash_key in self.cache:\n",
    "            print(f\"Loading cached result for {self.step.name} from memory\")\n",
    "            return self.cache[hash_key]\n",
    "        else:\n",
    "            print(f\"Cache miss for {self.step.name}, executing step and saving result in memory\")\n",
    "            result = self.step.execute(*args, **kwargs)\n",
    "            self.cache[hash_key] = result\n",
    "            return result\n",
    "\n",
    "    def get_execute_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get the parameters for the execute method of the wrapped step.\n",
    "        \"\"\"\n",
    "        return self._execute_params\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the name of the step.\n",
    "        \"\"\"\n",
    "        return self.step.name\n",
    "    \n",
    "\n",
    "class CachedPipelineMixin:\n",
    "    def in_disk_cache(self, cache_dir: str = \".cache\") -> Self:\n",
    "        \"\"\"\n",
    "        It activate the in-disk cache using the InDisKCache class. returns the step itself.\n",
    "        Args:\n",
    "            cache_dir (str): Directory where the cache will be stored.\n",
    "        \"\"\"\n",
    "        execute_params = self.get_execute_params()\n",
    "        return InDiskCacheWrapper(self, cache_dir=cache_dir, execute_params=execute_params)\n",
    "    \n",
    "    def in_memory_cache(self) -> Self:\n",
    "        \"\"\"\n",
    "        It activate the in-memory cache using the InMemoryCache class. returns the step itself.\n",
    "        \"\"\"\n",
    "        execute_params = self.get_execute_params()\n",
    "        return InMemoryCacheWrapper(self, execute_params=execute_params)\n",
    "    \n",
    "\n",
    "class PipelineStep(ABC, CachedPipelineMixin):\n",
    "    \"\"\"\n",
    "    Abstract base class for pipeline steps.\n",
    "    Each step in the pipeline must inherit from this class and implement the execute method.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize a pipeline step.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the step for identification and logging purposes.\n",
    "        \"\"\"\n",
    "        self._name = name or self.__class__.__name__\n",
    "\n",
    "    @abstractmethod\n",
    "    def execute(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Execute the pipeline step.\n",
    "    \n",
    "        Args:\n",
    "            pipeline (Pipeline): The pipeline instance that contains this step.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save_artifact(self, pipeline: \"Pipeline\", artifact_name: str, artifact: Any) -> None:\n",
    "        \"\"\"\n",
    "        Save an artifact produced by this step to the pipeline.\n",
    "\n",
    "        Args:\n",
    "            pipeline (Pipeline): The pipeline instance.\n",
    "            artifact_name (str): Name to identify the artifact.\n",
    "            artifact (Any): The artifact to save.\n",
    "        \"\"\"\n",
    "        pipeline.save_artifact(artifact_name, artifact)\n",
    "\n",
    "    def get_artifact(self, pipeline: \"Pipeline\", artifact_name: str, default=None, raise_not_found=True) -> Any:\n",
    "        \"\"\"\n",
    "        Retrieve a stored artifact from the pipeline.\n",
    "\n",
    "        Args:\n",
    "            pipeline (Pipeline): The pipeline instance.\n",
    "            artifact_name (str): Name of the artifact to retrieve.\n",
    "            default: Default value to return if the artifact is not found.\n",
    "            raise_not_found (bool): Whether to raise an error if the artifact is not found.\n",
    "\n",
    "        Returns:\n",
    "            Any: The requested artifact or default value.\n",
    "        \"\"\"\n",
    "        return pipeline.get_artifact(artifact_name, default=default, raise_not_found=raise_not_found)\n",
    "    \n",
    "    def del_artifact(self, pipeline: \"Pipeline\", artifact_name: str, soft=True) -> None:\n",
    "        \"\"\"\n",
    "        Delete a stored artifact from the pipeline and free memory.\n",
    "\n",
    "        Args:\n",
    "            pipeline (Pipeline): The pipeline instance.\n",
    "            artifact_name (str): Name of the artifact to delete.\n",
    "            soft (bool): If True, performs a soft delete; if False, forces garbage collection.\n",
    "        \"\"\"\n",
    "        pipeline.del_artifact(artifact_name, soft=soft)\n",
    "\n",
    "    def get_execute_params(self):\n",
    "        sig = inspect.signature(self.execute)\n",
    "        return sig.parameters\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @name.setter\n",
    "    def name(self, value):\n",
    "        self._name = value\n",
    "    \n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline class that manages the execution of steps and storage of artifacts.\n",
    "    \"\"\"\n",
    "    def __init__(self, steps: Optional[List[PipelineStep]] = None, optimize_arftifacts_memory: bool = True):\n",
    "        \"\"\"Initialize the pipeline.\"\"\"\n",
    "        self.steps: List[PipelineStep] = steps if steps is not None else []\n",
    "        self.artifacts: Dict[str, Any] = {}\n",
    "        self.optimize_arftifacts_memory = optimize_arftifacts_memory\n",
    "\n",
    "    def add_step(self, step: PipelineStep, position: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Add a new step to the pipeline.\n",
    "\n",
    "        Args:\n",
    "            step (PipelineStep): The step to add.\n",
    "            position (Optional[int]): Position where to insert the step. If None, appends to the end.\n",
    "        \"\"\"\n",
    "        if position is not None:\n",
    "            self.steps.insert(position, step)\n",
    "        else:\n",
    "            self.steps.append(step)\n",
    "\n",
    "    def save_artifact(self, artifact_name: str, artifact: Any) -> None:\n",
    "        \"\"\"\n",
    "        Save an artifact from a given step.\n",
    "\n",
    "        Args:\n",
    "            artifact_name (str): Name to identify the artifact.\n",
    "            artifact (Any): The artifact to save.\n",
    "        \"\"\"\n",
    "        if not self.optimize_arftifacts_memory:\n",
    "            self.artifacts[artifact_name] = artifact\n",
    "        else:\n",
    "            # guarda el artifact en /tmp/ para no guardarlo en memoria\n",
    "            if not os.path.exists(\"/tmp/\"):\n",
    "                os.makedirs(\"/tmp/\")\n",
    "            artifact_path = os.path.join(\"/tmp/\", artifact_name)\n",
    "            with open(artifact_path, 'wb') as f:\n",
    "                pickle.dump(artifact, f)\n",
    "            self.artifacts[artifact_name] = artifact_path\n",
    "\n",
    "    def get_artifact(self, artifact_name: str, default=None, raise_not_found=True) -> Any:\n",
    "        \"\"\"\n",
    "        Retrieve a stored artifact.\n",
    "\n",
    "        Args:\n",
    "            artifact_name (str): Name of the artifact to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Any: The requested artifact.\n",
    "        \"\"\"\n",
    "        if not self.optimize_arftifacts_memory:\n",
    "            return self.artifacts.get(artifact_name)\n",
    "        else:\n",
    "            artifact_path = self.artifacts.get(artifact_name)\n",
    "            if artifact_path and os.path.exists(artifact_path):\n",
    "                with open(artifact_path, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            else:\n",
    "                if raise_not_found:\n",
    "                    raise FileNotFoundError(f\"Artifact {artifact_name} not found in /tmp/\")\n",
    "                return default\n",
    "    \n",
    "    def del_artifact(self, artifact_name: str, soft=True) -> None:\n",
    "        \"\"\"\n",
    "        Delete a stored artifact and free memory.\n",
    "\n",
    "        Args:\n",
    "            artifact_name (str): Name of the artifact to delete.\n",
    "        \"\"\"\n",
    "        del self.artifacts[artifact_name]\n",
    "        if not soft:\n",
    "            # Force garbage collection if not soft delete\n",
    "            gc.collect()\n",
    "    \n",
    "    \n",
    "    def run(self, verbose: bool = True, last_step_callback: Callable = None) -> None:\n",
    "        \"\"\"\n",
    "        Execute all steps in sequence and log execution time.\n",
    "        \"\"\"        \n",
    "        \n",
    "        # Run steps from the last completed step\n",
    "        for step in self.steps:\n",
    "            if verbose:\n",
    "                print(f\"Executing step: {step.name}\")\n",
    "            start_time = time.time()\n",
    "            params = self.__fill_params_from_step(step)\n",
    "            artifacts_to_save = step.execute(**params)\n",
    "            if artifacts_to_save is None:\n",
    "                artifacts_to_save = {}\n",
    "            self.__save_step_artifacts(artifacts_to_save)\n",
    "            end_time = time.time()\n",
    "            if verbose:\n",
    "                print(f\"Step {step.name} completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "    def __fill_params_from_step(self, step: PipelineStep) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Obtiene los nombres de los parametros de la implementacion de la funcion execute del paso. (excepto el pipeline el cual es obligatorio)\n",
    "        luego obtengo todos los artefactos del pipeline y los paso como parametros al paso.\n",
    "        \"\"\"\n",
    "        step_params = step.get_execute_params()\n",
    "        params = {}\n",
    "        for name, param in step_params.items():\n",
    "            if name == 'pipeline':\n",
    "                params[name] = self\n",
    "            elif param.default is inspect.Parameter.empty:\n",
    "                params[name] = self.get_artifact(name)\n",
    "            else:\n",
    "                params[name] = self.get_artifact(name, default=param.default, raise_not_found=False)\n",
    "        return params\n",
    "\n",
    "    \n",
    "\n",
    "    def __save_step_artifacts(self, artifacts_to_save: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Save artifacts produced by a step to the pipeline.\n",
    "\n",
    "        Args:\n",
    "            artifacts_to_save (Dict[str, Any]): Artifacts to save.\n",
    "        \"\"\"\n",
    "\n",
    "        for name, artifact in artifacts_to_save.items():\n",
    "            self.save_artifact(name, artifact)\n",
    "\n",
    "\n",
    "\n",
    "    def clear(self, collect_garbage: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Clean up all artifacts and free memory.\n",
    "        \"\"\"\n",
    "        if collect_garbage:\n",
    "            del self.artifacts\n",
    "            gc.collect()\n",
    "        self.artifacts = {}\n",
    "        self.last_step = None\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "class LoadDataFrameStep(PipelineStep):\n",
    "    \"\"\"\n",
    "    Example step that loads a DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.path = path\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        df = pd.read_parquet(self.path)\n",
    "        df = df.drop(columns=[\"periodo\"])\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class CastDataTypesStep(PipelineStep):\n",
    "    def __init__(self, dtypes: Dict[str, str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.dtypes = dtypes\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        for col, dtype in self.dtypes.items():\n",
    "            df[col] = df[col].astype(dtype)\n",
    "        print(df.info())\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class ChangeDataTypesStep(PipelineStep):\n",
    "    def __init__(self, dtypes: Dict[str, str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.dtypes = dtypes\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        for original_dtype, dtype in self.dtypes.items():\n",
    "            for col in df.select_dtypes(include=[original_dtype]).columns:\n",
    "                df[col] = df[col].astype(dtype)\n",
    "        print(df.info())\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class FilterFirstDateStep(PipelineStep):\n",
    "    def __init__(self, first_date: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.first_date = first_date\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        df = df[df[\"fecha\"] >= self.first_date]\n",
    "        print(f\"Filtered DataFrame shape: {df.shape}\")\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class FeatureEngineeringLagStep(PipelineStep):\n",
    "    def __init__(self, lags: List[int], columns: List, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.lags = lags\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> dict:\n",
    "        # Ordenar por grupo y fecha para que los lags sean correctos\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        \n",
    "        # Crear lags usando groupby y shift (vectorizado)\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for column in self.columns:\n",
    "            for lag in self.lags:\n",
    "                df[f\"{column}_lag_{lag}\"] = grouped[column].shift(lag)\n",
    "        \n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "\n",
    "# 1. Media Móvil\n",
    "class RollingMeanFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_mean_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).mean()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "\n",
    "# 2. Máximo Móvil\n",
    "class RollingMaxFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_max_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).max()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "\n",
    "# 3. Mínimo Móvil\n",
    "class RollingMinFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_min_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).min()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "\n",
    "# 4. Desviación Estándar Móvil\n",
    "class RollingStdFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_std_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).std()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "\n",
    "# 5. Media Móvil Exponencial\n",
    "class ExponentialMovingAverageStep(PipelineStep):\n",
    "    def __init__(self, span: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.span = span\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_ema_{self.span}'] = grouped[col].transform(\n",
    "                lambda x: x.ewm(span=self.span, adjust=False).mean()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "\n",
    "# 6. Tendencia (Pendiente de regresión lineal)\n",
    "import tqdm\n",
    "class TrendFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        \n",
    "        def calculate_trend(series):\n",
    "            return series.rolling(self.window).apply(\n",
    "                lambda x: linregress(np.arange(len(x)), x)[0], raw=False\n",
    "            )\n",
    "        \n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_trend_{self.window}'] = grouped[col].transform(calculate_trend)\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "# 8. Diferencia con período anterior\n",
    "class DiffFeatureStep(PipelineStep):\n",
    "    def __init__(self, periods: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.periods = periods\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_diff_{self.periods}'] = grouped[col].diff(self.periods)\n",
    "        return {\"df\": df}\n",
    "\n",
    "# 9. Cambio porcentual\n",
    "class PctChangeFeatureStep(PipelineStep):\n",
    "    def __init__(self, periods: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.periods = periods\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            # le sumo un valor minimo al grouped[col] para evitar division por cero\n",
    "            df[f'{col}_pct_change_{self.periods}'] = grouped[col].pct_change(self.periods)\n",
    "        return {\"df\": df}\n",
    "\n",
    "# 10. Mediana Móvil\n",
    "class RollingMedianFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_median_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).median()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "\n",
    "class CreateTotalCategoryStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None, cat: str = \"cat1\", tn: str = \"tn\"):\n",
    "        super().__init__(name)\n",
    "        self.cat = cat\n",
    "        self.tn = tn\n",
    "    \n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(['fecha', self.cat])\n",
    "        df[f\"{self.tn}_{self.cat}_vendidas\"] = (\n",
    "            df.groupby(['fecha', self.cat])[self.tn]\n",
    "              .transform('sum')\n",
    "        )\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class CreateWeightByCustomerStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Añade en df la columna 'customer_weight' calculada como:\n",
    "            tn_customer_vendidas / tn_total_vendidas\n",
    "        sin crear DataFrames intermedios.\n",
    "        \"\"\"\n",
    "\n",
    "        # Aseguramos orden estable (opcional, mejora legibilidad)\n",
    "        df = df.sort_values(['fecha', 'customer_id'])\n",
    "        \n",
    "        # 1) Sumatoria de 'tn' por (fecha, customer_id) directamente en cada fila\n",
    "        df['tn_customer_vendidas'] = (\n",
    "            df.groupby(['fecha', 'customer_id'])['tn']\n",
    "              .transform('sum')\n",
    "        )\n",
    "        \n",
    "        # 2) Sumatoria total de 'tn' por fecha\n",
    "        df['tn_total_vendidas'] = (\n",
    "            df.groupby('fecha')['tn']\n",
    "              .transform('sum')\n",
    "        )\n",
    "        \n",
    "        # 3) Ratio\n",
    "        df['customer_weight'] = df['tn_customer_vendidas'] / df['tn_total_vendidas']\n",
    "        \n",
    "        # 4) (Opcional) Eliminamos columnas temporales si no las necesitamos\n",
    "        #df.drop(columns=['tn_customer_vendidas', 'tn_total_vendidas'], inplace=True)\n",
    "        \n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class CreateWeightByProductStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Añade en df la columna 'product_weight' calculada como:\n",
    "            tn_product_vendidas / tn_total_vendidas\n",
    "        sin crear DataFrames intermedios.\n",
    "        \"\"\"\n",
    "\n",
    "        # Aseguramos orden estable (opcional, mejora legibilidad)\n",
    "        df = df.sort_values(['fecha', 'product_id'])\n",
    "        \n",
    "        # 1) Sumatoria de 'tn' por (fecha, product_id) directamente en cada fila\n",
    "        df['tn_product_vendidas'] = (\n",
    "            df.groupby(['fecha', 'product_id'])['tn']\n",
    "              .transform('sum')\n",
    "        )\n",
    "        \n",
    "        # 2) Sumatoria total de 'tn' por fecha\n",
    "        df['tn_total_product_vendidas'] = (\n",
    "            df.groupby('fecha')['tn']\n",
    "              .transform('sum')\n",
    "        )\n",
    "        \n",
    "        # 3) Ratio\n",
    "        df['product_weight'] = df['tn_product_vendidas'] / df['tn_total_product_vendidas']\n",
    "        \n",
    "        # 4) (Opcional) Eliminamos columnas temporales si no las necesitamos\n",
    "        #df.drop(columns=['tn_product_vendidas', 'tn_total_vendidas'], inplace=True)\n",
    "        \n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class CreateWeightMeanStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Añade en df la columna 'mean_weight' calculada como:\n",
    "            tn_mean_vendidas / tn_total_vendidas\n",
    "        sin crear DataFrames intermedios.\n",
    "        \"\"\"\n",
    "\n",
    "        # la columna weight es la media entre todas las columnas que digan weight\n",
    "        weight_columns = [col for col in df.columns if 'weight' in col]\n",
    "        print(f\"Weight columns found: {weight_columns}\")\n",
    "        if not weight_columns:\n",
    "            raise ValueError(\"No weight columns found in the DataFrame.\")\n",
    "        df[\"weight\"] = df[weight_columns].mean(axis=1)\n",
    "        # le sumo 0.5 a weight para que no hayan weights de 0\n",
    "        df[\"weight\"] = df[\"weight\"] + 0.5\n",
    "        return {\"df\": df}\n",
    "\n",
    "import tqdm\n",
    "\n",
    "class FeatureEngineeringProductInteractionStep(PipelineStep):\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        \"\"\"\n",
    "        El dataframe tiene una columna product_id y customer_id y fecha.\n",
    "        Quiero obtener los 100 productos con mas tn del ultimo mes y crear 100 nuevas columnas que es la suma de tn de esos productos (para todos los customer)\n",
    "        se deben agregan entonces respetando la temporalidad la columna product_{product_id}_total_tn\n",
    "        \"\"\"\n",
    "        last_date = df[\"fecha\"].max()\n",
    "        last_month_df = df[df[\"fecha\"] == last_date]\n",
    "        top_products = last_month_df.groupby(\"product_id\").aggregate({\"tn\": \"sum\"}).nlargest(10, \"tn\").index.tolist()\n",
    "        # TODO: mejor agruparlo por categoria y hacer una columna por cada categoria tanto de agrup por product como por customer\n",
    "        for product_id in tqdm.tqdm(top_products):\n",
    "            # creo un subset que es el total de product_id vendidos para todos los customer en cada t y lo mergeo a df\n",
    "            product_df = df[df[\"product_id\"] == product_id].groupby(\"fecha\").aggregate({\"tn\": \"sum\"}).reset_index()\n",
    "            product_df = product_df.rename(columns={\"tn\": f\"product_{product_id}_total_tn\"})\n",
    "            product_df = product_df[[\"fecha\", f\"product_{product_id}_total_tn\"]]\n",
    "            df = df.merge(product_df, on=\"fecha\", how=\"left\")\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "\n",
    "class FeatureEngineeringProductCatInteractionStep(PipelineStep):\n",
    "\n",
    "    def __init__(self, cat=\"cat1\", name: Optional[str] = None, tn=\"tn\"):\n",
    "        super().__init__(name)\n",
    "        self.cat = cat\n",
    "        self.tn = tn\n",
    "\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        # agrupo el dataframe por cat1 (sumando), obteniendo fecha, cat1 y\n",
    "        # luego paso el dataframe a wide format, donde cada columna es una categoria  y la fila es la suma de tn para cada cat1\n",
    "        # luego mergeo al dataframe original por fecha y product_id\n",
    "        df_cat = df.groupby([\"fecha\", self.cat]).agg({self.tn: \"sum\"}).reset_index()\n",
    "        df_cat = df_cat.pivot(index=\"fecha\", columns=self.cat, values=self.tn).reset_index()\n",
    "        df = df.merge(df_cat, on=\"fecha\", how=\"left\")\n",
    "        return {\"df\": df}\n",
    "        \n",
    "\n",
    "class FeatureDivInteractionStep(PipelineStep):\n",
    "    def __init__(self, columns: List[Tuple[str, str]], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        for col1, col2 in self.columns:\n",
    "            df[f\"{col1}_div_{col2}\"] = df[col1] / (df[col2] + 1e-6)  # Evitar división por cero\n",
    "        return {\"df\": df}\n",
    "\n",
    "\n",
    "class FeatureProdInteractionStep(PipelineStep):\n",
    "    def __init__(self, columns: List[Tuple[str, str]], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        for col1, col2 in self.columns:\n",
    "            df[f\"{col1}_prod_{col2}\"] = df[col1] * df[col2]\n",
    "        return {\"df\": df}\n",
    "\n",
    "class DateRelatedFeaturesStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        df[\"year\"] = df[\"fecha\"].dt.year\n",
    "        df[\"mes\"] = df[\"fecha\"].dt.month\n",
    "        return {\"df\": df}\n",
    "\n",
    "        \n",
    "\n",
    "class SplitDataFrameStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, df) -> None:\n",
    "        sorted_dated = sorted(df[\"fecha\"].unique())\n",
    "        last_date = sorted_dated[-1] # es 12-2019\n",
    "        last_test_date = sorted_dated[-3] # needs a gap because forecast moth+2\n",
    "        last_train_date = sorted_dated[-4] #\n",
    "\n",
    "        kaggle_pred = df[df[\"fecha\"] == last_date]\n",
    "        test = df[df[\"fecha\"] == last_test_date]\n",
    "        eval_data = df[df[\"fecha\"] == last_train_date]\n",
    "        train = df[(df[\"fecha\"] < last_train_date)]\n",
    "        return {\n",
    "            \"train\": train,\n",
    "            \"eval_data\": eval_data,\n",
    "            \"test\": test,\n",
    "            \"kaggle_pred\": kaggle_pred\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "class CustomMetric:\n",
    "    def __init__(self, df_eval, product_id_col='product_id', scaler=None):\n",
    "        self.scaler = scaler\n",
    "        self.df_eval = df_eval\n",
    "        self.product_id_col = product_id_col\n",
    "    \n",
    "    def __call__(self, preds, train_data):\n",
    "        labels = train_data.get_label()\n",
    "        df_temp = self.df_eval.copy()\n",
    "        df_temp['preds'] = preds\n",
    "        df_temp['labels'] = labels\n",
    "\n",
    "        if self.scaler:\n",
    "            df_temp['preds'] = self.scaler.inverse_transform(df_temp[['preds']])\n",
    "            df_temp['labels'] = self.scaler.inverse_transform(df_temp[['labels']])\n",
    "        \n",
    "        # Agrupar por product_id y calcular el error\n",
    "        por_producto = df_temp.groupby(self.product_id_col).agg({'labels': 'sum', 'preds': 'sum'})\n",
    "        \n",
    "        # Calcular el error personalizado\n",
    "        error = np.sum(np.abs(por_producto['labels'] - por_producto['preds'])) / np.sum(np.abs(por_producto['labels']))\n",
    "        \n",
    "        # LightGBM espera que el segundo valor sea mayor cuando el modelo es mejor\n",
    "        return 'custom_error', error, False\n",
    "\n",
    "\n",
    "    \n",
    "class CustomMetricAutoML:\n",
    "    def __init__(self, df_eval, product_id_col='product_id', scaler=None):\n",
    "        self.df_eval = df_eval\n",
    "        self.product_id_col = product_id_col\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __call__(self, X_val, y_val, estimator, *args, **kwargs):\n",
    "        df_temp = X_val.copy()\n",
    "        df_temp['preds'] = estimator.predict(X_val)\n",
    "        df_temp['labels'] = y_val\n",
    "\n",
    "        if self.scaler:\n",
    "            df_temp['preds'] = self.scaler.inverse_transform(df_temp[['preds']])\n",
    "            df_temp['labels'] = self.scaler.inverse_transform(df_temp[['labels']])\n",
    "        \n",
    "        # Agrupar por product_id y calcular el error\n",
    "        por_producto = df_temp.groupby(self.product_id_col).agg({'labels': 'sum', 'preds': 'sum'})\n",
    "        \n",
    "        # Calcular el error personalizado\n",
    "        error = np.sum(np.abs(por_producto['labels'] - por_producto['preds'])) / np.sum(por_producto['labels'])\n",
    "        \n",
    "        return error, {\"total_error\": error}\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "class TrainScalerDataStep(PipelineStep):\n",
    "    def __init__(self, scaler = RobustScaler, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def execute(self, train) -> None:\n",
    "        scaler = self.scaler()\n",
    "\n",
    "        # escalo las columnas que son int32 o float32\n",
    "        columns_to_scale = train.select_dtypes(include=['int32', 'float32']).columns.tolist()\n",
    "        # saco la columna target\n",
    "        columns_to_scale = [col for col in columns_to_scale if col not in\n",
    "                [\"periodo\", 'fecha', 'target']]\n",
    "\n",
    "        scaler = scaler.fit(train[columns_to_scale])\n",
    "        scaler_target = self.scaler()\n",
    "        scaler_target = scaler_target.fit(train[[\"target\"]])\n",
    "        return {\n",
    "            \"scaler\": scaler,\n",
    "            \"scaler_target\": scaler_target,\n",
    "            \"columns_to_scale\": columns_to_scale\n",
    "        }\n",
    "\n",
    " \n",
    "   \n",
    "class PrepareXYStep(PipelineStep):\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def execute(self, train, eval_data, test, kaggle_pred) -> None:\n",
    "        features = [col for col in train.columns if col not in\n",
    "                        ['fecha', 'target']]\n",
    "        target = 'target'\n",
    "\n",
    "        X_train = pd.concat([train[features], eval_data[features]]) # [train + eval] + [eval] -> [test] \n",
    "        y_train = pd.concat([train[target], eval_data[target]])\n",
    "\n",
    "        X_train_alone = train[features]\n",
    "        y_train_alone = train[target]\n",
    "\n",
    "        X_eval = eval_data[features]\n",
    "        y_eval = eval_data[target]\n",
    "\n",
    "        X_test = test[features]\n",
    "        y_test = test[target]\n",
    "\n",
    "        X_train_final = pd.concat([train[features], eval_data[features], test[features]])\n",
    "        y_train_final = pd.concat([train[target], eval_data[target], test[target]])\n",
    "\n",
    "        X_kaggle = kaggle_pred[features]\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_train_alone\": X_train_alone,\n",
    "            \"y_train_alone\": y_train_alone,\n",
    "            \"X_eval\": X_eval,\n",
    "            \"y_eval\": y_eval,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "            \"X_train_final\": X_train_final,\n",
    "            \"y_train_final\": y_train_final,\n",
    "            \"X_kaggle\": X_kaggle\n",
    "        }\n",
    "        \n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "class TrainModelXGBStep(PipelineStep):\n",
    "    def __init__(self, params: Dict = {}, train_eval_sets = {}, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        if not params:\n",
    "            params = {\n",
    "                'learning_rate': 0.05,\n",
    "                'max_depth': 6,\n",
    "                'n_estimators': 800,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'reg_alpha': 1,\n",
    "                'reg_lambda': 5,\n",
    "                'random_state': 42,\n",
    "                \"enable_categorical\": True,\n",
    "            }\n",
    "        if not train_eval_sets:\n",
    "            train_eval_sets = {\n",
    "                \"X_train\": \"X_train\",\n",
    "                \"y_train\": \"y_train\",\n",
    "                \"X_eval\": \"X_eval\",\n",
    "                \"y_eval\": \"y_eval\",\n",
    "            }\n",
    "        self.params = params\n",
    "        self.train_eval_sets = train_eval_sets\n",
    "    def execute(self, pipeline: Pipeline, scaler=None, scaler_target=None):\n",
    "        X_train = pipeline.get_artifact(self.train_eval_sets[\"X_train\"])\n",
    "        y_train = pipeline.get_artifact(self.train_eval_sets[\"y_train\"])\n",
    "        X_eval = pipeline.get_artifact(self.train_eval_sets[\"X_eval\"])\n",
    "        y_eval = pipeline.get_artifact(self.train_eval_sets[\"y_eval\"])\n",
    "\n",
    "        if scaler:\n",
    "            X_train[scaler.feature_names_in_] = scaler.transform(X_train[scaler.feature_names_in_])\n",
    "            X_eval[scaler.feature_names_in_] = scaler.transform(X_eval[scaler.feature_names_in_])\n",
    "            y_train = pd.Series(\n",
    "                scaler_target.transform(y_train.values.reshape(-1, 1)).flatten(),\n",
    "                index=y_train.index,\n",
    "            )\n",
    "            y_eval = pd.Series(\n",
    "                scaler_target.transform(y_eval.values.reshape(-1, 1)).flatten(),\n",
    "                index=y_eval.index,\n",
    "            )\n",
    "\n",
    "        model = XGBRegressor(**self.params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_eval, y_eval)])\n",
    "        return {\"model\": model}\n",
    "\n",
    "\n",
    "class TrainModelLGBStep(PipelineStep):\n",
    "    def __init__(self, params: Dict = {}, train_eval_sets = {}, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        if not params:\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"boosting_type\": \"gbdt\",\n",
    "                \"num_leaves\": 31,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"feature_fraction\": 0.9,\n",
    "                \"bagging_fraction\": 0.8,\n",
    "                \"bagging_freq\": 5,\n",
    "                \"n_estimators\": 1000,\n",
    "                \"verbose\": -1\n",
    "            }\n",
    "        if not train_eval_sets:\n",
    "            train_eval_sets = {\n",
    "                \"X_train\": \"X_train\",\n",
    "                \"y_train\": \"y_train\",\n",
    "                \"X_eval\": \"X_eval\",\n",
    "                \"y_eval\": \"y_eval\",\n",
    "                \"eval_data\": \"eval_data\",\n",
    "            }\n",
    "        self.params = params\n",
    "        self.train_eval_sets = train_eval_sets\n",
    "\n",
    "    def execute(self, pipeline: Pipeline, scaler=None, scaler_target=None) -> None:\n",
    "        X_train = pipeline.get_artifact(self.train_eval_sets[\"X_train\"])\n",
    "        y_train = pipeline.get_artifact(self.train_eval_sets[\"y_train\"])\n",
    "        X_eval = pipeline.get_artifact(self.train_eval_sets[\"X_eval\"])\n",
    "        y_eval = pipeline.get_artifact(self.train_eval_sets[\"y_eval\"])\n",
    "        df_eval = pipeline.get_artifact(self.train_eval_sets[\"eval_data\"])\n",
    "\n",
    "        cat_features = [col for col in X_train.columns if X_train[col].dtype.name == 'category']\n",
    "\n",
    "        if scaler:\n",
    "            X_train[scaler.feature_names_in_] = scaler.transform(X_train[scaler.feature_names_in_])\n",
    "            X_eval[scaler.feature_names_in_] = scaler.transform(X_eval[scaler.feature_names_in_])\n",
    "            y_train = pd.Series(\n",
    "                scaler_target.transform(y_train.values.reshape(-1, 1)).flatten(),\n",
    "                index=y_train.index,\n",
    "            )\n",
    "            y_eval = pd.Series(\n",
    "                scaler_target.transform(y_eval.values.reshape(-1, 1)).flatten(),\n",
    "                index=y_eval.index,\n",
    "            )\n",
    "\n",
    "        \n",
    "        weight = X_train['weight'] if 'weight' in X_train.columns else None\n",
    "        weight_eval = X_eval['weight'] if 'weight' in X_eval.columns else None\n",
    "        train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features, weight=weight)\n",
    "        eval_data = lgb.Dataset(X_eval, label=y_eval, reference=train_data, categorical_feature=cat_features, weight=weight_eval)\n",
    "        custom_metric = CustomMetric(df_eval, product_id_col='product_id', scaler=scaler_target)\n",
    "        callbacks = [\n",
    "            #lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(100),\n",
    "        ]\n",
    "        model = lgb.train(\n",
    "            self.params,\n",
    "            train_data,\n",
    "            #num_boost_round=1200,\n",
    "            #num_boost_round=50, # test\n",
    "            valid_sets=[eval_data],\n",
    "            feval=custom_metric,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        return {\"model\": model}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creo una clase CustomAutoMLSplitter que implementa la interfaz de KFold asi se usa en automl\n",
    "# lo que hace es simplemente splittear dejando el ultimo test para eval y el resto para test\n",
    "# por ahora queda harcodeado 1 split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "class CustomAutoMLSplitter:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.n_splits = 1 # siempre 1\n",
    "\n",
    "    def split(self, X, y=None, groups=None):        \n",
    "        # Solo un split, el resto es para test\n",
    "        last_month = X['fecha'].max()\n",
    "        last_month_df = X[X['fecha'] == last_month]\n",
    "        train_df = X[X['fecha'] < last_month]\n",
    "\n",
    "        # Devolvemos un solo split, el resto es para test\n",
    "        yield train_df.index, last_month_df.index\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "class TrainModelAutoMLStep(PipelineStep):\n",
    "    def __init__(self, train_eval_sets = {}, time_budget: int = 100, products_proportion: float = 1.0, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.time_budget = time_budget\n",
    "        if not train_eval_sets:\n",
    "            train_eval_sets = {\n",
    "                \"X_train\": \"X_train\",\n",
    "                \"y_train\": \"y_train\",\n",
    "                \"X_eval\": \"X_eval\",\n",
    "                \"y_eval\": \"y_eval\",\n",
    "                \"eval_data\": \"eval_data\",\n",
    "            }\n",
    "        self.train_eval_sets = train_eval_sets\n",
    "        self.products_proportion = products_proportion\n",
    "\n",
    "    def execute(self, pipeline: Pipeline, scaler=None, scaler_target=None) -> None:\n",
    "        X_train = pipeline.get_artifact(self.train_eval_sets[\"X_train\"])\n",
    "        y_train = pipeline.get_artifact(self.train_eval_sets[\"y_train\"])\n",
    "        X_eval = pipeline.get_artifact(self.train_eval_sets[\"X_eval\"])\n",
    "        y_eval = pipeline.get_artifact(self.train_eval_sets[\"y_eval\"])\n",
    "        df_eval = pipeline.get_artifact(self.train_eval_sets[\"eval_data\"])\n",
    "\n",
    "        # para que sea mas rapido si self.products_proportion < 1.0, tomo una muestra de los productos\n",
    "        if self.products_proportion < 1.0:\n",
    "            unique_products = X_train['product_id'].unique()\n",
    "            sample_size = int(len(unique_products) * self.products_proportion)\n",
    "            sampled_products = np.random.choice(unique_products, size=sample_size, replace=False)\n",
    "            X_train = X_train[X_train['product_id'].isin(sampled_products)]\n",
    "            y_train = y_train[X_train.index]\n",
    "            X_eval = X_eval[X_eval['product_id'].isin(sampled_products)]\n",
    "            y_eval = y_eval[X_eval.index]\n",
    "            df_eval = df_eval[df_eval['product_id'].isin(sampled_products)]\n",
    "        \n",
    "        if scaler:\n",
    "            X_train[scaler.feature_names_in_] = scaler.transform(X_train[scaler.feature_names_in_])\n",
    "            X_eval[scaler.feature_names_in_] = scaler.transform(X_eval[scaler.feature_names_in_])\n",
    "            y_train = pd.Series(\n",
    "                scaler_target.transform(y_train.values.reshape(-1, 1)).flatten(),\n",
    "                index=y_train.index,\n",
    "            )\n",
    "            y_eval = pd.Series(\n",
    "                scaler_target.transform(y_eval.values.reshape(-1, 1)).flatten(),\n",
    "                index=y_eval.index,\n",
    "            )\n",
    "        automl = AutoML()\n",
    "        metric = CustomMetricAutoML(df_eval, product_id_col='product_id', scaler=scaler_target)\n",
    "        automl_params = {\n",
    "            \"time_budget\": self.time_budget,\n",
    "            \"task\": \"regression\",\n",
    "            \"metric\": metric,\n",
    "            \"eval_method\": \"holdout\",\n",
    "            #\"estimator_list\": [\"lgbm\", \"xgboost\", \"catboost\"],\n",
    "            \"estimator_list\": [\"lgbm\"],            \n",
    "        }\n",
    "            \n",
    "        automl.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_val=X_eval,\n",
    "            y_val=y_eval,\n",
    "            **automl_params\n",
    "        )\n",
    "        # Save the model\n",
    "        automl_ml_best_model = automl.model.estimator\n",
    "        return {\n",
    "            \"automl_best_model\": automl_ml_best_model,\n",
    "            \"automl\": automl\n",
    "        }        \n",
    "\n",
    "\n",
    "\n",
    "class ReTrainAutoMLBestModelStep(PipelineStep):\n",
    "    def __init__(self, train_eval_sets = {}, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        if not train_eval_sets:\n",
    "            train_eval_sets = {\n",
    "                \"X_train\": \"X_train\",\n",
    "                \"y_train\": \"y_train\",\n",
    "                \"X_eval\": \"X_eval\",\n",
    "                \"y_eval\": \"y_eval\",\n",
    "                \"eval_data\": \"eval_data\",\n",
    "            }\n",
    "        self.train_eval_sets = train_eval_sets\n",
    "\n",
    "    def execute(self, pipeline: Pipeline, automl_best_model, scaler=None, scaler_target=None) -> None:\n",
    "        X_train = pipeline.get_artifact(self.train_eval_sets[\"X_train\"])\n",
    "        y_train = pipeline.get_artifact(self.train_eval_sets[\"y_train\"])\n",
    "        X_eval = pipeline.get_artifact(self.train_eval_sets[\"X_eval\"])\n",
    "        y_eval = pipeline.get_artifact(self.train_eval_sets[\"y_eval\"])\n",
    "        df_eval = pipeline.get_artifact(self.train_eval_sets[\"eval_data\"])\n",
    "\n",
    "        if scaler:\n",
    "            X_train[scaler.feature_names_in_] = scaler.transform(X_train[scaler.feature_names_in_])\n",
    "            X_eval[scaler.feature_names_in_] = scaler.transform(X_eval[scaler.feature_names_in_])\n",
    "            y_train = pd.Series(\n",
    "                scaler_target.transform(y_train.values.reshape(-1, 1)).flatten(),\n",
    "                index=y_train.index,\n",
    "            )\n",
    "            y_eval = pd.Series(\n",
    "                scaler_target.transform(y_eval.values.reshape(-1, 1)).flatten(),\n",
    "                index=y_eval.index,\n",
    "            )\n",
    "        if isinstance(automl_best_model, lgb.LGBMRegressor):\n",
    "            categorical_features = [col for col in X_train.columns if X_train[col].dtype.name == 'category']\n",
    "            train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "            eval_data = lgb.Dataset(X_eval, label=y_eval, reference=train_data, categorical_feature=categorical_features)\n",
    "            custom_metric = CustomMetric(df_eval, product_id_col='product_id', scaler=scaler_target)\n",
    "            callbacks = [\n",
    "                #lgb.early_stopping(50),\n",
    "                lgb.log_evaluation(100),\n",
    "            ]\n",
    "            model = lgb.train(\n",
    "                automl_best_model.get_params(),\n",
    "                train_data,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=[eval_data],\n",
    "                feval=custom_metric,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"The model is not a valid LightGBM model.\")\n",
    "        # Save the model\n",
    "        return {\"model\": model}\n",
    "\n",
    "\n",
    "\n",
    "class PredictStep(PipelineStep):\n",
    "    def __init__(self, predict_set: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.predict_set = predict_set\n",
    "\n",
    "    def execute(self, pipeline: Pipeline, model, scaler=None, scaler_target=None) -> None:\n",
    "        X_predict = pipeline.get_artifact(self.predict_set)\n",
    "        if scaler:\n",
    "            X_predict[scaler.feature_names_in_] = scaler.transform(X_predict[scaler.feature_names_in_])\n",
    "        predictions = model.predict(X_predict)\n",
    "        # los valores de predictions que dan menores a 0 los seteo en 0\n",
    "        if scaler_target:\n",
    "            predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "        # la columna de predictions seria \"predictions\" y le agrego columna de product_id\n",
    "        predictions = pd.DataFrame(predictions, columns=[\"predictions\"], index=X_predict.index)\n",
    "        predictions[\"product_id\"] = X_predict[\"product_id\"]\n",
    "        predictions[\"customer_id\"] = X_predict[\"customer_id\"]\n",
    "        return {\"predictions\": predictions}\n",
    "\n",
    "\n",
    "\n",
    "class EvaluatePredictionsSteps(PipelineStep):\n",
    "    def __init__(self, y_actual_df: str, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.y_actual_df = y_actual_df\n",
    "\n",
    "    def execute(self, pipeline: Pipeline, predictions) -> None:\n",
    "        y_actual = pipeline.get_artifact(self.y_actual_df)\n",
    "\n",
    "        product_actual = y_actual.groupby(\"product_id\")[\"target\"].sum()\n",
    "        product_pred = predictions.groupby(\"product_id\")[\"predictions\"].sum()\n",
    "\n",
    "        eval_df = pd.DataFrame({\n",
    "            \"product_id\": product_actual.index,\n",
    "            \"tn_real\": product_actual.values,\n",
    "            \"tn_pred\": product_pred.values\n",
    "        })\n",
    "\n",
    "        total_error = np.sum(np.abs(eval_df['tn_real'] - eval_df['tn_pred'])) / np.sum(eval_df['tn_real'])\n",
    "        print(f\"Error en test: {total_error:.4f}\")\n",
    "        print(\"\\nTop 5 productos con mayor error absoluto:\")\n",
    "        eval_df['error_absoluto'] = np.abs(eval_df['tn_real'] - eval_df['tn_pred'])\n",
    "        print(eval_df.sort_values('error_absoluto', ascending=False).head())\n",
    "        return {\n",
    "            \"eval_df\": eval_df,\n",
    "            \"total_error\": total_error\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "class PlotFeatureImportanceStep(PipelineStep):\n",
    "\n",
    "    def execute(self, model) -> None:\n",
    "        lgb.plot_importance(model)\n",
    "\n",
    "\n",
    "\n",
    "class KaggleSubmissionStep(PipelineStep):\n",
    "    def execute(self, predictions) -> None:\n",
    "        submission = predictions.groupby(\"product_id\")[\"predictions\"].sum().reset_index()\n",
    "        submission.columns = [\"product_id\", \"tn\"]\n",
    "        return {\"submission\": submission}\n",
    "\n",
    "\n",
    "\n",
    "class SaveExperimentStep(PipelineStep):\n",
    "    def __init__(self, exp_name: str, save_dataframes=False, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.exp_name = exp_name\n",
    "        self.save_dataframes = save_dataframes\n",
    "\n",
    "    def execute(self, pipeline: Pipeline) -> None:\n",
    "\n",
    "        # Create the experiment directory\n",
    "        exp_dir = f\"experiments/{self.exp_name}\"\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "        # obtengo el model\n",
    "        model = pipeline.get_artifact(\"model\")\n",
    "        # Save the model as a pickle file\n",
    "        with open(os.path.join(exp_dir, \"model.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        # guardo el error total de test\n",
    "        total_error = pipeline.get_artifact(\"total_error\")\n",
    "        with open(os.path.join(exp_dir, \"total_error.txt\"), \"w\") as f:\n",
    "            f.write(str(total_error))\n",
    "\n",
    "        # Save the submission file\n",
    "        submission = pipeline.get_artifact(\"submission\")\n",
    "        submission.to_csv(os.path.join(exp_dir, f\"submission_{self.exp_name}_{total_error:.4f}.csv\"), index=False)\n",
    "\n",
    "        # borro submission model y error de los artifacts\n",
    "        pipeline.del_artifact(\"submission\")\n",
    "        \n",
    "        # Guardo los artifacts restantes que son dataframes como csvs\n",
    "        if self.save_dataframes:\n",
    "            for artifact_name, artifact in pipeline.artifacts.items():\n",
    "                if isinstance(artifact, pd.DataFrame):\n",
    "                    artifact.to_csv(os.path.join(exp_dir, f\"{artifact_name}.csv\"), index=False)\n",
    "\n",
    "\n",
    "        # Save a copy of the notebook\n",
    "        notebook_path = fallback_latest_notebook()\n",
    "        shutil.copy(notebook_path, os.path.join(exp_dir, f\"notebook_{self.exp_name}.ipynb\"))\n",
    "\n",
    "\n",
    "\n",
    "class FilterProductsIDStep(PipelineStep):\n",
    "    def __init__(self, product_file = \"product_id_apredecir201912.txt\", dfs=[\"df\"], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.file = product_file\n",
    "        self.dfs = dfs\n",
    "\n",
    "    def execute(self, pipeline: Pipeline) -> None:\n",
    "        \"\"\" el txt es un csv que tiene columna product_id separado por tabulaciones \"\"\"\n",
    "        converted_dfs = {}\n",
    "        for df_key in self.dfs:\n",
    "            df = pipeline.get_artifact(df_key)\n",
    "            product_ids = pd.read_csv(self.file, sep=\"\\t\")[\"product_id\"].tolist()\n",
    "            df = df[df[\"product_id\"].isin(product_ids)]\n",
    "            converted_dfs[df_key] = df\n",
    "            print(f\"Filtered DataFrame {df_key} shape: {df.shape}\")\n",
    "        return converted_dfs\n",
    "    \n",
    "\n",
    "class FilterProductForTestingStep(PipelineStep):\n",
    "    def __init__(self, total_products_ids: int = 100, name: Optional[str] = None, random=True):\n",
    "        super().__init__(name)\n",
    "        self.total_products_ids = total_products_ids\n",
    "        self.random = random\n",
    "        \n",
    "    def execute(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\" Filtra el DataFrame para que contenga solo los primeros total_products_ids productos \"\"\"\n",
    "        unique_products = df['product_id'].unique()\n",
    "        if len(unique_products) > self.total_products_ids:\n",
    "            if self.random:\n",
    "                products = np.random.choice(unique_products, size=self.total_products_ids, replace=False)\n",
    "            else:\n",
    "                products = unique_products[:self.total_products_ids]\n",
    "            df = df[df['product_id'].isin(products)]\n",
    "        print(f\"Filtered DataFrame shape: {df.shape}\")\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "from typing import List, Dict, Union, Optional, Type\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class CustomStandarScaler:\n",
    "    def __init__(self, column: str):\n",
    "        self.column = column\n",
    "        self.scaler_data = None\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        agg = df.groupby(['product_id', 'customer_id'])[self.column].agg(['mean', 'std']).rename(\n",
    "            columns={'mean': f'{self.column}_mean_scaler', 'std': f'{self.column}_std_scaler'})\n",
    "        self.scaler_data = agg\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df[f'{self.column}_scaled'] = (df[self.column] - df[f'{self.column}_mean_scaler']) / (df[f'{self.column}_std_scaler'])\n",
    "        df[f'{self.column}_scaled'] = df[f'{self.column}_scaled'].fillna(0)\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_mean_scaler', f'{self.column}_std_scaler'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        return self.fit(df).transform(df)\n",
    "    \n",
    "    def inverse_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.scaler_data is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "        \n",
    "        # agrego columnas temporales\n",
    "        df = df.merge(self.scaler_data, on=['product_id', 'customer_id'], how='left')\n",
    "        df[f\"{self.column}_scaled\"] = (df[f'{self.column}_scaled'] * (df[f'{self.column}_std_scaler'])) + df[f'{self.column}_mean_scaler']\n",
    "        # elimino las columnas temporales\n",
    "        df.drop(columns=[f'{self.column}_mean_scaler', f'{self.column}_std_scaler'], inplace=True, errors='ignore')\n",
    "        return df\n",
    "    \n",
    "    \n",
    "class ScaleFeatureStep(PipelineStep):\n",
    "    def __init__(self, column: str, scaler=CustomStandarScaler, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.column = column\n",
    "        self.scaler_cls = scaler\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        scaler = self.scaler_cls(\n",
    "            column=self.column,\n",
    "        )\n",
    "        df_scaled = scaler.fit_transform(df)\n",
    "        return {\n",
    "            \"df\": df_scaled,\n",
    "            \"custom_scaler\": scaler\n",
    "        }\n",
    "    \n",
    "class InverseScalePredictionsStep(PipelineStep):\n",
    "    def execute(self, custom_scaler, predictions, y_test, test) -> Dict:\n",
    "        \"\"\"\n",
    "        Inverse scale the predictions using the provided grouped scaler.\n",
    "        \"\"\"\n",
    "        # cambio el nombre de la columna a tn_scaled para que coincida con el scaler\n",
    "        predictions = predictions.rename(columns={\"predictions\": \"tn_scaled\"})\n",
    "        predictions = custom_scaler.inverse_transform(predictions).rename(columns={\"tn_scaled\": \"predictions\"})\n",
    "\n",
    "        # renombro target por tn_scaled en y_pred\n",
    "        # paso y_test de serie a df para que sea compatible\n",
    "\n",
    "        y_test_scaled = test[[\"target\", \"product_id\", \"customer_id\"]]\n",
    "        y_test_scaled = y_test_scaled.rename(columns={\"target\": \"tn_scaled\"})\n",
    "        y_test_scaled = custom_scaler.inverse_transform(y_test_scaled)\n",
    "        y_test_scaled = y_test_scaled.rename(columns={\"tn_scaled\": \"target\"})\n",
    "        y_test_scaled.index = test.index\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"y_test_unscale\": y_test_scaled\n",
    "        }\n",
    "        \n",
    "class CreateTargetColumStep(PipelineStep):\n",
    "    #df['target'] = df.groupby(['product_id', 'customer_id'])['tn'].shift(-2)\n",
    "    def __init__(self, name: Optional[str] = None, target_col: str = 'tn'):\n",
    "        super().__init__(name)\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Crea la columna 'target' como la suma de 'tn' de los próximos 2 meses\n",
    "        para cada combinación de 'product_id' y 'customer_id'.\n",
    "        \"\"\"\n",
    "        df.drop(columns=[\"target\"], inplace=True, errors='ignore')  # Elimina la columna si ya existe\n",
    "        df = df.sort_values(['product_id', 'customer_id', 'fecha'])\n",
    "\n",
    "        df['target'] = df.groupby(['product_id', 'customer_id'])[self.target_col].shift(-2)\n",
    "        \n",
    "        return {\"df\": df}\n",
    "\n",
    "class SaveDataFrameStep(PipelineStep):\n",
    "    def __init__(self, df_name: str, file_name: str, ext = \"pickle\", name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.df_name = df_name\n",
    "        self.file_name = file_name\n",
    "        self.ext = ext\n",
    "\n",
    "    def execute(self, pipeline: Pipeline) -> None:\n",
    "        df = pipeline.get_artifact(self.df_name)\n",
    "        if self.ext == \"pickle\":\n",
    "            df.to_pickle(self.file_name)\n",
    "        elif self.ext == \"parquet\":\n",
    "            df.to_parquet(f\"{self.file_name}.parquet\", index=False)\n",
    "        elif self.ext == \"csv\":\n",
    "            df.to_csv(f\"{self.file_name}.csv\", index=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {self.ext}\")\n",
    "        \n",
    "class RollingSkewFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            df[f'{col}_rolling_skew_{self.window}'] = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).skew()\n",
    "            )\n",
    "        return {\"df\": df}\n",
    "    \n",
    "class RollingZscoreFeatureStep(PipelineStep):\n",
    "    def __init__(self, window: int, columns: List[str], name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self.window = window\n",
    "        self.columns = columns\n",
    "\n",
    "    def execute(self, df: pd.DataFrame) -> Dict:\n",
    "        df = df.sort_values(by=['product_id', 'customer_id', 'fecha'])\n",
    "        grouped = df.groupby(['product_id', 'customer_id'])\n",
    "        for col in self.columns:\n",
    "            rolling_mean = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).mean()\n",
    "            )\n",
    "            rolling_std = grouped[col].transform(\n",
    "                lambda x: x.rolling(self.window, min_periods=1).std()\n",
    "            )\n",
    "            df[f'{col}_rolling_zscore_{self.window}'] = (df[col] - rolling_mean) / (rolling_std + 1e-6)\n",
    "        return {\"df\": df}\n",
    "    \n",
    "\n",
    "class ReduceMemoryUsageStep(PipelineStep):\n",
    "    def execute(self, df):\n",
    "        initial_mem_usage = df.memory_usage().sum() / 1024**2\n",
    "        for col in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if pd.api.types.is_float_dtype(df[col]):\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                elif pd.api.types.is_integer_dtype(df[col]):\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "        \n",
    "        final_mem_usage = df.memory_usage().sum() / 1024**2\n",
    "        print('--- Memory usage before: {:.2f} MB'.format(initial_mem_usage))\n",
    "        print('--- Memory usage after: {:.2f} MB'.format(final_mem_usage))\n",
    "        print('--- Decreased memory usage by {:.1f}%\\n'.format(100 * (initial_mem_usage - final_mem_usage) / initial_mem_usage))\n",
    "        return {\"df\": df}      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing step: LoadDataFrameStep\n",
      "Step LoadDataFrameStep completed in 2.28 seconds\n",
      "Executing step: FilterProductsIDStep\n",
      "Filtered DataFrame df shape: (11101445, 13)\n",
      "Step FilterProductsIDStep completed in 2.73 seconds\n",
      "Executing step: ScaleFeatureStep\n",
      "Step ScaleFeatureStep completed in 2.58 seconds\n",
      "Executing step: CreateTargetColumStep\n",
      "Step CreateTargetColumStep completed in 2.69 seconds\n",
      "Executing step: DateRelatedFeaturesStep\n",
      "Step DateRelatedFeaturesStep completed in 2.80 seconds\n",
      "Executing step: CastDataTypesStep\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11101445 entries, 0 to 11101444\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Dtype    \n",
      "---  ------                 -----    \n",
      " 0   product_id             uint32   \n",
      " 1   fecha                  period[M]\n",
      " 2   customer_id            uint32   \n",
      " 3   plan_precios_cuidados  float32  \n",
      " 4   cust_request_qty       int32    \n",
      " 5   cust_request_tn        float32  \n",
      " 6   tn                     float32  \n",
      " 7   stock_final            float32  \n",
      " 8   cat1                   category \n",
      " 9   cat2                   category \n",
      " 10  cat3                   category \n",
      " 11  brand                  category \n",
      " 12  sku_size               float32  \n",
      " 13  tn_scaled              float32  \n",
      " 14  target                 float32  \n",
      " 15  year                   uint16   \n",
      " 16  mes                    uint16   \n",
      "dtypes: category(4), float32(7), int32(1), period[M](1), uint16(2), uint32(2)\n",
      "memory usage: 592.9 MB\n",
      "None\n",
      "Step CastDataTypesStep completed in 1.39 seconds\n",
      "Executing step: ReduceMemoryUsageStep\n",
      "--- Memory usage before: 592.88 MB\n",
      "--- Memory usage after: 359.96 MB\n",
      "--- Decreased memory usage by 39.3%\n",
      "\n",
      "Step ReduceMemoryUsageStep completed in 1.60 seconds\n",
      "Executing step: FeatureEngineeringLagStep\n",
      "Step FeatureEngineeringLagStep completed in 5.18 seconds\n",
      "Executing step: RollingMeanFeatureStep\n",
      "Step RollingMeanFeatureStep completed in 215.95 seconds\n",
      "Executing step: RollingMaxFeatureStep\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        LoadDataFrameStep(path=\"df_intermedio.parquet\"),\n",
    "        #FilterProductForTestingStep(total_products_ids=2, random=True), # para hacer pruebas mas rapidas\n",
    "        FilterProductsIDStep(dfs=[\"df\"]), # hago este aca para que el dataset sea mas chico y pueda correrlo en local\n",
    "        ScaleFeatureStep(\n",
    "            column=\"tn\",\n",
    "            scaler=CustomStandarScaler,\n",
    "        ),\n",
    "        CreateTargetColumStep(target_col=\"tn_scaled\"),\n",
    "        DateRelatedFeaturesStep(),\n",
    "        CastDataTypesStep(dtypes=\n",
    "            {\n",
    "                \"product_id\": \"uint32\", \n",
    "                \"customer_id\": \"uint32\",\n",
    "                \"mes\": \"uint16\",\n",
    "                \"year\": \"uint16\",\n",
    "                \"brand\": \"category\",\n",
    "                \"cat1\": \"category\",\n",
    "                \"cat2\": \"category\",\n",
    "                \"cat3\": \"category\",\n",
    "            }\n",
    "        ),\n",
    "\n",
    "        ReduceMemoryUsageStep(),\n",
    "        FeatureEngineeringLagStep(lags=[1,2,3,5,11, 23], columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        RollingMeanFeatureStep(window=3, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        RollingMaxFeatureStep(window=3, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        RollingMinFeatureStep(window=3, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        RollingMeanFeatureStep(window=9, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        RollingMaxFeatureStep(window=9, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        RollingMinFeatureStep(window=9, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "\n",
    "        ReduceMemoryUsageStep(),\n",
    "\n",
    "        RollingStdFeatureStep(window=3, columns=[\"tn\", \"cust_request_qty\"]),\n",
    "        RollingStdFeatureStep(window=6, columns=[\"tn\", \"cust_request_qty\"]),\n",
    "        RollingStdFeatureStep(window=12, columns=[\"tn\", \"cust_request_qty\"]), \n",
    "\n",
    "        RollingSkewFeatureStep(window=3, columns=[\"tn\", \"cust_request_qty\"]),\n",
    "        RollingSkewFeatureStep(window=6, columns=[\"tn\", \"cust_request_qty\"]),\n",
    "        RollingSkewFeatureStep(window=12, columns=[\"tn\", \"cust_request_qty\"]),\n",
    "        ReduceMemoryUsageStep(),\n",
    "\n",
    "        RollingZscoreFeatureStep(window=3, columns=[\"tn\", \"cust_request_qty\"]),\n",
    "        RollingZscoreFeatureStep(window=6, columns=[\"tn\", \"cust_request_qty\"]),\n",
    "        RollingZscoreFeatureStep(window=12, columns=[\"tn\", \"cust_request_qty\"]),\n",
    "\n",
    "        ReduceMemoryUsageStep(),\n",
    "\n",
    "        DiffFeatureStep(periods=1, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        DiffFeatureStep(periods=2, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        DiffFeatureStep(periods=3, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        DiffFeatureStep(periods=4, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        DiffFeatureStep(periods=5, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        DiffFeatureStep(periods=11, columns=[\"tn_scaled\", \"cust_request_qty\", \"stock_final\"]),\n",
    "        FeatureEngineeringProductCatInteractionStep(cat=\"cat1\", tn=\"tn_scaled\"),\n",
    "        FeatureEngineeringProductCatInteractionStep(cat=\"cat2\", tn=\"tn_scaled\"),\n",
    "        FeatureEngineeringProductCatInteractionStep(cat=\"cat3\", tn=\"tn_scaled\"),\n",
    "\n",
    "        ReduceMemoryUsageStep(),\n",
    "\n",
    "        CreateTotalCategoryStep(cat=\"cat1\"),\n",
    "        CreateTotalCategoryStep(cat=\"cat2\"),\n",
    "        CreateTotalCategoryStep(cat=\"cat3\"),\n",
    "        CreateTotalCategoryStep(cat=\"brand\"),\n",
    "        CreateTotalCategoryStep(cat=\"customer_id\"),\n",
    "        CreateTotalCategoryStep(cat=\"product_id\"),\n",
    "\n",
    "        ReduceMemoryUsageStep(),\n",
    "\n",
    "        CreateTotalCategoryStep(cat=\"cat1\", tn=\"tn_scaled\"),\n",
    "        CreateTotalCategoryStep(cat=\"cat2\", tn=\"tn_scaled\"),\n",
    "        CreateTotalCategoryStep(cat=\"cat3\", tn=\"tn_scaled\"),\n",
    "        CreateTotalCategoryStep(cat=\"brand\", tn=\"tn_scaled\"),\n",
    "        CreateTotalCategoryStep(cat=\"customer_id\", tn=\"tn_scaled\"),\n",
    "        CreateTotalCategoryStep(cat=\"product_id\", tn=\"tn_scaled\"),\n",
    "                \n",
    "        CreateTotalCategoryStep(cat=\"cat1\", tn=\"stock_final\"),\n",
    "        CreateTotalCategoryStep(cat=\"cat2\", tn=\"stock_final\"),\n",
    "        CreateTotalCategoryStep(cat=\"cat3\", tn=\"stock_final\"),\n",
    "\n",
    "        ReduceMemoryUsageStep(),\n",
    "\n",
    "\n",
    "        FeatureDivInteractionStep(columns=[\n",
    "            (\"tn\", \"tn_cat1_vendidas\"), \n",
    "            (\"tn\", \"tn_cat2_vendidas\"), \n",
    "            (\"tn\", \"tn_cat3_vendidas\"), \n",
    "            (\"tn\", \"tn_brand_vendidas\")]\n",
    "        ),\n",
    "        ReduceMemoryUsageStep(),\n",
    "\n",
    "        FeatureProdInteractionStep(columns=[(\"tn_scaled\", \"cust_request_qty\")]),\n",
    "        CreateWeightByCustomerStep(),\n",
    "        CreateWeightByProductStep(),\n",
    "        ReduceMemoryUsageStep(),\n",
    "\n",
    "\n",
    "        SaveDataFrameStep(df_name=\"df\", file_name=\"df_fe_big.pickle\"),\n",
    "\n",
    "        SplitDataFrameStep(),\n",
    "        FilterProductsIDStep(dfs=[\"test\", \"eval_data\", \"kaggle_pred\"]),\n",
    "        PrepareXYStep(),\n",
    "        TrainModelLGBStep(\n",
    "            train_eval_sets={\n",
    "                \"X_train\": \"X_train\",\n",
    "                \"y_train\": \"y_train\",\n",
    "                \"X_eval\": \"X_eval\",\n",
    "                \"y_eval\": \"y_eval\",\n",
    "                \"eval_data\": \"eval_data\"\n",
    "            },\n",
    "        ),\n",
    "        PredictStep(predict_set=\"X_test\"),\n",
    "        InverseScalePredictionsStep(),\n",
    "        EvaluatePredictionsSteps(y_actual_df=\"y_test_unscale\"),\n",
    "        PlotFeatureImportanceStep(),\n",
    "        TrainModelLGBStep(\n",
    "            train_eval_sets={\n",
    "               \"X_train\": \"X_train_final\",\n",
    "               \"y_train\": \"y_train_final\",\n",
    "                \"X_eval\": \"X_test\",\n",
    "                \"y_eval\": \"y_test\",\n",
    "                \"eval_data\": \"test\",\n",
    "            },\n",
    "        ),\n",
    "        PredictStep(predict_set=\"X_kaggle\"),\n",
    "        InverseScalePredictionsStep(),\n",
    "        KaggleSubmissionStep(), # falta hacer la inversa de la prediccion\n",
    "        SaveExperimentStep(exp_name=f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M')}_exp_lgb_scaled_target_no_feature_df\", save_dataframes=False),\n",
    "\n",
    "    ],\n",
    "        optimize_arftifacts_memory=True\n",
    ")\n",
    "pipeline.run(verbose=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
